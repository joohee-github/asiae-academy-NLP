{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습의 자동중단\n",
    "\n",
    "- 와인의 종류 예측하기 : 학습의 자동중단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.9975 - accuracy: 0.7080\n",
      "Epoch 00001: val_loss improved from inf to 0.38286, saving model to /.model\\01-0.382857.hdf5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.5628 - accuracy: 0.7594 - val_loss: 0.3829 - val_accuracy: 0.8214\n",
      "Epoch 2/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.4128 - accuracy: 0.7940\n",
      "Epoch 00002: val_loss improved from 0.38286 to 0.29474, saving model to /.model\\02-0.294740.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.3644 - accuracy: 0.8676 - val_loss: 0.2947 - val_accuracy: 0.8909\n",
      "Epoch 3/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.3301 - accuracy: 0.8940\n",
      "Epoch 00003: val_loss improved from 0.29474 to 0.28058, saving model to /.model\\03-0.280583.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8784 - val_loss: 0.2806 - val_accuracy: 0.8974\n",
      "Epoch 4/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.3014 - accuracy: 0.8940\n",
      "Epoch 00004: val_loss improved from 0.28058 to 0.26377, saving model to /.model\\04-0.263766.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.3038 - accuracy: 0.8987 - val_loss: 0.2638 - val_accuracy: 0.9254\n",
      "Epoch 5/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.3019 - accuracy: 0.9000\n",
      "Epoch 00005: val_loss improved from 0.26377 to 0.25514, saving model to /.model\\05-0.255143.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.2877 - accuracy: 0.9150 - val_loss: 0.2551 - val_accuracy: 0.9310\n",
      "Epoch 6/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.3004 - accuracy: 0.9120\n",
      "Epoch 00006: val_loss improved from 0.25514 to 0.24117, saving model to /.model\\06-0.241167.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.2757 - accuracy: 0.9113 - val_loss: 0.2412 - val_accuracy: 0.9263\n",
      "Epoch 7/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2494 - accuracy: 0.9320\n",
      "Epoch 00007: val_loss improved from 0.24117 to 0.23533, saving model to /.model\\07-0.235328.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2664 - accuracy: 0.9120 - val_loss: 0.2353 - val_accuracy: 0.9268\n",
      "Epoch 8/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2734 - accuracy: 0.9040\n",
      "Epoch 00008: val_loss improved from 0.23533 to 0.22850, saving model to /.model\\08-0.228501.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.9173 - val_loss: 0.2285 - val_accuracy: 0.9273\n",
      "Epoch 9/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2452 - accuracy: 0.9160\n",
      "Epoch 00009: val_loss improved from 0.22850 to 0.22045, saving model to /.model\\09-0.220448.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.9157 - val_loss: 0.2204 - val_accuracy: 0.9282\n",
      "Epoch 10/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2930 - accuracy: 0.8940\n",
      "Epoch 00010: val_loss improved from 0.22045 to 0.21533, saving model to /.model\\10-0.215332.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9198 - val_loss: 0.2153 - val_accuracy: 0.9315\n",
      "Epoch 11/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2552 - accuracy: 0.9240\n",
      "Epoch 00011: val_loss improved from 0.21533 to 0.20428, saving model to /.model\\11-0.204279.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2358 - accuracy: 0.9244 - val_loss: 0.2043 - val_accuracy: 0.9347\n",
      "Epoch 12/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2254 - accuracy: 0.9320\n",
      "Epoch 00012: val_loss improved from 0.20428 to 0.19698, saving model to /.model\\12-0.196976.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9267 - val_loss: 0.1970 - val_accuracy: 0.9380\n",
      "Epoch 13/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1952 - accuracy: 0.9380\n",
      "Epoch 00013: val_loss improved from 0.19698 to 0.19285, saving model to /.model\\13-0.192846.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.9292 - val_loss: 0.1928 - val_accuracy: 0.9366\n",
      "Epoch 14/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2217 - accuracy: 0.9320\n",
      "Epoch 00014: val_loss improved from 0.19285 to 0.18909, saving model to /.model\\14-0.189094.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2141 - accuracy: 0.9301 - val_loss: 0.1891 - val_accuracy: 0.9371\n",
      "Epoch 15/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2162 - accuracy: 0.9220\n",
      "Epoch 00015: val_loss improved from 0.18909 to 0.18797, saving model to /.model\\15-0.187970.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2110 - accuracy: 0.9311 - val_loss: 0.1880 - val_accuracy: 0.9361\n",
      "Epoch 16/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1743 - accuracy: 0.9480\n",
      "Epoch 00016: val_loss improved from 0.18797 to 0.18489, saving model to /.model\\16-0.184891.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2078 - accuracy: 0.9308 - val_loss: 0.1849 - val_accuracy: 0.9380\n",
      "Epoch 17/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1839 - accuracy: 0.9360\n",
      "Epoch 00017: val_loss improved from 0.18489 to 0.18402, saving model to /.model\\17-0.184016.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9313 - val_loss: 0.1840 - val_accuracy: 0.9366\n",
      "Epoch 18/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2317 - accuracy: 0.9160\n",
      "Epoch 00018: val_loss improved from 0.18402 to 0.18219, saving model to /.model\\18-0.182190.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9311 - val_loss: 0.1822 - val_accuracy: 0.9375\n",
      "Epoch 19/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2048 - accuracy: 0.9160\n",
      "Epoch 00019: val_loss improved from 0.18219 to 0.18148, saving model to /.model\\19-0.181480.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.2013 - accuracy: 0.9315 - val_loss: 0.1815 - val_accuracy: 0.9385\n",
      "Epoch 20/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1781 - accuracy: 0.9400\n",
      "Epoch 00020: val_loss improved from 0.18148 to 0.18109, saving model to /.model\\20-0.181091.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9334 - val_loss: 0.1811 - val_accuracy: 0.9385\n",
      "Epoch 21/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1545 - accuracy: 0.9500\n",
      "Epoch 00021: val_loss improved from 0.18109 to 0.17826, saving model to /.model\\21-0.178261.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9320 - val_loss: 0.1783 - val_accuracy: 0.9413\n",
      "Epoch 22/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2078 - accuracy: 0.9340\n",
      "Epoch 00022: val_loss did not improve from 0.17826\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9338 - val_loss: 0.1793 - val_accuracy: 0.9389\n",
      "Epoch 23/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1772 - accuracy: 0.9340\n",
      "Epoch 00023: val_loss improved from 0.17826 to 0.17790, saving model to /.model\\23-0.177904.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1937 - accuracy: 0.9334 - val_loss: 0.1779 - val_accuracy: 0.9399\n",
      "Epoch 24/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1998 - accuracy: 0.9380\n",
      "Epoch 00024: val_loss improved from 0.17790 to 0.17714, saving model to /.model\\24-0.177141.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1922 - accuracy: 0.9341 - val_loss: 0.1771 - val_accuracy: 0.9403\n",
      "Epoch 25/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1910 - accuracy: 0.9460\n",
      "Epoch 00025: val_loss improved from 0.17714 to 0.17521, saving model to /.model\\25-0.175206.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1907 - accuracy: 0.9345 - val_loss: 0.1752 - val_accuracy: 0.9417\n",
      "Epoch 26/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1647 - accuracy: 0.9420\n",
      "Epoch 00026: val_loss did not improve from 0.17521\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1895 - accuracy: 0.9343 - val_loss: 0.1757 - val_accuracy: 0.9408\n",
      "Epoch 27/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1946 - accuracy: 0.9280\n",
      "Epoch 00027: val_loss improved from 0.17521 to 0.17352, saving model to /.model\\27-0.173516.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1881 - accuracy: 0.9347 - val_loss: 0.1735 - val_accuracy: 0.9422\n",
      "Epoch 28/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2099 - accuracy: 0.9260\n",
      "Epoch 00028: val_loss improved from 0.17352 to 0.17283, saving model to /.model\\28-0.172828.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1869 - accuracy: 0.9354 - val_loss: 0.1728 - val_accuracy: 0.9422\n",
      "Epoch 29/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1521 - accuracy: 0.9340\n",
      "Epoch 00029: val_loss did not improve from 0.17283\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1855 - accuracy: 0.9357 - val_loss: 0.1732 - val_accuracy: 0.9417\n",
      "Epoch 30/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1763 - accuracy: 0.9340\n",
      "Epoch 00030: val_loss improved from 0.17283 to 0.17141, saving model to /.model\\30-0.171415.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.9361 - val_loss: 0.1714 - val_accuracy: 0.9427\n",
      "Epoch 31/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2411 - accuracy: 0.9120\n",
      "Epoch 00031: val_loss improved from 0.17141 to 0.16842, saving model to /.model\\31-0.168423.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1839 - accuracy: 0.9364 - val_loss: 0.1684 - val_accuracy: 0.9436\n",
      "Epoch 32/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2082 - accuracy: 0.9360\n",
      "Epoch 00032: val_loss did not improve from 0.16842\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1813 - accuracy: 0.9361 - val_loss: 0.1719 - val_accuracy: 0.9422\n",
      "Epoch 33/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1911 - accuracy: 0.9280\n",
      "Epoch 00033: val_loss improved from 0.16842 to 0.16681, saving model to /.model\\33-0.166812.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1810 - accuracy: 0.9354 - val_loss: 0.1668 - val_accuracy: 0.9450\n",
      "Epoch 34/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1576 - accuracy: 0.9420\n",
      "Epoch 00034: val_loss improved from 0.16681 to 0.16667, saving model to /.model\\34-0.166673.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1787 - accuracy: 0.9364 - val_loss: 0.1667 - val_accuracy: 0.9445\n",
      "Epoch 35/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1723 - accuracy: 0.9360\n",
      "Epoch 00035: val_loss did not improve from 0.16667\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1774 - accuracy: 0.9368 - val_loss: 0.1691 - val_accuracy: 0.9436\n",
      "Epoch 36/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1585 - accuracy: 0.9440\n",
      "Epoch 00036: val_loss improved from 0.16667 to 0.16351, saving model to /.model\\36-0.163510.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.9359 - val_loss: 0.1635 - val_accuracy: 0.9450\n",
      "Epoch 37/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1947 - accuracy: 0.9260\n",
      "Epoch 00037: val_loss did not improve from 0.16351\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1742 - accuracy: 0.9368 - val_loss: 0.1676 - val_accuracy: 0.9436\n",
      "Epoch 38/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1768 - accuracy: 0.9380\n",
      "Epoch 00038: val_loss improved from 0.16351 to 0.16124, saving model to /.model\\38-0.161243.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.9384 - val_loss: 0.1612 - val_accuracy: 0.9455\n",
      "Epoch 39/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1827 - accuracy: 0.9360\n",
      "Epoch 00039: val_loss did not improve from 0.16124\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1710 - accuracy: 0.9389 - val_loss: 0.1653 - val_accuracy: 0.9441\n",
      "Epoch 40/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1594 - accuracy: 0.9480\n",
      "Epoch 00040: val_loss improved from 0.16124 to 0.15965, saving model to /.model\\40-0.159645.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1698 - accuracy: 0.9382 - val_loss: 0.1596 - val_accuracy: 0.9469\n",
      "Epoch 41/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1501 - accuracy: 0.9560\n",
      "Epoch 00041: val_loss did not improve from 0.15965\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1683 - accuracy: 0.9407 - val_loss: 0.1626 - val_accuracy: 0.9450\n",
      "Epoch 42/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1471 - accuracy: 0.9500\n",
      "Epoch 00042: val_loss improved from 0.15965 to 0.15696, saving model to /.model\\42-0.156960.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1675 - accuracy: 0.9393 - val_loss: 0.1570 - val_accuracy: 0.9464\n",
      "Epoch 43/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1513 - accuracy: 0.9420\n",
      "Epoch 00043: val_loss improved from 0.15696 to 0.15660, saving model to /.model\\43-0.156603.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1656 - accuracy: 0.9409 - val_loss: 0.1566 - val_accuracy: 0.9483\n",
      "Epoch 44/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.2126 - accuracy: 0.9140\n",
      "Epoch 00044: val_loss improved from 0.15660 to 0.15404, saving model to /.model\\44-0.154045.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.9400 - val_loss: 0.1540 - val_accuracy: 0.9473\n",
      "Epoch 45/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1975 - accuracy: 0.9340\n",
      "Epoch 00045: val_loss improved from 0.15404 to 0.15310, saving model to /.model\\45-0.153104.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1620 - accuracy: 0.9414 - val_loss: 0.1531 - val_accuracy: 0.9483\n",
      "Epoch 46/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1810 - accuracy: 0.9380\n",
      "Epoch 00046: val_loss did not improve from 0.15310\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1594 - accuracy: 0.9428 - val_loss: 0.1537 - val_accuracy: 0.9478\n",
      "Epoch 47/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1444 - accuracy: 0.9440\n",
      "Epoch 00047: val_loss improved from 0.15310 to 0.15247, saving model to /.model\\47-0.152468.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.9428 - val_loss: 0.1525 - val_accuracy: 0.9469\n",
      "Epoch 48/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1343 - accuracy: 0.9520\n",
      "Epoch 00048: val_loss did not improve from 0.15247\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1573 - accuracy: 0.9428 - val_loss: 0.1540 - val_accuracy: 0.9469\n",
      "Epoch 49/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1770 - accuracy: 0.9240\n",
      "Epoch 00049: val_loss improved from 0.15247 to 0.14654, saving model to /.model\\49-0.146538.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.9449 - val_loss: 0.1465 - val_accuracy: 0.9487\n",
      "Epoch 50/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1471 - accuracy: 0.9420\n",
      "Epoch 00050: val_loss improved from 0.14654 to 0.14429, saving model to /.model\\50-0.144290.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1514 - accuracy: 0.9446 - val_loss: 0.1443 - val_accuracy: 0.9492\n",
      "Epoch 51/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1492 - accuracy: 0.9480\n",
      "Epoch 00051: val_loss did not improve from 0.14429\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1491 - accuracy: 0.9469 - val_loss: 0.1447 - val_accuracy: 0.9497\n",
      "Epoch 52/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1581 - accuracy: 0.9420\n",
      "Epoch 00052: val_loss improved from 0.14429 to 0.14372, saving model to /.model\\52-0.143723.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1484 - accuracy: 0.9462 - val_loss: 0.1437 - val_accuracy: 0.9497\n",
      "Epoch 53/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1331 - accuracy: 0.9460\n",
      "Epoch 00053: val_loss improved from 0.14372 to 0.14221, saving model to /.model\\53-0.142214.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1456 - accuracy: 0.9483 - val_loss: 0.1422 - val_accuracy: 0.9510\n",
      "Epoch 54/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1515 - accuracy: 0.9540\n",
      "Epoch 00054: val_loss improved from 0.14221 to 0.13982, saving model to /.model\\54-0.139819.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1441 - accuracy: 0.9483 - val_loss: 0.1398 - val_accuracy: 0.9515\n",
      "Epoch 55/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1304 - accuracy: 0.9560\n",
      "Epoch 00055: val_loss improved from 0.13982 to 0.13864, saving model to /.model\\55-0.138639.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1429 - accuracy: 0.9481 - val_loss: 0.1386 - val_accuracy: 0.9510\n",
      "Epoch 56/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1372 - accuracy: 0.9420\n",
      "Epoch 00056: val_loss improved from 0.13864 to 0.13687, saving model to /.model\\56-0.136870.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1413 - accuracy: 0.9474 - val_loss: 0.1369 - val_accuracy: 0.9524\n",
      "Epoch 57/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1487 - accuracy: 0.9440\n",
      "Epoch 00057: val_loss did not improve from 0.13687\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1397 - accuracy: 0.9490 - val_loss: 0.1399 - val_accuracy: 0.9510\n",
      "Epoch 58/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0981 - accuracy: 0.9720\n",
      "Epoch 00058: val_loss improved from 0.13687 to 0.13492, saving model to /.model\\58-0.134923.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.9490 - val_loss: 0.1349 - val_accuracy: 0.9515\n",
      "Epoch 59/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1059 - accuracy: 0.9720\n",
      "Epoch 00059: val_loss did not improve from 0.13492\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.9504 - val_loss: 0.1368 - val_accuracy: 0.9529\n",
      "Epoch 60/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1226 - accuracy: 0.9480\n",
      "Epoch 00060: val_loss did not improve from 0.13492\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.9499 - val_loss: 0.1349 - val_accuracy: 0.9515\n",
      "Epoch 61/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1199 - accuracy: 0.9680\n",
      "Epoch 00061: val_loss did not improve from 0.13492\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9508 - val_loss: 0.1401 - val_accuracy: 0.9552\n",
      "Epoch 62/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1227 - accuracy: 0.9620\n",
      "Epoch 00062: val_loss improved from 0.13492 to 0.13441, saving model to /.model\\62-0.134413.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1381 - accuracy: 0.9499 - val_loss: 0.1344 - val_accuracy: 0.9534\n",
      "Epoch 63/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1002 - accuracy: 0.9600\n",
      "Epoch 00063: val_loss did not improve from 0.13441\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1361 - accuracy: 0.9490 - val_loss: 0.1369 - val_accuracy: 0.9557\n",
      "Epoch 64/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1306 - accuracy: 0.9500\n",
      "Epoch 00064: val_loss did not improve from 0.13441\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1305 - accuracy: 0.9497 - val_loss: 0.1353 - val_accuracy: 0.9576\n",
      "Epoch 65/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1138 - accuracy: 0.9740\n",
      "Epoch 00065: val_loss improved from 0.13441 to 0.13411, saving model to /.model\\65-0.134106.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1297 - accuracy: 0.9513 - val_loss: 0.1341 - val_accuracy: 0.9548\n",
      "Epoch 66/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1349 - accuracy: 0.9520\n",
      "Epoch 00066: val_loss improved from 0.13411 to 0.13042, saving model to /.model\\66-0.130425.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9508 - val_loss: 0.1304 - val_accuracy: 0.9566\n",
      "Epoch 67/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1158 - accuracy: 0.9700\n",
      "Epoch 00067: val_loss improved from 0.13042 to 0.12892, saving model to /.model\\67-0.128920.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9529 - val_loss: 0.1289 - val_accuracy: 0.9576\n",
      "Epoch 68/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1495 - accuracy: 0.9480\n",
      "Epoch 00068: val_loss improved from 0.12892 to 0.12770, saving model to /.model\\68-0.127697.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9529 - val_loss: 0.1277 - val_accuracy: 0.9562\n",
      "Epoch 69/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1484 - accuracy: 0.9480\n",
      "Epoch 00069: val_loss improved from 0.12770 to 0.12637, saving model to /.model\\69-0.126374.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9589 - val_loss: 0.1264 - val_accuracy: 0.9557\n",
      "Epoch 70/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1529 - accuracy: 0.9480\n",
      "Epoch 00070: val_loss improved from 0.12637 to 0.12136, saving model to /.model\\70-0.121358.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9529 - val_loss: 0.1214 - val_accuracy: 0.9543\n",
      "Epoch 71/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1119 - accuracy: 0.9620\n",
      "Epoch 00071: val_loss improved from 0.12136 to 0.11994, saving model to /.model\\71-0.119942.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9561 - val_loss: 0.1199 - val_accuracy: 0.9557\n",
      "Epoch 72/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1225 - accuracy: 0.9560\n",
      "Epoch 00072: val_loss did not improve from 0.11994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1207 - accuracy: 0.9545 - val_loss: 0.1208 - val_accuracy: 0.9590\n",
      "Epoch 73/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1434 - accuracy: 0.9440\n",
      "Epoch 00073: val_loss did not improve from 0.11994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9554 - val_loss: 0.1203 - val_accuracy: 0.9594\n",
      "Epoch 74/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1265 - accuracy: 0.9480\n",
      "Epoch 00074: val_loss did not improve from 0.11994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1188 - accuracy: 0.9545 - val_loss: 0.1216 - val_accuracy: 0.9594\n",
      "Epoch 75/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1173 - accuracy: 0.9600\n",
      "Epoch 00075: val_loss improved from 0.11994 to 0.11607, saving model to /.model\\75-0.116069.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1177 - accuracy: 0.9566 - val_loss: 0.1161 - val_accuracy: 0.9590\n",
      "Epoch 76/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0984 - accuracy: 0.9620\n",
      "Epoch 00076: val_loss improved from 0.11607 to 0.11563, saving model to /.model\\76-0.115625.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1182 - accuracy: 0.9584 - val_loss: 0.1156 - val_accuracy: 0.9576\n",
      "Epoch 77/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0987 - accuracy: 0.9560\n",
      "Epoch 00077: val_loss improved from 0.11563 to 0.11445, saving model to /.model\\77-0.114447.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1184 - accuracy: 0.9568 - val_loss: 0.1144 - val_accuracy: 0.9599\n",
      "Epoch 78/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1444 - accuracy: 0.9420\n",
      "Epoch 00078: val_loss improved from 0.11445 to 0.11356, saving model to /.model\\78-0.113561.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1166 - accuracy: 0.9589 - val_loss: 0.1136 - val_accuracy: 0.9618\n",
      "Epoch 79/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1361 - accuracy: 0.9540\n",
      "Epoch 00079: val_loss improved from 0.11356 to 0.11294, saving model to /.model\\79-0.112937.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1160 - accuracy: 0.9614 - val_loss: 0.1129 - val_accuracy: 0.9594\n",
      "Epoch 80/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1179 - accuracy: 0.9540\n",
      "Epoch 00080: val_loss improved from 0.11294 to 0.11189, saving model to /.model\\80-0.111888.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.9586 - val_loss: 0.1119 - val_accuracy: 0.9636\n",
      "Epoch 81/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1159 - accuracy: 0.9580\n",
      "Epoch 00081: val_loss did not improve from 0.11189\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9609 - val_loss: 0.1136 - val_accuracy: 0.9627\n",
      "Epoch 82/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0960 - accuracy: 0.9640\n",
      "Epoch 00082: val_loss improved from 0.11189 to 0.11098, saving model to /.model\\82-0.110983.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1127 - accuracy: 0.9591 - val_loss: 0.1110 - val_accuracy: 0.9627\n",
      "Epoch 83/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1290 - accuracy: 0.9520\n",
      "Epoch 00083: val_loss improved from 0.11098 to 0.11089, saving model to /.model\\83-0.110894.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1117 - accuracy: 0.9607 - val_loss: 0.1109 - val_accuracy: 0.9585\n",
      "Epoch 84/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1587 - accuracy: 0.9320\n",
      "Epoch 00084: val_loss improved from 0.11089 to 0.10901, saving model to /.model\\84-0.109010.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.9616 - val_loss: 0.1090 - val_accuracy: 0.9632\n",
      "Epoch 85/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1313 - accuracy: 0.9520\n",
      "Epoch 00085: val_loss improved from 0.10901 to 0.10780, saving model to /.model\\85-0.107799.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1091 - accuracy: 0.9605 - val_loss: 0.1078 - val_accuracy: 0.9650\n",
      "Epoch 86/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1144 - accuracy: 0.9660\n",
      "Epoch 00086: val_loss improved from 0.10780 to 0.10715, saving model to /.model\\86-0.107152.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9602 - val_loss: 0.1072 - val_accuracy: 0.9646\n",
      "Epoch 87/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1311 - accuracy: 0.9540\n",
      "Epoch 00087: val_loss improved from 0.10715 to 0.10642, saving model to /.model\\87-0.106423.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1127 - accuracy: 0.9621 - val_loss: 0.1064 - val_accuracy: 0.9632\n",
      "Epoch 88/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1058 - accuracy: 0.9540\n",
      "Epoch 00088: val_loss improved from 0.10642 to 0.10533, saving model to /.model\\88-0.105327.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9644 - val_loss: 0.1053 - val_accuracy: 0.9636\n",
      "Epoch 89/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0951 - accuracy: 0.9680\n",
      "Epoch 00089: val_loss improved from 0.10533 to 0.10490, saving model to /.model\\89-0.104901.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1057 - accuracy: 0.9642 - val_loss: 0.1049 - val_accuracy: 0.9636\n",
      "Epoch 90/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1259 - accuracy: 0.9540\n",
      "Epoch 00090: val_loss improved from 0.10490 to 0.10487, saving model to /.model\\90-0.104869.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.1046 - accuracy: 0.9639 - val_loss: 0.1049 - val_accuracy: 0.9660\n",
      "Epoch 91/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1123 - accuracy: 0.9620\n",
      "Epoch 00091: val_loss improved from 0.10487 to 0.10312, saving model to /.model\\91-0.103123.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1036 - accuracy: 0.9642 - val_loss: 0.1031 - val_accuracy: 0.9660\n",
      "Epoch 92/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1308 - accuracy: 0.9500\n",
      "Epoch 00092: val_loss improved from 0.10312 to 0.10304, saving model to /.model\\92-0.103043.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1038 - accuracy: 0.9644 - val_loss: 0.1030 - val_accuracy: 0.9636\n",
      "Epoch 93/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1021 - accuracy: 0.9580\n",
      "Epoch 00093: val_loss improved from 0.10304 to 0.10249, saving model to /.model\\93-0.102495.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1042 - accuracy: 0.9642 - val_loss: 0.1025 - val_accuracy: 0.9627\n",
      "Epoch 94/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0666 - accuracy: 0.9740\n",
      "Epoch 00094: val_loss improved from 0.10249 to 0.10064, saving model to /.model\\94-0.100645.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1031 - accuracy: 0.9658 - val_loss: 0.1006 - val_accuracy: 0.9641\n",
      "Epoch 95/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0907 - accuracy: 0.9620\n",
      "Epoch 00095: val_loss did not improve from 0.10064\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.9639 - val_loss: 0.1019 - val_accuracy: 0.9683\n",
      "Epoch 96/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1136 - accuracy: 0.9720\n",
      "Epoch 00096: val_loss did not improve from 0.10064\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1005 - accuracy: 0.9669 - val_loss: 0.1026 - val_accuracy: 0.9688\n",
      "Epoch 97/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0930 - accuracy: 0.9820\n",
      "Epoch 00097: val_loss improved from 0.10064 to 0.10004, saving model to /.model\\97-0.100038.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0993 - accuracy: 0.9674 - val_loss: 0.1000 - val_accuracy: 0.9688\n",
      "Epoch 98/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0902 - accuracy: 0.9700\n",
      "Epoch 00098: val_loss improved from 0.10004 to 0.09942, saving model to /.model\\98-0.099418.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9701 - val_loss: 0.0994 - val_accuracy: 0.9692\n",
      "Epoch 99/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1038 - accuracy: 0.9780\n",
      "Epoch 00099: val_loss did not improve from 0.09942\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0989 - accuracy: 0.9690 - val_loss: 0.1000 - val_accuracy: 0.9706\n",
      "Epoch 100/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1008 - accuracy: 0.9600\n",
      "Epoch 00100: val_loss did not improve from 0.09942\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.9681 - val_loss: 0.0996 - val_accuracy: 0.9711\n",
      "Epoch 101/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0964 - accuracy: 0.9720\n",
      "Epoch 00101: val_loss improved from 0.09942 to 0.09638, saving model to /.model\\101-0.096379.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0977 - accuracy: 0.9694 - val_loss: 0.0964 - val_accuracy: 0.9688\n",
      "Epoch 102/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1122 - accuracy: 0.9660\n",
      "Epoch 00102: val_loss improved from 0.09638 to 0.09566, saving model to /.model\\102-0.095665.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9697 - val_loss: 0.0957 - val_accuracy: 0.9683\n",
      "Epoch 103/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0919 - accuracy: 0.9700\n",
      "Epoch 00103: val_loss did not improve from 0.09566\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0954 - accuracy: 0.9699 - val_loss: 0.0965 - val_accuracy: 0.9702\n",
      "Epoch 104/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1250 - accuracy: 0.9680\n",
      "Epoch 00104: val_loss did not improve from 0.09566\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9704 - val_loss: 0.0979 - val_accuracy: 0.9730\n",
      "Epoch 105/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1016 - accuracy: 0.9680\n",
      "Epoch 00105: val_loss did not improve from 0.09566\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.9667 - val_loss: 0.1082 - val_accuracy: 0.9697\n",
      "Epoch 106/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0975 - accuracy: 0.9720\n",
      "Epoch 00106: val_loss did not improve from 0.09566\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1001 - accuracy: 0.9646 - val_loss: 0.1072 - val_accuracy: 0.9702\n",
      "Epoch 107/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0881 - accuracy: 0.9720\n",
      "Epoch 00107: val_loss did not improve from 0.09566\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9681 - val_loss: 0.0982 - val_accuracy: 0.9739\n",
      "Epoch 108/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1214 - accuracy: 0.9680\n",
      "Epoch 00108: val_loss improved from 0.09566 to 0.09073, saving model to /.model\\108-0.090729.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9715 - val_loss: 0.0907 - val_accuracy: 0.9730\n",
      "Epoch 109/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0690 - accuracy: 0.9800\n",
      "Epoch 00109: val_loss improved from 0.09073 to 0.08976, saving model to /.model\\109-0.089764.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.9722 - val_loss: 0.0898 - val_accuracy: 0.9720\n",
      "Epoch 110/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0885 - accuracy: 0.9780\n",
      "Epoch 00110: val_loss improved from 0.08976 to 0.08906, saving model to /.model\\110-0.089056.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.9743 - val_loss: 0.0891 - val_accuracy: 0.9720\n",
      "Epoch 111/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0941 - accuracy: 0.9720\n",
      "Epoch 00111: val_loss improved from 0.08906 to 0.08807, saving model to /.model\\111-0.088068.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9704 - val_loss: 0.0881 - val_accuracy: 0.9711\n",
      "Epoch 112/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1321 - accuracy: 0.9580\n",
      "Epoch 00112: val_loss did not improve from 0.08807\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9683 - val_loss: 0.0887 - val_accuracy: 0.9692\n",
      "Epoch 113/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1235 - accuracy: 0.9600\n",
      "Epoch 00113: val_loss did not improve from 0.08807\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.9701 - val_loss: 0.0904 - val_accuracy: 0.9678\n",
      "Epoch 114/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0913 - accuracy: 0.9640\n",
      "Epoch 00114: val_loss did not improve from 0.08807\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0948 - accuracy: 0.9708 - val_loss: 0.0897 - val_accuracy: 0.9674\n",
      "Epoch 115/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0996 - accuracy: 0.9560\n",
      "Epoch 00115: val_loss improved from 0.08807 to 0.08615, saving model to /.model\\115-0.086154.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.9715 - val_loss: 0.0862 - val_accuracy: 0.9720\n",
      "Epoch 116/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0804 - accuracy: 0.9720\n",
      "Epoch 00116: val_loss improved from 0.08615 to 0.08590, saving model to /.model\\116-0.085899.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0878 - accuracy: 0.9740 - val_loss: 0.0859 - val_accuracy: 0.9753\n",
      "Epoch 117/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0991 - accuracy: 0.9780\n",
      "Epoch 00117: val_loss improved from 0.08590 to 0.08490, saving model to /.model\\117-0.084896.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0885 - accuracy: 0.9731 - val_loss: 0.0849 - val_accuracy: 0.9739\n",
      "Epoch 118/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0935 - accuracy: 0.9740\n",
      "Epoch 00118: val_loss improved from 0.08490 to 0.08476, saving model to /.model\\118-0.084757.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9745 - val_loss: 0.0848 - val_accuracy: 0.9716\n",
      "Epoch 119/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0864 - accuracy: 0.9760\n",
      "Epoch 00119: val_loss improved from 0.08476 to 0.08382, saving model to /.model\\119-0.083825.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0878 - accuracy: 0.9729 - val_loss: 0.0838 - val_accuracy: 0.9730\n",
      "Epoch 120/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0793 - accuracy: 0.9700\n",
      "Epoch 00120: val_loss did not improve from 0.08382\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0890 - accuracy: 0.9747 - val_loss: 0.0838 - val_accuracy: 0.9758\n",
      "Epoch 121/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0771 - accuracy: 0.9800\n",
      "Epoch 00121: val_loss did not improve from 0.08382\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.9745 - val_loss: 0.0864 - val_accuracy: 0.9753\n",
      "Epoch 122/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1052 - accuracy: 0.9700\n",
      "Epoch 00122: val_loss improved from 0.08382 to 0.08305, saving model to /.model\\122-0.083052.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0870 - accuracy: 0.9738 - val_loss: 0.0831 - val_accuracy: 0.9753\n",
      "Epoch 123/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0951 - accuracy: 0.9680\n",
      "Epoch 00123: val_loss improved from 0.08305 to 0.08257, saving model to /.model\\123-0.082566.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 0.9729 - val_loss: 0.0826 - val_accuracy: 0.9762\n",
      "Epoch 124/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0594 - accuracy: 0.9840\n",
      "Epoch 00124: val_loss did not improve from 0.08257\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9736 - val_loss: 0.0844 - val_accuracy: 0.9767\n",
      "Epoch 125/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0755 - accuracy: 0.9860\n",
      "Epoch 00125: val_loss improved from 0.08257 to 0.08217, saving model to /.model\\125-0.082173.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0907 - accuracy: 0.9715 - val_loss: 0.0822 - val_accuracy: 0.9762\n",
      "Epoch 126/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0634 - accuracy: 0.9880\n",
      "Epoch 00126: val_loss did not improve from 0.08217\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0875 - accuracy: 0.9717 - val_loss: 0.0834 - val_accuracy: 0.9762\n",
      "Epoch 127/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0809 - accuracy: 0.9740\n",
      "Epoch 00127: val_loss did not improve from 0.08217\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9743 - val_loss: 0.0829 - val_accuracy: 0.9762\n",
      "Epoch 128/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9820\n",
      "Epoch 00128: val_loss improved from 0.08217 to 0.08173, saving model to /.model\\128-0.081732.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9747 - val_loss: 0.0817 - val_accuracy: 0.9767\n",
      "Epoch 129/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0769 - accuracy: 0.9740\n",
      "Epoch 00129: val_loss did not improve from 0.08173\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9759 - val_loss: 0.0907 - val_accuracy: 0.9744\n",
      "Epoch 130/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0813 - accuracy: 0.9780\n",
      "Epoch 00130: val_loss did not improve from 0.08173\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9743 - val_loss: 0.0831 - val_accuracy: 0.9767\n",
      "Epoch 131/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1037 - accuracy: 0.9600\n",
      "Epoch 00131: val_loss improved from 0.08173 to 0.07971, saving model to /.model\\131-0.079710.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0831 - accuracy: 0.9743 - val_loss: 0.0797 - val_accuracy: 0.9758\n",
      "Epoch 132/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9800\n",
      "Epoch 00132: val_loss did not improve from 0.07971\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0810 - accuracy: 0.9745 - val_loss: 0.0813 - val_accuracy: 0.9711\n",
      "Epoch 133/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0720 - accuracy: 0.9760\n",
      "Epoch 00133: val_loss improved from 0.07971 to 0.07934, saving model to /.model\\133-0.079339.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9745 - val_loss: 0.0793 - val_accuracy: 0.9767\n",
      "Epoch 134/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0615 - accuracy: 0.9840\n",
      "Epoch 00134: val_loss did not improve from 0.07934\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 0.9754 - val_loss: 0.0875 - val_accuracy: 0.9730\n",
      "Epoch 135/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1142 - accuracy: 0.9560\n",
      "Epoch 00135: val_loss did not improve from 0.07934\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9747 - val_loss: 0.0803 - val_accuracy: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0687 - accuracy: 0.9760\n",
      "Epoch 00136: val_loss improved from 0.07934 to 0.07820, saving model to /.model\\136-0.078202.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.9768 - val_loss: 0.0782 - val_accuracy: 0.9772\n",
      "Epoch 137/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0731 - accuracy: 0.9800\n",
      "Epoch 00137: val_loss improved from 0.07820 to 0.07798, saving model to /.model\\137-0.077982.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9747 - val_loss: 0.0780 - val_accuracy: 0.9776\n",
      "Epoch 138/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1154 - accuracy: 0.9720\n",
      "Epoch 00138: val_loss did not improve from 0.07798\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0838 - accuracy: 0.9756 - val_loss: 0.0822 - val_accuracy: 0.9762\n",
      "Epoch 139/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1075 - accuracy: 0.9820\n",
      "Epoch 00139: val_loss did not improve from 0.07798\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.9754 - val_loss: 0.0828 - val_accuracy: 0.9748\n",
      "Epoch 140/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0882 - accuracy: 0.9800\n",
      "Epoch 00140: val_loss improved from 0.07798 to 0.07753, saving model to /.model\\140-0.077532.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0799 - accuracy: 0.9761 - val_loss: 0.0775 - val_accuracy: 0.9781\n",
      "Epoch 141/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0604 - accuracy: 0.9800\n",
      "Epoch 00141: val_loss did not improve from 0.07753\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0788 - accuracy: 0.9779 - val_loss: 0.0785 - val_accuracy: 0.9767\n",
      "Epoch 142/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0862 - accuracy: 0.9820\n",
      "Epoch 00142: val_loss did not improve from 0.07753\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9775 - val_loss: 0.0825 - val_accuracy: 0.9748\n",
      "Epoch 143/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0552 - accuracy: 0.9820\n",
      "Epoch 00143: val_loss improved from 0.07753 to 0.07543, saving model to /.model\\143-0.075428.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9782 - val_loss: 0.0754 - val_accuracy: 0.9767\n",
      "Epoch 144/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0749 - accuracy: 0.9840\n",
      "Epoch 00144: val_loss did not improve from 0.07543\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9770 - val_loss: 0.0777 - val_accuracy: 0.9725\n",
      "Epoch 145/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1003 - accuracy: 0.9760\n",
      "Epoch 00145: val_loss improved from 0.07543 to 0.07542, saving model to /.model\\145-0.075425.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.9759 - val_loss: 0.0754 - val_accuracy: 0.9748\n",
      "Epoch 146/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0911 - accuracy: 0.9700\n",
      "Epoch 00146: val_loss improved from 0.07542 to 0.07428, saving model to /.model\\146-0.074277.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9782 - val_loss: 0.0743 - val_accuracy: 0.9767\n",
      "Epoch 147/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1053 - accuracy: 0.9720\n",
      "Epoch 00147: val_loss improved from 0.07428 to 0.07400, saving model to /.model\\147-0.074003.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.9784 - val_loss: 0.0740 - val_accuracy: 0.9767\n",
      "Epoch 148/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0778 - accuracy: 0.9700\n",
      "Epoch 00148: val_loss improved from 0.07400 to 0.07391, saving model to /.model\\148-0.073910.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9773 - val_loss: 0.0739 - val_accuracy: 0.9776\n",
      "Epoch 149/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0536 - accuracy: 0.9860\n",
      "Epoch 00149: val_loss improved from 0.07391 to 0.07372, saving model to /.model\\149-0.073722.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9782 - val_loss: 0.0737 - val_accuracy: 0.9767\n",
      "Epoch 150/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0688 - accuracy: 0.9820\n",
      "Epoch 00150: val_loss did not improve from 0.07372\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9786 - val_loss: 0.0749 - val_accuracy: 0.9790\n",
      "Epoch 151/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0745 - accuracy: 0.9740\n",
      "Epoch 00151: val_loss did not improve from 0.07372\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.9782 - val_loss: 0.0755 - val_accuracy: 0.9781\n",
      "Epoch 152/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0484 - accuracy: 0.9820\n",
      "Epoch 00152: val_loss improved from 0.07372 to 0.07363, saving model to /.model\\152-0.073635.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9777 - val_loss: 0.0736 - val_accuracy: 0.9786\n",
      "Epoch 153/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0643 - accuracy: 0.9780\n",
      "Epoch 00153: val_loss improved from 0.07363 to 0.07359, saving model to /.model\\153-0.073587.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9789 - val_loss: 0.0736 - val_accuracy: 0.9790\n",
      "Epoch 154/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0649 - accuracy: 0.9800\n",
      "Epoch 00154: val_loss did not improve from 0.07359\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0754 - accuracy: 0.9777 - val_loss: 0.0764 - val_accuracy: 0.9762\n",
      "Epoch 155/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0974 - accuracy: 0.9580\n",
      "Epoch 00155: val_loss did not improve from 0.07359\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.9779 - val_loss: 0.0868 - val_accuracy: 0.9753\n",
      "Epoch 156/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0719 - accuracy: 0.9800\n",
      "Epoch 00156: val_loss did not improve from 0.07359\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9773 - val_loss: 0.0800 - val_accuracy: 0.9748\n",
      "Epoch 157/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0743 - accuracy: 0.9760\n",
      "Epoch 00157: val_loss did not improve from 0.07359\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9782 - val_loss: 0.0739 - val_accuracy: 0.9786\n",
      "Epoch 158/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0607 - accuracy: 0.9860\n",
      "Epoch 00158: val_loss improved from 0.07359 to 0.07151, saving model to /.model\\158-0.071513.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.9782 - val_loss: 0.0715 - val_accuracy: 0.9790\n",
      "Epoch 159/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0787 - accuracy: 0.9800\n",
      "Epoch 00159: val_loss improved from 0.07151 to 0.07144, saving model to /.model\\159-0.071442.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9793 - val_loss: 0.0714 - val_accuracy: 0.9776\n",
      "Epoch 160/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0686 - accuracy: 0.9820\n",
      "Epoch 00160: val_loss improved from 0.07144 to 0.07084, saving model to /.model\\160-0.070836.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9789 - val_loss: 0.0708 - val_accuracy: 0.9776\n",
      "Epoch 161/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0883 - accuracy: 0.9780\n",
      "Epoch 00161: val_loss did not improve from 0.07084\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0767 - accuracy: 0.9770 - val_loss: 0.0765 - val_accuracy: 0.9725\n",
      "Epoch 162/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0755 - accuracy: 0.9620\n",
      "Epoch 00162: val_loss improved from 0.07084 to 0.07023, saving model to /.model\\162-0.070234.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.9759 - val_loss: 0.0702 - val_accuracy: 0.9786\n",
      "Epoch 163/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9800\n",
      "Epoch 00163: val_loss did not improve from 0.07023\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9793 - val_loss: 0.0707 - val_accuracy: 0.9767\n",
      "Epoch 164/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0675 - accuracy: 0.9720\n",
      "Epoch 00164: val_loss did not improve from 0.07023\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9770 - val_loss: 0.0709 - val_accuracy: 0.9767\n",
      "Epoch 165/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0828 - accuracy: 0.9740\n",
      "Epoch 00165: val_loss did not improve from 0.07023\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9807 - val_loss: 0.0715 - val_accuracy: 0.9748\n",
      "Epoch 166/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0554 - accuracy: 0.9800\n",
      "Epoch 00166: val_loss improved from 0.07023 to 0.07018, saving model to /.model\\166-0.070181.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9784 - val_loss: 0.0702 - val_accuracy: 0.9776\n",
      "Epoch 167/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0694 - accuracy: 0.9800\n",
      "Epoch 00167: val_loss improved from 0.07018 to 0.06907, saving model to /.model\\167-0.069066.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9789 - val_loss: 0.0691 - val_accuracy: 0.9790\n",
      "Epoch 168/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0812 - accuracy: 0.9720\n",
      "Epoch 00168: val_loss improved from 0.06907 to 0.06844, saving model to /.model\\168-0.068440.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9782 - val_loss: 0.0684 - val_accuracy: 0.9800\n",
      "Epoch 169/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0970 - accuracy: 0.9800\n",
      "Epoch 00169: val_loss improved from 0.06844 to 0.06824, saving model to /.model\\169-0.068242.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.9802 - val_loss: 0.0682 - val_accuracy: 0.9795\n",
      "Epoch 170/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0632 - accuracy: 0.9880\n",
      "Epoch 00170: val_loss did not improve from 0.06824\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.9807 - val_loss: 0.0702 - val_accuracy: 0.9795\n",
      "Epoch 171/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0750 - accuracy: 0.9780\n",
      "Epoch 00171: val_loss improved from 0.06824 to 0.06816, saving model to /.model\\171-0.068159.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9798 - val_loss: 0.0682 - val_accuracy: 0.9804\n",
      "Epoch 172/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0775 - accuracy: 0.9820\n",
      "Epoch 00172: val_loss did not improve from 0.06816\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0705 - accuracy: 0.9807 - val_loss: 0.0689 - val_accuracy: 0.9795\n",
      "Epoch 173/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9840\n",
      "Epoch 00173: val_loss improved from 0.06816 to 0.06776, saving model to /.model\\173-0.067758.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9812 - val_loss: 0.0678 - val_accuracy: 0.9800\n",
      "Epoch 174/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9880\n",
      "Epoch 00174: val_loss did not improve from 0.06776\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9816 - val_loss: 0.0697 - val_accuracy: 0.9795\n",
      "Epoch 175/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0827 - accuracy: 0.9780\n",
      "Epoch 00175: val_loss did not improve from 0.06776\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0712 - accuracy: 0.9807 - val_loss: 0.0682 - val_accuracy: 0.9795\n",
      "Epoch 176/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0982 - accuracy: 0.9760\n",
      "Epoch 00176: val_loss did not improve from 0.06776\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0743 - accuracy: 0.9793 - val_loss: 0.0697 - val_accuracy: 0.9800\n",
      "Epoch 177/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0584 - accuracy: 0.9820\n",
      "Epoch 00177: val_loss improved from 0.06776 to 0.06692, saving model to /.model\\177-0.066917.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9789 - val_loss: 0.0669 - val_accuracy: 0.9804\n",
      "Epoch 178/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0427 - accuracy: 0.9820\n",
      "Epoch 00178: val_loss did not improve from 0.06692\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9800 - val_loss: 0.0677 - val_accuracy: 0.9800\n",
      "Epoch 179/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0687 - accuracy: 0.9800\n",
      "Epoch 00179: val_loss improved from 0.06692 to 0.06564, saving model to /.model\\179-0.065636.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9812 - val_loss: 0.0656 - val_accuracy: 0.9795\n",
      "Epoch 180/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0461 - accuracy: 0.9880\n",
      "Epoch 00180: val_loss improved from 0.06564 to 0.06554, saving model to /.model\\180-0.065542.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0690 - accuracy: 0.9812 - val_loss: 0.0655 - val_accuracy: 0.9809\n",
      "Epoch 181/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0458 - accuracy: 0.9860\n",
      "Epoch 00181: val_loss did not improve from 0.06554\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0688 - accuracy: 0.9800 - val_loss: 0.0673 - val_accuracy: 0.9772\n",
      "Epoch 182/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0510 - accuracy: 0.9860\n",
      "Epoch 00182: val_loss did not improve from 0.06554\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 0.9816 - val_loss: 0.0662 - val_accuracy: 0.9781\n",
      "Epoch 183/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0370 - accuracy: 0.9860\n",
      "Epoch 00183: val_loss improved from 0.06554 to 0.06551, saving model to /.model\\183-0.065511.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9798 - val_loss: 0.0655 - val_accuracy: 0.9809\n",
      "Epoch 184/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0438 - accuracy: 0.9880\n",
      "Epoch 00184: val_loss did not improve from 0.06551\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0682 - accuracy: 0.9814 - val_loss: 0.0707 - val_accuracy: 0.9795\n",
      "Epoch 185/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0854 - accuracy: 0.9800\n",
      "Epoch 00185: val_loss did not improve from 0.06551\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0693 - accuracy: 0.9807 - val_loss: 0.0662 - val_accuracy: 0.9800\n",
      "Epoch 186/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0554 - accuracy: 0.9800\n",
      "Epoch 00186: val_loss did not improve from 0.06551\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0679 - accuracy: 0.9809 - val_loss: 0.0666 - val_accuracy: 0.9804\n",
      "Epoch 187/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0687 - accuracy: 0.9760\n",
      "Epoch 00187: val_loss improved from 0.06551 to 0.06427, saving model to /.model\\187-0.064274.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0675 - accuracy: 0.9809 - val_loss: 0.0643 - val_accuracy: 0.9814\n",
      "Epoch 188/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0662 - accuracy: 0.9800\n",
      "Epoch 00188: val_loss improved from 0.06427 to 0.06389, saving model to /.model\\188-0.063895.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0688 - accuracy: 0.9816 - val_loss: 0.0639 - val_accuracy: 0.9814\n",
      "Epoch 189/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0440 - accuracy: 0.9860\n",
      "Epoch 00189: val_loss did not improve from 0.06389\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0665 - accuracy: 0.9821 - val_loss: 0.0642 - val_accuracy: 0.9804\n",
      "Epoch 190/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0727 - accuracy: 0.9700\n",
      "Epoch 00190: val_loss did not improve from 0.06389\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9823 - val_loss: 0.0655 - val_accuracy: 0.9809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0526 - accuracy: 0.9940\n",
      "Epoch 00191: val_loss did not improve from 0.06389\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0665 - accuracy: 0.9823 - val_loss: 0.0661 - val_accuracy: 0.9809\n",
      "Epoch 192/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0784 - accuracy: 0.9740\n",
      "Epoch 00192: val_loss improved from 0.06389 to 0.06344, saving model to /.model\\192-0.063437.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0669 - accuracy: 0.9821 - val_loss: 0.0634 - val_accuracy: 0.9818\n",
      "Epoch 193/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0551 - accuracy: 0.9880\n",
      "Epoch 00193: val_loss did not improve from 0.06344\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9830 - val_loss: 0.0635 - val_accuracy: 0.9814\n",
      "Epoch 194/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0586 - accuracy: 0.9820\n",
      "Epoch 00194: val_loss improved from 0.06344 to 0.06305, saving model to /.model\\194-0.063050.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9818 - val_loss: 0.0630 - val_accuracy: 0.9818\n",
      "Epoch 195/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0458 - accuracy: 0.9840\n",
      "Epoch 00195: val_loss improved from 0.06305 to 0.06231, saving model to /.model\\195-0.062311.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9807 - val_loss: 0.0623 - val_accuracy: 0.9809\n",
      "Epoch 196/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0682 - accuracy: 0.9840\n",
      "Epoch 00196: val_loss did not improve from 0.06231\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.9812 - val_loss: 0.0645 - val_accuracy: 0.9814\n",
      "Epoch 197/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0736 - accuracy: 0.9840\n",
      "Epoch 00197: val_loss improved from 0.06231 to 0.06222, saving model to /.model\\197-0.062224.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9825 - val_loss: 0.0622 - val_accuracy: 0.9814\n",
      "Epoch 198/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0959 - accuracy: 0.9820\n",
      "Epoch 00198: val_loss improved from 0.06222 to 0.06219, saving model to /.model\\198-0.062192.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 0.9828 - val_loss: 0.0622 - val_accuracy: 0.9823\n",
      "Epoch 199/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0543 - accuracy: 0.9800\n",
      "Epoch 00199: val_loss did not improve from 0.06219\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9809 - val_loss: 0.0623 - val_accuracy: 0.9814\n",
      "Epoch 200/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0779 - accuracy: 0.9800\n",
      "Epoch 00200: val_loss improved from 0.06219 to 0.06152, saving model to /.model\\200-0.061518.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9807 - val_loss: 0.0615 - val_accuracy: 0.9823\n",
      "Epoch 201/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0747 - accuracy: 0.9880\n",
      "Epoch 00201: val_loss did not improve from 0.06152\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9814 - val_loss: 0.0618 - val_accuracy: 0.9818\n",
      "Epoch 202/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0415 - accuracy: 0.9860\n",
      "Epoch 00202: val_loss did not improve from 0.06152\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9823 - val_loss: 0.0622 - val_accuracy: 0.9814\n",
      "Epoch 203/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0716 - accuracy: 0.9900\n",
      "Epoch 00203: val_loss did not improve from 0.06152\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0662 - accuracy: 0.9818 - val_loss: 0.0616 - val_accuracy: 0.9823\n",
      "Epoch 204/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0879 - accuracy: 0.9860\n",
      "Epoch 00204: val_loss did not improve from 0.06152\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9802 - val_loss: 0.0622 - val_accuracy: 0.9814\n",
      "Epoch 205/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0836 - accuracy: 0.9740\n",
      "Epoch 00205: val_loss did not improve from 0.06152\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9773 - val_loss: 0.0637 - val_accuracy: 0.9809\n",
      "Epoch 206/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0950 - accuracy: 0.9820\n",
      "Epoch 00206: val_loss improved from 0.06152 to 0.06125, saving model to /.model\\206-0.061253.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0675 - accuracy: 0.9814 - val_loss: 0.0613 - val_accuracy: 0.9823\n",
      "Epoch 207/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0419 - accuracy: 0.9820\n",
      "Epoch 00207: val_loss improved from 0.06125 to 0.06076, saving model to /.model\\207-0.060757.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9816 - val_loss: 0.0608 - val_accuracy: 0.9823\n",
      "Epoch 208/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0739 - accuracy: 0.9860\n",
      "Epoch 00208: val_loss did not improve from 0.06076\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.9825 - val_loss: 0.0687 - val_accuracy: 0.9804\n",
      "Epoch 209/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0891 - accuracy: 0.9780\n",
      "Epoch 00209: val_loss did not improve from 0.06076\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9816 - val_loss: 0.0645 - val_accuracy: 0.9823\n",
      "Epoch 210/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0566 - accuracy: 0.9800\n",
      "Epoch 00210: val_loss did not improve from 0.06076\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0639 - accuracy: 0.9821 - val_loss: 0.0612 - val_accuracy: 0.9814\n",
      "Epoch 211/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0363 - accuracy: 0.9900\n",
      "Epoch 00211: val_loss did not improve from 0.06076\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0634 - accuracy: 0.9828 - val_loss: 0.0612 - val_accuracy: 0.9814\n",
      "Epoch 212/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0629 - accuracy: 0.9820\n",
      "Epoch 00212: val_loss improved from 0.06076 to 0.06037, saving model to /.model\\212-0.060370.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0644 - accuracy: 0.9825 - val_loss: 0.0604 - val_accuracy: 0.9814\n",
      "Epoch 213/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0635 - accuracy: 0.9780\n",
      "Epoch 00213: val_loss improved from 0.06037 to 0.06011, saving model to /.model\\213-0.060111.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0632 - accuracy: 0.9825 - val_loss: 0.0601 - val_accuracy: 0.9814\n",
      "Epoch 214/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0331 - accuracy: 0.9920\n",
      "Epoch 00214: val_loss did not improve from 0.06011\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 0.9828 - val_loss: 0.0602 - val_accuracy: 0.9809\n",
      "Epoch 215/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0606 - accuracy: 0.9880\n",
      "Epoch 00215: val_loss did not improve from 0.06011\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9821 - val_loss: 0.0609 - val_accuracy: 0.9814\n",
      "Epoch 216/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0658 - accuracy: 0.9800\n",
      "Epoch 00216: val_loss improved from 0.06011 to 0.05908, saving model to /.model\\216-0.059084.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9816 - val_loss: 0.0591 - val_accuracy: 0.9828\n",
      "Epoch 217/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0518 - accuracy: 0.9880\n",
      "Epoch 00217: val_loss did not improve from 0.05908\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9835 - val_loss: 0.0595 - val_accuracy: 0.9814\n",
      "Epoch 218/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0615 - accuracy: 0.9840\n",
      "Epoch 00218: val_loss improved from 0.05908 to 0.05840, saving model to /.model\\218-0.058400.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9818 - val_loss: 0.0584 - val_accuracy: 0.9832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0483 - accuracy: 0.9840\n",
      "Epoch 00219: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0632 - accuracy: 0.9825 - val_loss: 0.0592 - val_accuracy: 0.9823\n",
      "Epoch 220/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0441 - accuracy: 0.9880\n",
      "Epoch 00220: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.9837 - val_loss: 0.0587 - val_accuracy: 0.9818\n",
      "Epoch 221/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1070 - accuracy: 0.9720\n",
      "Epoch 00221: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0623 - accuracy: 0.9835 - val_loss: 0.0600 - val_accuracy: 0.9818\n",
      "Epoch 222/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0411 - accuracy: 0.9860\n",
      "Epoch 00222: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9839 - val_loss: 0.0648 - val_accuracy: 0.9823\n",
      "Epoch 223/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0429 - accuracy: 0.9900\n",
      "Epoch 00223: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0641 - accuracy: 0.9825 - val_loss: 0.0603 - val_accuracy: 0.9823\n",
      "Epoch 224/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0623 - accuracy: 0.9860\n",
      "Epoch 00224: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9835 - val_loss: 0.0614 - val_accuracy: 0.9818\n",
      "Epoch 225/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0673 - accuracy: 0.9800\n",
      "Epoch 00225: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0627 - accuracy: 0.9835 - val_loss: 0.0595 - val_accuracy: 0.9818\n",
      "Epoch 226/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0580 - accuracy: 0.9740\n",
      "Epoch 00226: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0636 - accuracy: 0.9832 - val_loss: 0.0593 - val_accuracy: 0.9818\n",
      "Epoch 227/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0463 - accuracy: 0.9820\n",
      "Epoch 00227: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9830 - val_loss: 0.0584 - val_accuracy: 0.9832\n",
      "Epoch 228/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0374 - accuracy: 0.9840\n",
      "Epoch 00228: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9825 - val_loss: 0.0585 - val_accuracy: 0.9823\n",
      "Epoch 229/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0683 - accuracy: 0.9840\n",
      "Epoch 00229: val_loss did not improve from 0.05840\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9823 - val_loss: 0.0614 - val_accuracy: 0.9828\n",
      "Epoch 230/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0574 - accuracy: 0.9780\n",
      "Epoch 00230: val_loss improved from 0.05840 to 0.05780, saving model to /.model\\230-0.057805.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9821 - val_loss: 0.0578 - val_accuracy: 0.9832\n",
      "Epoch 231/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0500 - accuracy: 0.9800\n",
      "Epoch 00231: val_loss did not improve from 0.05780\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9823 - val_loss: 0.0592 - val_accuracy: 0.9828\n",
      "Epoch 232/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0391 - accuracy: 0.9880\n",
      "Epoch 00232: val_loss did not improve from 0.05780\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0648 - accuracy: 0.9828 - val_loss: 0.0740 - val_accuracy: 0.9786\n",
      "Epoch 233/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0641 - accuracy: 0.9800\n",
      "Epoch 00233: val_loss improved from 0.05780 to 0.05722, saving model to /.model\\233-0.057223.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9823 - val_loss: 0.0572 - val_accuracy: 0.9828\n",
      "Epoch 234/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0580 - accuracy: 0.9880\n",
      "Epoch 00234: val_loss improved from 0.05722 to 0.05712, saving model to /.model\\234-0.057118.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9846 - val_loss: 0.0571 - val_accuracy: 0.9828\n",
      "Epoch 235/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0989 - accuracy: 0.9760\n",
      "Epoch 00235: val_loss improved from 0.05712 to 0.05683, saving model to /.model\\235-0.056828.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9828 - val_loss: 0.0568 - val_accuracy: 0.9828\n",
      "Epoch 236/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0739 - accuracy: 0.9880\n",
      "Epoch 00236: val_loss did not improve from 0.05683\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9858 - val_loss: 0.0609 - val_accuracy: 0.9786\n",
      "Epoch 237/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0938 - accuracy: 0.9820\n",
      "Epoch 00237: val_loss improved from 0.05683 to 0.05676, saving model to /.model\\237-0.056758.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9818 - val_loss: 0.0568 - val_accuracy: 0.9832\n",
      "Epoch 238/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0543 - accuracy: 0.9860\n",
      "Epoch 00238: val_loss improved from 0.05676 to 0.05597, saving model to /.model\\238-0.055966.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9837 - val_loss: 0.0560 - val_accuracy: 0.9832\n",
      "Epoch 239/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0494 - accuracy: 0.9780\n",
      "Epoch 00239: val_loss did not improve from 0.05597\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9825 - val_loss: 0.0567 - val_accuracy: 0.9832\n",
      "Epoch 240/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1362 - accuracy: 0.9680\n",
      "Epoch 00240: val_loss did not improve from 0.05597\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.9814 - val_loss: 0.0581 - val_accuracy: 0.9814\n",
      "Epoch 241/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0630 - accuracy: 0.9820\n",
      "Epoch 00241: val_loss did not improve from 0.05597\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9828 - val_loss: 0.0584 - val_accuracy: 0.9823\n",
      "Epoch 242/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0889 - accuracy: 0.9740\n",
      "Epoch 00242: val_loss did not improve from 0.05597\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9828 - val_loss: 0.0560 - val_accuracy: 0.9832\n",
      "Epoch 243/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0651 - accuracy: 0.9760\n",
      "Epoch 00243: val_loss improved from 0.05597 to 0.05492, saving model to /.model\\243-0.054919.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 0.9839 - val_loss: 0.0549 - val_accuracy: 0.9832\n",
      "Epoch 244/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0728 - accuracy: 0.9820\n",
      "Epoch 00244: val_loss did not improve from 0.05492\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0591 - accuracy: 0.9841 - val_loss: 0.0566 - val_accuracy: 0.9832\n",
      "Epoch 245/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0486 - accuracy: 0.9860\n",
      "Epoch 00245: val_loss did not improve from 0.05492\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9835 - val_loss: 0.0557 - val_accuracy: 0.9832\n",
      "Epoch 246/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0672 - accuracy: 0.9800\n",
      "Epoch 00246: val_loss did not improve from 0.05492\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9835 - val_loss: 0.0566 - val_accuracy: 0.9828\n",
      "Epoch 247/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1098 - accuracy: 0.9760\n",
      "Epoch 00247: val_loss did not improve from 0.05492\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9844 - val_loss: 0.0591 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0549 - accuracy: 0.9880\n",
      "Epoch 00248: val_loss did not improve from 0.05492\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9814 - val_loss: 0.0571 - val_accuracy: 0.9823\n",
      "Epoch 249/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0593 - accuracy: 0.9800\n",
      "Epoch 00249: val_loss did not improve from 0.05492\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9839 - val_loss: 0.0552 - val_accuracy: 0.9841\n",
      "Epoch 250/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0526 - accuracy: 0.9820\n",
      "Epoch 00250: val_loss did not improve from 0.05492\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0602 - accuracy: 0.9841 - val_loss: 0.0573 - val_accuracy: 0.9846\n",
      "Epoch 251/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0387 - accuracy: 0.9940\n",
      "Epoch 00251: val_loss did not improve from 0.05492\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9855 - val_loss: 0.0558 - val_accuracy: 0.9837\n",
      "Epoch 252/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0561 - accuracy: 0.9840\n",
      "Epoch 00252: val_loss improved from 0.05492 to 0.05384, saving model to /.model\\252-0.053840.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9844 - val_loss: 0.0538 - val_accuracy: 0.9851\n",
      "Epoch 253/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0598 - accuracy: 0.9800\n",
      "Epoch 00253: val_loss did not improve from 0.05384\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9853 - val_loss: 0.0559 - val_accuracy: 0.9841\n",
      "Epoch 254/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0339 - accuracy: 0.9900\n",
      "Epoch 00254: val_loss did not improve from 0.05384\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9832 - val_loss: 0.0604 - val_accuracy: 0.9837\n",
      "Epoch 255/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0362 - accuracy: 0.9960\n",
      "Epoch 00255: val_loss improved from 0.05384 to 0.05362, saving model to /.model\\255-0.053620.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9851 - val_loss: 0.0536 - val_accuracy: 0.9851\n",
      "Epoch 256/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0540 - accuracy: 0.9840\n",
      "Epoch 00256: val_loss did not improve from 0.05362\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 0.9846 - val_loss: 0.0546 - val_accuracy: 0.9841\n",
      "Epoch 257/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0389 - accuracy: 0.9800\n",
      "Epoch 00257: val_loss improved from 0.05362 to 0.05325, saving model to /.model\\257-0.053247.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9839 - val_loss: 0.0532 - val_accuracy: 0.9851\n",
      "Epoch 258/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0360 - accuracy: 0.9880\n",
      "Epoch 00258: val_loss did not improve from 0.05325\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9851 - val_loss: 0.0552 - val_accuracy: 0.9860\n",
      "Epoch 259/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9900\n",
      "Epoch 00259: val_loss did not improve from 0.05325\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9848 - val_loss: 0.0569 - val_accuracy: 0.9837\n",
      "Epoch 260/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0610 - accuracy: 0.9860\n",
      "Epoch 00260: val_loss improved from 0.05325 to 0.05268, saving model to /.model\\260-0.052680.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9841 - val_loss: 0.0527 - val_accuracy: 0.9851\n",
      "Epoch 261/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0640 - accuracy: 0.9920\n",
      "Epoch 00261: val_loss did not improve from 0.05268\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9846 - val_loss: 0.0534 - val_accuracy: 0.9841\n",
      "Epoch 262/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0768 - accuracy: 0.9760\n",
      "Epoch 00262: val_loss did not improve from 0.05268\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9848 - val_loss: 0.0553 - val_accuracy: 0.9832\n",
      "Epoch 263/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0786 - accuracy: 0.9820\n",
      "Epoch 00263: val_loss improved from 0.05268 to 0.05245, saving model to /.model\\263-0.052446.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 0.9844 - val_loss: 0.0524 - val_accuracy: 0.9851\n",
      "Epoch 264/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0724 - accuracy: 0.9840\n",
      "Epoch 00264: val_loss did not improve from 0.05245\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9848 - val_loss: 0.0528 - val_accuracy: 0.9846\n",
      "Epoch 265/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0413 - accuracy: 0.9880\n",
      "Epoch 00265: val_loss improved from 0.05245 to 0.05238, saving model to /.model\\265-0.052383.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9848 - val_loss: 0.0524 - val_accuracy: 0.9851\n",
      "Epoch 266/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0613 - accuracy: 0.9860\n",
      "Epoch 00266: val_loss did not improve from 0.05238\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9851 - val_loss: 0.0543 - val_accuracy: 0.9846\n",
      "Epoch 267/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0420 - accuracy: 0.9860\n",
      "Epoch 00267: val_loss did not improve from 0.05238\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9860 - val_loss: 0.0649 - val_accuracy: 0.9814\n",
      "Epoch 268/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0754 - accuracy: 0.9800\n",
      "Epoch 00268: val_loss did not improve from 0.05238\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9841 - val_loss: 0.0694 - val_accuracy: 0.9786\n",
      "Epoch 269/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0496 - accuracy: 0.9780\n",
      "Epoch 00269: val_loss did not improve from 0.05238\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9835 - val_loss: 0.0630 - val_accuracy: 0.9823\n",
      "Epoch 270/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0890 - accuracy: 0.9740\n",
      "Epoch 00270: val_loss did not improve from 0.05238\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9837 - val_loss: 0.0524 - val_accuracy: 0.9851\n",
      "Epoch 271/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0954 - accuracy: 0.9820\n",
      "Epoch 00271: val_loss did not improve from 0.05238\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9844 - val_loss: 0.0527 - val_accuracy: 0.9846\n",
      "Epoch 272/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0636 - accuracy: 0.9820\n",
      "Epoch 00272: val_loss improved from 0.05238 to 0.05206, saving model to /.model\\272-0.052058.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9848 - val_loss: 0.0521 - val_accuracy: 0.9851\n",
      "Epoch 273/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1315 - accuracy: 0.9740\n",
      "Epoch 00273: val_loss did not improve from 0.05206\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9814 - val_loss: 0.0523 - val_accuracy: 0.9851\n",
      "Epoch 274/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0645 - accuracy: 0.9840\n",
      "Epoch 00274: val_loss did not improve from 0.05206\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9851 - val_loss: 0.0529 - val_accuracy: 0.9846\n",
      "Epoch 275/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0370 - accuracy: 0.9860\n",
      "Epoch 00275: val_loss improved from 0.05206 to 0.05204, saving model to /.model\\275-0.052036.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0560 - accuracy: 0.9841 - val_loss: 0.0520 - val_accuracy: 0.9851\n",
      "Epoch 276/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0381 - accuracy: 0.9860\n",
      "Epoch 00276: val_loss did not improve from 0.05204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0568 - accuracy: 0.9848 - val_loss: 0.0528 - val_accuracy: 0.9851\n",
      "Epoch 277/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0428 - accuracy: 0.9920\n",
      "Epoch 00277: val_loss did not improve from 0.05204\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9832 - val_loss: 0.0574 - val_accuracy: 0.9809\n",
      "Epoch 278/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0805 - accuracy: 0.9800\n",
      "Epoch 00278: val_loss did not improve from 0.05204\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0611 - accuracy: 0.9844 - val_loss: 0.0528 - val_accuracy: 0.9846\n",
      "Epoch 279/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0396 - accuracy: 0.9840\n",
      "Epoch 00279: val_loss improved from 0.05204 to 0.05190, saving model to /.model\\279-0.051898.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9848 - val_loss: 0.0519 - val_accuracy: 0.9846\n",
      "Epoch 280/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0679 - accuracy: 0.9800\n",
      "Epoch 00280: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9844 - val_loss: 0.0521 - val_accuracy: 0.9851\n",
      "Epoch 281/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0626 - accuracy: 0.9840\n",
      "Epoch 00281: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0579 - accuracy: 0.9844 - val_loss: 0.0534 - val_accuracy: 0.9841\n",
      "Epoch 282/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0881 - accuracy: 0.9820\n",
      "Epoch 00282: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9860 - val_loss: 0.0571 - val_accuracy: 0.9818\n",
      "Epoch 283/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0301 - accuracy: 0.9880\n",
      "Epoch 00283: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9841 - val_loss: 0.0531 - val_accuracy: 0.9851\n",
      "Epoch 284/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0420 - accuracy: 0.9820\n",
      "Epoch 00284: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9851 - val_loss: 0.0530 - val_accuracy: 0.9841\n",
      "Epoch 285/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0340 - accuracy: 0.9880\n",
      "Epoch 00285: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0570 - accuracy: 0.9846 - val_loss: 0.0567 - val_accuracy: 0.9809\n",
      "Epoch 286/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0631 - accuracy: 0.9880\n",
      "Epoch 00286: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0590 - accuracy: 0.9837 - val_loss: 0.0567 - val_accuracy: 0.9814\n",
      "Epoch 287/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0635 - accuracy: 0.9800\n",
      "Epoch 00287: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9835 - val_loss: 0.0524 - val_accuracy: 0.9851\n",
      "Epoch 288/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0685 - accuracy: 0.9800\n",
      "Epoch 00288: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9844 - val_loss: 0.0550 - val_accuracy: 0.9855\n",
      "Epoch 289/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0954 - accuracy: 0.9780\n",
      "Epoch 00289: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9855 - val_loss: 0.0522 - val_accuracy: 0.9860\n",
      "Epoch 290/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0332 - accuracy: 0.9920\n",
      "Epoch 00290: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9851 - val_loss: 0.0541 - val_accuracy: 0.9855\n",
      "Epoch 291/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0708 - accuracy: 0.9820\n",
      "Epoch 00291: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9841 - val_loss: 0.0522 - val_accuracy: 0.9855\n",
      "Epoch 292/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0657 - accuracy: 0.9880\n",
      "Epoch 00292: val_loss did not improve from 0.05190\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9844 - val_loss: 0.0528 - val_accuracy: 0.9846\n",
      "Epoch 293/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0678 - accuracy: 0.9780\n",
      "Epoch 00293: val_loss improved from 0.05190 to 0.05148, saving model to /.model\\293-0.051481.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9844 - val_loss: 0.0515 - val_accuracy: 0.9851\n",
      "Epoch 294/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0340 - accuracy: 0.9940\n",
      "Epoch 00294: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0542 - accuracy: 0.9858 - val_loss: 0.0548 - val_accuracy: 0.9855\n",
      "Epoch 295/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0601 - accuracy: 0.9860\n",
      "Epoch 00295: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9851 - val_loss: 0.0538 - val_accuracy: 0.9851\n",
      "Epoch 296/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0709 - accuracy: 0.9820\n",
      "Epoch 00296: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.9851 - val_loss: 0.0518 - val_accuracy: 0.9860\n",
      "Epoch 297/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0579 - accuracy: 0.9820\n",
      "Epoch 00297: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9851 - val_loss: 0.0530 - val_accuracy: 0.9860\n",
      "Epoch 298/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0663 - accuracy: 0.9800\n",
      "Epoch 00298: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9860 - val_loss: 0.0516 - val_accuracy: 0.9855\n",
      "Epoch 299/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0419 - accuracy: 0.9840\n",
      "Epoch 00299: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9837 - val_loss: 0.0529 - val_accuracy: 0.9846\n",
      "Epoch 300/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0651 - accuracy: 0.9860\n",
      "Epoch 00300: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9855 - val_loss: 0.0516 - val_accuracy: 0.9851\n",
      "Epoch 301/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0338 - accuracy: 0.9860\n",
      "Epoch 00301: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9846 - val_loss: 0.0553 - val_accuracy: 0.9851\n",
      "Epoch 302/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0610 - accuracy: 0.9860\n",
      "Epoch 00302: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9846 - val_loss: 0.0548 - val_accuracy: 0.9828\n",
      "Epoch 303/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0752 - accuracy: 0.9800\n",
      "Epoch 00303: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9869 - val_loss: 0.0560 - val_accuracy: 0.9832\n",
      "Epoch 304/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0323 - accuracy: 0.9900\n",
      "Epoch 00304: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0579 - accuracy: 0.9876 - val_loss: 0.0524 - val_accuracy: 0.9851\n",
      "Epoch 305/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0826 - accuracy: 0.9780\n",
      "Epoch 00305: val_loss did not improve from 0.05148\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0542 - accuracy: 0.9853 - val_loss: 0.0527 - val_accuracy: 0.9841\n",
      "Epoch 306/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9840\n",
      "Epoch 00306: val_loss improved from 0.05148 to 0.05125, saving model to /.model\\306-0.051255.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9851 - val_loss: 0.0513 - val_accuracy: 0.9851\n",
      "Epoch 307/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0707 - accuracy: 0.9820\n",
      "Epoch 00307: val_loss did not improve from 0.05125\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9846 - val_loss: 0.0524 - val_accuracy: 0.9860\n",
      "Epoch 308/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0518 - accuracy: 0.9820\n",
      "Epoch 00308: val_loss did not improve from 0.05125\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9848 - val_loss: 0.0517 - val_accuracy: 0.9860\n",
      "Epoch 309/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0341 - accuracy: 0.9880\n",
      "Epoch 00309: val_loss improved from 0.05125 to 0.05121, saving model to /.model\\309-0.051214.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.9858 - val_loss: 0.0512 - val_accuracy: 0.9855\n",
      "Epoch 310/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0422 - accuracy: 0.9880\n",
      "Epoch 00310: val_loss did not improve from 0.05121\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9846 - val_loss: 0.0589 - val_accuracy: 0.9851\n",
      "Epoch 311/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0525 - accuracy: 0.9860\n",
      "Epoch 00311: val_loss improved from 0.05121 to 0.05105, saving model to /.model\\311-0.051052.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9860 - val_loss: 0.0511 - val_accuracy: 0.9855\n",
      "Epoch 312/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0431 - accuracy: 0.9860\n",
      "Epoch 00312: val_loss did not improve from 0.05105\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9855 - val_loss: 0.0552 - val_accuracy: 0.9855\n",
      "Epoch 313/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.1046 - accuracy: 0.9680\n",
      "Epoch 00313: val_loss did not improve from 0.05105\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9864 - val_loss: 0.0511 - val_accuracy: 0.9860\n",
      "Epoch 314/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0514 - accuracy: 0.9900\n",
      "Epoch 00314: val_loss improved from 0.05105 to 0.05057, saving model to /.model\\314-0.050566.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9869 - val_loss: 0.0506 - val_accuracy: 0.9851\n",
      "Epoch 315/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0379 - accuracy: 0.9920\n",
      "Epoch 00315: val_loss did not improve from 0.05057\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9855 - val_loss: 0.0508 - val_accuracy: 0.9851\n",
      "Epoch 316/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9900\n",
      "Epoch 00316: val_loss did not improve from 0.05057\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9851 - val_loss: 0.0509 - val_accuracy: 0.9851\n",
      "Epoch 317/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0282 - accuracy: 0.9940\n",
      "Epoch 00317: val_loss did not improve from 0.05057\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0541 - accuracy: 0.9869 - val_loss: 0.0512 - val_accuracy: 0.9865\n",
      "Epoch 318/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0610 - accuracy: 0.9760\n",
      "Epoch 00318: val_loss did not improve from 0.05057\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9853 - val_loss: 0.0528 - val_accuracy: 0.9841\n",
      "Epoch 319/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0710 - accuracy: 0.9840\n",
      "Epoch 00319: val_loss did not improve from 0.05057\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9860 - val_loss: 0.0506 - val_accuracy: 0.9851\n",
      "Epoch 320/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0773 - accuracy: 0.9800\n",
      "Epoch 00320: val_loss did not improve from 0.05057\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9864 - val_loss: 0.0511 - val_accuracy: 0.9851\n",
      "Epoch 321/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0780 - accuracy: 0.9860\n",
      "Epoch 00321: val_loss did not improve from 0.05057\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9855 - val_loss: 0.0508 - val_accuracy: 0.9855\n",
      "Epoch 322/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0404 - accuracy: 0.9900\n",
      "Epoch 00322: val_loss improved from 0.05057 to 0.04994, saving model to /.model\\322-0.049943.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9867 - val_loss: 0.0499 - val_accuracy: 0.9851\n",
      "Epoch 323/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0793 - accuracy: 0.9860\n",
      "Epoch 00323: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9848 - val_loss: 0.0532 - val_accuracy: 0.9841\n",
      "Epoch 324/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0235 - accuracy: 0.9940\n",
      "Epoch 00324: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9851 - val_loss: 0.0555 - val_accuracy: 0.9828\n",
      "Epoch 325/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0857 - accuracy: 0.9740\n",
      "Epoch 00325: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0567 - accuracy: 0.9837 - val_loss: 0.0506 - val_accuracy: 0.9851\n",
      "Epoch 326/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0649 - accuracy: 0.9920\n",
      "Epoch 00326: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9862 - val_loss: 0.0511 - val_accuracy: 0.9855\n",
      "Epoch 327/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0395 - accuracy: 0.9900\n",
      "Epoch 00327: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 0.9853 - val_loss: 0.0505 - val_accuracy: 0.9855\n",
      "Epoch 328/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9940\n",
      "Epoch 00328: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9869 - val_loss: 0.0508 - val_accuracy: 0.9855\n",
      "Epoch 329/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0283 - accuracy: 0.9920\n",
      "Epoch 00329: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0522 - accuracy: 0.9862 - val_loss: 0.0505 - val_accuracy: 0.9855\n",
      "Epoch 330/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9940\n",
      "Epoch 00330: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9858 - val_loss: 0.0509 - val_accuracy: 0.9865\n",
      "Epoch 331/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0579 - accuracy: 0.9800\n",
      "Epoch 00331: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9858 - val_loss: 0.0500 - val_accuracy: 0.9860\n",
      "Epoch 332/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0504 - accuracy: 0.9860\n",
      "Epoch 00332: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9864 - val_loss: 0.0531 - val_accuracy: 0.9846\n",
      "Epoch 333/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0912 - accuracy: 0.9820\n",
      "Epoch 00333: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9864 - val_loss: 0.0505 - val_accuracy: 0.9855\n",
      "Epoch 334/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0452 - accuracy: 0.9900\n",
      "Epoch 00334: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9867 - val_loss: 0.0540 - val_accuracy: 0.9841\n",
      "Epoch 335/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0487 - accuracy: 0.9860\n",
      "Epoch 00335: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9846 - val_loss: 0.0538 - val_accuracy: 0.9855\n",
      "Epoch 336/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0634 - accuracy: 0.9860\n",
      "Epoch 00336: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9846 - val_loss: 0.0535 - val_accuracy: 0.9846\n",
      "Epoch 337/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9860\n",
      "Epoch 00337: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9853 - val_loss: 0.0510 - val_accuracy: 0.9855\n",
      "Epoch 338/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0361 - accuracy: 0.9880\n",
      "Epoch 00338: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9864 - val_loss: 0.0504 - val_accuracy: 0.9855\n",
      "Epoch 339/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0425 - accuracy: 0.9880\n",
      "Epoch 00339: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9855 - val_loss: 0.0526 - val_accuracy: 0.9860\n",
      "Epoch 340/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0340 - accuracy: 0.9880\n",
      "Epoch 00340: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9867 - val_loss: 0.0503 - val_accuracy: 0.9855\n",
      "Epoch 341/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0325 - accuracy: 0.9860\n",
      "Epoch 00341: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9862 - val_loss: 0.0548 - val_accuracy: 0.9837\n",
      "Epoch 342/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0619 - accuracy: 0.9760\n",
      "Epoch 00342: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.9846 - val_loss: 0.0514 - val_accuracy: 0.9855\n",
      "Epoch 343/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0594 - accuracy: 0.9800\n",
      "Epoch 00343: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9844 - val_loss: 0.0609 - val_accuracy: 0.9795\n",
      "Epoch 344/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0698 - accuracy: 0.9780\n",
      "Epoch 00344: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0604 - accuracy: 0.9835 - val_loss: 0.0543 - val_accuracy: 0.9860\n",
      "Epoch 345/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0387 - accuracy: 0.9860\n",
      "Epoch 00345: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0546 - accuracy: 0.9867 - val_loss: 0.0502 - val_accuracy: 0.9851\n",
      "Epoch 346/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0666 - accuracy: 0.9720\n",
      "Epoch 00346: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9858 - val_loss: 0.0525 - val_accuracy: 0.9855\n",
      "Epoch 347/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0804 - accuracy: 0.9820\n",
      "Epoch 00347: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9869 - val_loss: 0.0512 - val_accuracy: 0.9855\n",
      "Epoch 348/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0868 - accuracy: 0.9840\n",
      "Epoch 00348: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0515 - accuracy: 0.9860 - val_loss: 0.0562 - val_accuracy: 0.9814\n",
      "Epoch 349/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0656 - accuracy: 0.9800\n",
      "Epoch 00349: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9860 - val_loss: 0.0530 - val_accuracy: 0.9860\n",
      "Epoch 350/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0584 - accuracy: 0.9840\n",
      "Epoch 00350: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9832 - val_loss: 0.0519 - val_accuracy: 0.9860\n",
      "Epoch 351/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9900\n",
      "Epoch 00351: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9855 - val_loss: 0.0525 - val_accuracy: 0.9860\n",
      "Epoch 352/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0361 - accuracy: 0.9880\n",
      "Epoch 00352: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9874 - val_loss: 0.0562 - val_accuracy: 0.9846\n",
      "Epoch 353/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9840\n",
      "Epoch 00353: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9862 - val_loss: 0.0536 - val_accuracy: 0.9851\n",
      "Epoch 354/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0815 - accuracy: 0.9820\n",
      "Epoch 00354: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9864 - val_loss: 0.0502 - val_accuracy: 0.9855\n",
      "Epoch 355/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0641 - accuracy: 0.9820\n",
      "Epoch 00355: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9862 - val_loss: 0.0505 - val_accuracy: 0.9855\n",
      "Epoch 356/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0641 - accuracy: 0.9900\n",
      "Epoch 00356: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9864 - val_loss: 0.0502 - val_accuracy: 0.9855\n",
      "Epoch 357/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0467 - accuracy: 0.9880\n",
      "Epoch 00357: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.9876 - val_loss: 0.0514 - val_accuracy: 0.9855\n",
      "Epoch 358/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0774 - accuracy: 0.9840\n",
      "Epoch 00358: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0516 - accuracy: 0.9862 - val_loss: 0.0507 - val_accuracy: 0.9865\n",
      "Epoch 359/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0404 - accuracy: 0.9880\n",
      "Epoch 00359: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9837 - val_loss: 0.0583 - val_accuracy: 0.9814\n",
      "Epoch 360/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0637 - accuracy: 0.9740\n",
      "Epoch 00360: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9830 - val_loss: 0.0598 - val_accuracy: 0.9804\n",
      "Epoch 361/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0850 - accuracy: 0.9780\n",
      "Epoch 00361: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0589 - accuracy: 0.9846 - val_loss: 0.0562 - val_accuracy: 0.9818\n",
      "Epoch 362/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0674 - accuracy: 0.9720\n",
      "Epoch 00362: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9837 - val_loss: 0.0512 - val_accuracy: 0.9869\n",
      "Epoch 363/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0500 - accuracy: 0.9820\n",
      "Epoch 00363: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0575 - accuracy: 0.9832 - val_loss: 0.0509 - val_accuracy: 0.9851\n",
      "Epoch 364/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0667 - accuracy: 0.9860\n",
      "Epoch 00364: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.9860 - val_loss: 0.0543 - val_accuracy: 0.9860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0728 - accuracy: 0.9880\n",
      "Epoch 00365: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9862 - val_loss: 0.0551 - val_accuracy: 0.9851\n",
      "Epoch 366/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9880\n",
      "Epoch 00366: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9858 - val_loss: 0.0548 - val_accuracy: 0.9855\n",
      "Epoch 367/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0498 - accuracy: 0.9800\n",
      "Epoch 00367: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9853 - val_loss: 0.0546 - val_accuracy: 0.9865\n",
      "Epoch 368/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0272 - accuracy: 0.9940\n",
      "Epoch 00368: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9848 - val_loss: 0.0670 - val_accuracy: 0.9804\n",
      "Epoch 369/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0649 - accuracy: 0.9840\n",
      "Epoch 00369: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9862 - val_loss: 0.0507 - val_accuracy: 0.9851\n",
      "Epoch 370/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0572 - accuracy: 0.9900\n",
      "Epoch 00370: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9874 - val_loss: 0.0516 - val_accuracy: 0.9860\n",
      "Epoch 371/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0477 - accuracy: 0.9900\n",
      "Epoch 00371: val_loss did not improve from 0.04994\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9864 - val_loss: 0.0507 - val_accuracy: 0.9869\n",
      "Epoch 372/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9900\n",
      "Epoch 00372: val_loss improved from 0.04994 to 0.04955, saving model to /.model\\372-0.049549.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 0.9869 - val_loss: 0.0495 - val_accuracy: 0.9860\n",
      "Epoch 373/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9820\n",
      "Epoch 00373: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9867 - val_loss: 0.0497 - val_accuracy: 0.9869\n",
      "Epoch 374/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0212 - accuracy: 0.9960\n",
      "Epoch 00374: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9876 - val_loss: 0.0502 - val_accuracy: 0.9865\n",
      "Epoch 375/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0373 - accuracy: 0.9860\n",
      "Epoch 00375: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9858 - val_loss: 0.0496 - val_accuracy: 0.9865\n",
      "Epoch 376/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0434 - accuracy: 0.9900\n",
      "Epoch 00376: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9862 - val_loss: 0.0504 - val_accuracy: 0.9865\n",
      "Epoch 377/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0807 - accuracy: 0.9860\n",
      "Epoch 00377: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9862 - val_loss: 0.0496 - val_accuracy: 0.9860\n",
      "Epoch 378/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0437 - accuracy: 0.9940\n",
      "Epoch 00378: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9871 - val_loss: 0.0549 - val_accuracy: 0.9855\n",
      "Epoch 379/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0698 - accuracy: 0.9840\n",
      "Epoch 00379: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0506 - accuracy: 0.9860 - val_loss: 0.0553 - val_accuracy: 0.9855\n",
      "Epoch 380/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0437 - accuracy: 0.9840\n",
      "Epoch 00380: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9851 - val_loss: 0.0622 - val_accuracy: 0.9832\n",
      "Epoch 381/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0975 - accuracy: 0.9780\n",
      "Epoch 00381: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9823 - val_loss: 0.0526 - val_accuracy: 0.9846\n",
      "Epoch 382/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0718 - accuracy: 0.9820\n",
      "Epoch 00382: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9853 - val_loss: 0.0520 - val_accuracy: 0.9855\n",
      "Epoch 383/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0443 - accuracy: 0.9800\n",
      "Epoch 00383: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9839 - val_loss: 0.0529 - val_accuracy: 0.9855\n",
      "Epoch 384/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0378 - accuracy: 0.9820\n",
      "Epoch 00384: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9851 - val_loss: 0.0517 - val_accuracy: 0.9855\n",
      "Epoch 385/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0427 - accuracy: 0.9900\n",
      "Epoch 00385: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9860 - val_loss: 0.0530 - val_accuracy: 0.9865\n",
      "Epoch 386/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0650 - accuracy: 0.9800\n",
      "Epoch 00386: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.9858 - val_loss: 0.0505 - val_accuracy: 0.9865\n",
      "Epoch 387/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0926 - accuracy: 0.9820\n",
      "Epoch 00387: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9867 - val_loss: 0.0505 - val_accuracy: 0.9855\n",
      "Epoch 388/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0515 - accuracy: 0.9860\n",
      "Epoch 00388: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 0.9862 - val_loss: 0.0519 - val_accuracy: 0.9869\n",
      "Epoch 389/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0539 - accuracy: 0.9860\n",
      "Epoch 00389: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9871 - val_loss: 0.0526 - val_accuracy: 0.9869\n",
      "Epoch 390/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0443 - accuracy: 0.9920\n",
      "Epoch 00390: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0506 - accuracy: 0.9881 - val_loss: 0.0508 - val_accuracy: 0.9865\n",
      "Epoch 391/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0539 - accuracy: 0.9900\n",
      "Epoch 00391: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9855 - val_loss: 0.0506 - val_accuracy: 0.9865\n",
      "Epoch 392/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0836 - accuracy: 0.9800\n",
      "Epoch 00392: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9858 - val_loss: 0.0504 - val_accuracy: 0.9865\n",
      "Epoch 393/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0525 - accuracy: 0.9800\n",
      "Epoch 00393: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9867 - val_loss: 0.0503 - val_accuracy: 0.9869\n",
      "Epoch 394/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0414 - accuracy: 0.9780\n",
      "Epoch 00394: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9871 - val_loss: 0.0542 - val_accuracy: 0.9860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0420 - accuracy: 0.9840\n",
      "Epoch 00395: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9876 - val_loss: 0.0503 - val_accuracy: 0.9855\n",
      "Epoch 396/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0734 - accuracy: 0.9860\n",
      "Epoch 00396: val_loss did not improve from 0.04955\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9851 - val_loss: 0.0499 - val_accuracy: 0.9865\n",
      "Epoch 397/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0491 - accuracy: 0.9880\n",
      "Epoch 00397: val_loss improved from 0.04955 to 0.04943, saving model to /.model\\397-0.049426.hdf5\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9874 - val_loss: 0.0494 - val_accuracy: 0.9865\n",
      "Epoch 398/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0463 - accuracy: 0.9920\n",
      "Epoch 00398: val_loss did not improve from 0.04943\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9874 - val_loss: 0.0507 - val_accuracy: 0.9865\n",
      "Epoch 399/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0581 - accuracy: 0.9860\n",
      "Epoch 00399: val_loss improved from 0.04943 to 0.04910, saving model to /.model\\399-0.049101.hdf5\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9860 - val_loss: 0.0491 - val_accuracy: 0.9855\n",
      "Epoch 400/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0781 - accuracy: 0.9820\n",
      "Epoch 00400: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9876 - val_loss: 0.0519 - val_accuracy: 0.9865\n",
      "Epoch 401/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0535 - accuracy: 0.9880\n",
      "Epoch 00401: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9874 - val_loss: 0.0519 - val_accuracy: 0.9860\n",
      "Epoch 402/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0632 - accuracy: 0.9820\n",
      "Epoch 00402: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9862 - val_loss: 0.0501 - val_accuracy: 0.9855\n",
      "Epoch 403/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0284 - accuracy: 0.9920\n",
      "Epoch 00403: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9869 - val_loss: 0.0556 - val_accuracy: 0.9855\n",
      "Epoch 404/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0569 - accuracy: 0.9800\n",
      "Epoch 00404: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9858 - val_loss: 0.0602 - val_accuracy: 0.9841\n",
      "Epoch 405/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0707 - accuracy: 0.9860\n",
      "Epoch 00405: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9858 - val_loss: 0.0510 - val_accuracy: 0.9855\n",
      "Epoch 406/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0565 - accuracy: 0.9860\n",
      "Epoch 00406: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0542 - accuracy: 0.9860 - val_loss: 0.0520 - val_accuracy: 0.9865\n",
      "Epoch 407/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0378 - accuracy: 0.9920\n",
      "Epoch 00407: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9874 - val_loss: 0.0528 - val_accuracy: 0.9865\n",
      "Epoch 408/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0944 - accuracy: 0.9740\n",
      "Epoch 00408: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9869 - val_loss: 0.0506 - val_accuracy: 0.9855\n",
      "Epoch 409/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0723 - accuracy: 0.9860\n",
      "Epoch 00409: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9855 - val_loss: 0.0496 - val_accuracy: 0.9860\n",
      "Epoch 410/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0447 - accuracy: 0.9880\n",
      "Epoch 00410: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9881 - val_loss: 0.0525 - val_accuracy: 0.9869\n",
      "Epoch 411/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0499 - accuracy: 0.9900\n",
      "Epoch 00411: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0501 - accuracy: 0.9876 - val_loss: 0.0495 - val_accuracy: 0.9855\n",
      "Epoch 412/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0694 - accuracy: 0.9780\n",
      "Epoch 00412: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9858 - val_loss: 0.0507 - val_accuracy: 0.9860\n",
      "Epoch 413/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0260 - accuracy: 0.9960\n",
      "Epoch 00413: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0617 - accuracy: 0.9825 - val_loss: 0.0547 - val_accuracy: 0.9860\n",
      "Epoch 414/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9920\n",
      "Epoch 00414: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9841 - val_loss: 0.0494 - val_accuracy: 0.9865\n",
      "Epoch 415/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0624 - accuracy: 0.9820\n",
      "Epoch 00415: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9855 - val_loss: 0.0507 - val_accuracy: 0.9860\n",
      "Epoch 416/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0610 - accuracy: 0.9860\n",
      "Epoch 00416: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9839 - val_loss: 0.0526 - val_accuracy: 0.9860\n",
      "Epoch 417/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0534 - accuracy: 0.9840\n",
      "Epoch 00417: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9860 - val_loss: 0.0523 - val_accuracy: 0.9865\n",
      "Epoch 418/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0325 - accuracy: 0.9860\n",
      "Epoch 00418: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9874 - val_loss: 0.0495 - val_accuracy: 0.9851\n",
      "Epoch 419/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0373 - accuracy: 0.9940\n",
      "Epoch 00419: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.9874 - val_loss: 0.0523 - val_accuracy: 0.9860\n",
      "Epoch 420/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0442 - accuracy: 0.9860\n",
      "Epoch 00420: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9867 - val_loss: 0.0598 - val_accuracy: 0.9846\n",
      "Epoch 421/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0904 - accuracy: 0.9800\n",
      "Epoch 00421: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9860 - val_loss: 0.0493 - val_accuracy: 0.9860\n",
      "Epoch 422/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0569 - accuracy: 0.9860\n",
      "Epoch 00422: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9878 - val_loss: 0.0505 - val_accuracy: 0.9855\n",
      "Epoch 423/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0354 - accuracy: 0.9900\n",
      "Epoch 00423: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9876 - val_loss: 0.0505 - val_accuracy: 0.9860\n",
      "Epoch 424/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0299 - accuracy: 0.9880\n",
      "Epoch 00424: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0501 - accuracy: 0.9869 - val_loss: 0.0675 - val_accuracy: 0.9804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0837 - accuracy: 0.9780\n",
      "Epoch 00425: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0583 - accuracy: 0.9839 - val_loss: 0.0541 - val_accuracy: 0.9855\n",
      "Epoch 426/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0491 - accuracy: 0.9880\n",
      "Epoch 00426: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9855 - val_loss: 0.0505 - val_accuracy: 0.9860\n",
      "Epoch 427/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9880\n",
      "Epoch 00427: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9862 - val_loss: 0.0510 - val_accuracy: 0.9865\n",
      "Epoch 428/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0746 - accuracy: 0.9780\n",
      "Epoch 00428: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0501 - accuracy: 0.9867 - val_loss: 0.0500 - val_accuracy: 0.9865\n",
      "Epoch 429/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9920\n",
      "Epoch 00429: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9876 - val_loss: 0.0524 - val_accuracy: 0.9865\n",
      "Epoch 430/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0550 - accuracy: 0.9860\n",
      "Epoch 00430: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 0.9858 - val_loss: 0.0495 - val_accuracy: 0.9855\n",
      "Epoch 431/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0304 - accuracy: 0.9860\n",
      "Epoch 00431: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9855 - val_loss: 0.0502 - val_accuracy: 0.9860\n",
      "Epoch 432/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0550 - accuracy: 0.9800\n",
      "Epoch 00432: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9855 - val_loss: 0.0502 - val_accuracy: 0.9855\n",
      "Epoch 433/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0502 - accuracy: 0.9900\n",
      "Epoch 00433: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9869 - val_loss: 0.0559 - val_accuracy: 0.9865\n",
      "Epoch 434/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0669 - accuracy: 0.9800\n",
      "Epoch 00434: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9864 - val_loss: 0.0517 - val_accuracy: 0.9860\n",
      "Epoch 435/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0539 - accuracy: 0.9840\n",
      "Epoch 00435: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9874 - val_loss: 0.0496 - val_accuracy: 0.9860\n",
      "Epoch 436/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0834 - accuracy: 0.9840\n",
      "Epoch 00436: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9871 - val_loss: 0.0505 - val_accuracy: 0.9865\n",
      "Epoch 437/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0644 - accuracy: 0.9860\n",
      "Epoch 00437: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0499 - accuracy: 0.9876 - val_loss: 0.0497 - val_accuracy: 0.9855\n",
      "Epoch 438/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0586 - accuracy: 0.9800\n",
      "Epoch 00438: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0493 - accuracy: 0.9867 - val_loss: 0.0505 - val_accuracy: 0.9865\n",
      "Epoch 439/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0692 - accuracy: 0.9820\n",
      "Epoch 00439: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9878 - val_loss: 0.0504 - val_accuracy: 0.9865\n",
      "Epoch 440/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0468 - accuracy: 0.9900\n",
      "Epoch 00440: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9867 - val_loss: 0.0499 - val_accuracy: 0.9865\n",
      "Epoch 441/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0492 - accuracy: 0.9880\n",
      "Epoch 00441: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9864 - val_loss: 0.0496 - val_accuracy: 0.9855\n",
      "Epoch 442/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9920\n",
      "Epoch 00442: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9881 - val_loss: 0.0507 - val_accuracy: 0.9860\n",
      "Epoch 443/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0450 - accuracy: 0.9880\n",
      "Epoch 00443: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0485 - accuracy: 0.9878 - val_loss: 0.0505 - val_accuracy: 0.9865\n",
      "Epoch 444/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9800\n",
      "Epoch 00444: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 0.9876 - val_loss: 0.0500 - val_accuracy: 0.9855\n",
      "Epoch 445/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0404 - accuracy: 0.9900\n",
      "Epoch 00445: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9864 - val_loss: 0.0516 - val_accuracy: 0.9860\n",
      "Epoch 446/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0648 - accuracy: 0.9840\n",
      "Epoch 00446: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9858 - val_loss: 0.0494 - val_accuracy: 0.9865\n",
      "Epoch 447/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0802 - accuracy: 0.9780\n",
      "Epoch 00447: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9883 - val_loss: 0.0510 - val_accuracy: 0.9865\n",
      "Epoch 448/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0765 - accuracy: 0.9800\n",
      "Epoch 00448: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9885 - val_loss: 0.0501 - val_accuracy: 0.9855\n",
      "Epoch 449/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9920\n",
      "Epoch 00449: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 0.9876 - val_loss: 0.0530 - val_accuracy: 0.9869\n",
      "Epoch 450/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0504 - accuracy: 0.9860\n",
      "Epoch 00450: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9885 - val_loss: 0.0495 - val_accuracy: 0.9860\n",
      "Epoch 451/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9940\n",
      "Epoch 00451: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9878 - val_loss: 0.0526 - val_accuracy: 0.9869\n",
      "Epoch 452/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0548 - accuracy: 0.9820\n",
      "Epoch 00452: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 0.9871 - val_loss: 0.0513 - val_accuracy: 0.9865\n",
      "Epoch 453/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0317 - accuracy: 0.9920\n",
      "Epoch 00453: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0484 - accuracy: 0.9883 - val_loss: 0.0495 - val_accuracy: 0.9860\n",
      "Epoch 454/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0477 - accuracy: 0.9860\n",
      "Epoch 00454: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 0.9881 - val_loss: 0.0499 - val_accuracy: 0.9855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0601 - accuracy: 0.9800\n",
      "Epoch 00455: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9876 - val_loss: 0.0506 - val_accuracy: 0.9865\n",
      "Epoch 456/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0276 - accuracy: 0.9940\n",
      "Epoch 00456: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0484 - accuracy: 0.9881 - val_loss: 0.0496 - val_accuracy: 0.9855\n",
      "Epoch 457/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0298 - accuracy: 0.9880\n",
      "Epoch 00457: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9876 - val_loss: 0.0517 - val_accuracy: 0.9869\n",
      "Epoch 458/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9900\n",
      "Epoch 00458: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9869 - val_loss: 0.0516 - val_accuracy: 0.9874\n",
      "Epoch 459/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0375 - accuracy: 0.9880\n",
      "Epoch 00459: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9860 - val_loss: 0.0500 - val_accuracy: 0.9874\n",
      "Epoch 460/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0735 - accuracy: 0.9800\n",
      "Epoch 00460: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9855 - val_loss: 0.0503 - val_accuracy: 0.9860\n",
      "Epoch 461/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0255 - accuracy: 0.9940\n",
      "Epoch 00461: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0506 - accuracy: 0.9871 - val_loss: 0.0503 - val_accuracy: 0.9865\n",
      "Epoch 462/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0437 - accuracy: 0.9860\n",
      "Epoch 00462: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0488 - accuracy: 0.9867 - val_loss: 0.0519 - val_accuracy: 0.9860\n",
      "Epoch 463/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9840\n",
      "Epoch 00463: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9860 - val_loss: 0.0493 - val_accuracy: 0.9855\n",
      "Epoch 464/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9940\n",
      "Epoch 00464: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9874 - val_loss: 0.0501 - val_accuracy: 0.9860\n",
      "Epoch 465/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0636 - accuracy: 0.9900\n",
      "Epoch 00465: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 0.9876 - val_loss: 0.0499 - val_accuracy: 0.9860\n",
      "Epoch 466/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0655 - accuracy: 0.9820\n",
      "Epoch 00466: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9876 - val_loss: 0.0502 - val_accuracy: 0.9860\n",
      "Epoch 467/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0531 - accuracy: 0.9840\n",
      "Epoch 00467: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9869 - val_loss: 0.0504 - val_accuracy: 0.9860\n",
      "Epoch 468/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0503 - accuracy: 0.9880\n",
      "Epoch 00468: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 0.9874 - val_loss: 0.0506 - val_accuracy: 0.9865\n",
      "Epoch 469/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0532 - accuracy: 0.9860\n",
      "Epoch 00469: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9878 - val_loss: 0.0500 - val_accuracy: 0.9860\n",
      "Epoch 470/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0843 - accuracy: 0.9840\n",
      "Epoch 00470: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9876 - val_loss: 0.0503 - val_accuracy: 0.9860\n",
      "Epoch 471/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0543 - accuracy: 0.9880\n",
      "Epoch 00471: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9871 - val_loss: 0.0548 - val_accuracy: 0.9851\n",
      "Epoch 472/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0443 - accuracy: 0.9760\n",
      "Epoch 00472: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9844 - val_loss: 0.0516 - val_accuracy: 0.9865\n",
      "Epoch 473/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0422 - accuracy: 0.9900\n",
      "Epoch 00473: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9874 - val_loss: 0.0509 - val_accuracy: 0.9860\n",
      "Epoch 474/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0256 - accuracy: 0.9880\n",
      "Epoch 00474: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9858 - val_loss: 0.0502 - val_accuracy: 0.9860\n",
      "Epoch 475/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0547 - accuracy: 0.9920\n",
      "Epoch 00475: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0482 - accuracy: 0.9881 - val_loss: 0.0513 - val_accuracy: 0.9860\n",
      "Epoch 476/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0851 - accuracy: 0.9840\n",
      "Epoch 00476: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9867 - val_loss: 0.0513 - val_accuracy: 0.9865\n",
      "Epoch 477/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0569 - accuracy: 0.9900\n",
      "Epoch 00477: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0506 - accuracy: 0.9878 - val_loss: 0.0545 - val_accuracy: 0.9832\n",
      "Epoch 478/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0366 - accuracy: 0.9840\n",
      "Epoch 00478: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9855 - val_loss: 0.0499 - val_accuracy: 0.9865\n",
      "Epoch 479/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0333 - accuracy: 0.9840\n",
      "Epoch 00479: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9862 - val_loss: 0.0503 - val_accuracy: 0.9865\n",
      "Epoch 480/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0435 - accuracy: 0.9940\n",
      "Epoch 00480: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9881 - val_loss: 0.0533 - val_accuracy: 0.9865\n",
      "Epoch 481/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0279 - accuracy: 0.9920\n",
      "Epoch 00481: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9871 - val_loss: 0.0557 - val_accuracy: 0.9851\n",
      "Epoch 482/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9940\n",
      "Epoch 00482: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9869 - val_loss: 0.0518 - val_accuracy: 0.9869\n",
      "Epoch 483/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0425 - accuracy: 0.9900\n",
      "Epoch 00483: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0480 - accuracy: 0.9881 - val_loss: 0.0497 - val_accuracy: 0.9860\n",
      "Epoch 484/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0512 - accuracy: 0.9860\n",
      "Epoch 00484: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 0.9878 - val_loss: 0.0518 - val_accuracy: 0.9865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0656 - accuracy: 0.9780\n",
      "Epoch 00485: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9874 - val_loss: 0.0507 - val_accuracy: 0.9860\n",
      "Epoch 486/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0543 - accuracy: 0.9840\n",
      "Epoch 00486: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9871 - val_loss: 0.0504 - val_accuracy: 0.9860\n",
      "Epoch 487/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0683 - accuracy: 0.9860\n",
      "Epoch 00487: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9885 - val_loss: 0.0500 - val_accuracy: 0.9860\n",
      "Epoch 488/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0354 - accuracy: 0.9960\n",
      "Epoch 00488: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 0.9883 - val_loss: 0.0502 - val_accuracy: 0.9860\n",
      "Epoch 489/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9880\n",
      "Epoch 00489: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9874 - val_loss: 0.0498 - val_accuracy: 0.9860\n",
      "Epoch 490/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0976 - accuracy: 0.9800\n",
      "Epoch 00490: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9867 - val_loss: 0.0505 - val_accuracy: 0.9860\n",
      "Epoch 491/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0777 - accuracy: 0.9840\n",
      "Epoch 00491: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9874 - val_loss: 0.0509 - val_accuracy: 0.9860\n",
      "Epoch 492/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0573 - accuracy: 0.9900\n",
      "Epoch 00492: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9885 - val_loss: 0.0521 - val_accuracy: 0.9869\n",
      "Epoch 493/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0430 - accuracy: 0.9880\n",
      "Epoch 00493: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0484 - accuracy: 0.9881 - val_loss: 0.0528 - val_accuracy: 0.9874\n",
      "Epoch 494/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0353 - accuracy: 0.9920\n",
      "Epoch 00494: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9862 - val_loss: 0.0503 - val_accuracy: 0.9855\n",
      "Epoch 495/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0589 - accuracy: 0.9860\n",
      "Epoch 00495: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9876 - val_loss: 0.0501 - val_accuracy: 0.9855\n",
      "Epoch 496/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9820\n",
      "Epoch 00496: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.9874 - val_loss: 0.0503 - val_accuracy: 0.9865\n",
      "Epoch 497/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0611 - accuracy: 0.9880\n",
      "Epoch 00497: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0485 - accuracy: 0.9871 - val_loss: 0.0499 - val_accuracy: 0.9865\n",
      "Epoch 498/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0282 - accuracy: 0.9920\n",
      "Epoch 00498: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 0.9876 - val_loss: 0.0499 - val_accuracy: 0.9860\n",
      "Epoch 499/3500\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.0320 - accuracy: 0.9920\n",
      "Epoch 00499: val_loss did not improve from 0.04910\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9874 - val_loss: 0.0555 - val_accuracy: 0.9865\n",
      "204/204 [==============================] - 0s 567us/step - loss: 0.0517 - accuracy: 0.9869\n",
      "\n",
      " Accuracy : 0.9869\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "##########################################################\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "##########################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "df_pre = pd.read_csv('C:\\\\Users\\\\user\\study1\\\\머신러닝\\data\\\\wine.csv', header = None)\n",
    "# 15%의 비율로 데이터를 랜덤하게 샘플링\n",
    "# 시간이 너무 오래 걸리지 않게하기 위함.\n",
    "df = df_pre.sample(frac = 1)\n",
    "\n",
    "dataset = df.values \n",
    "X = dataset[:,0:12]\n",
    "Y = dataset[:,12]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30,input_dim = 12, activation = 're8lu'))\n",
    "model.add(Dense(12, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# 모델 저장 폴더 만들기\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "modelpath = '/.model/{epoch:02d}-{val_loss:4f}.hdf5'\n",
    "\n",
    "# 모델업데이트 및 저장\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor = 'val_loss', verbose = 1,\n",
    "                              save_best_only  = True)\n",
    "\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "# 총 시행횟수 3500에서 100번 이상 연속적으로 개선이 없으면 멈추기\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 100)\n",
    "\n",
    "# 모델 실행 및 저장\n",
    "# 모델이 학습되는 과정을 history변수에 저장한다.\n",
    "history = model.fit(X,Y, validation_split = 0.33, epochs = 3500, batch_size = 500,\n",
    "         callbacks = [early_stopping_callback, checkpointer])\n",
    "\n",
    "print('\\n Accuracy : %.4f'%(model.evaluate(X,Y)[1]))\n",
    "\n",
    "# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# y_acc에 학습셋으로 측정한 정확도의 값을 저장\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "# x 값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시\n",
    "x_len = numpy.arange(len(y_acc))\n",
    "# 오차\n",
    "plt.plot(x_len, y_vloss, 'o', c = 'red', markersize = 3)\n",
    "# 정확도\n",
    "plt.plot(x_len, y_acc, 'o', c = 'blue', markersize = 3)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "-  monitor = 'val_loss' 학습 데이터는 과적합이 되면 될수록 성능이 좋아지고, 손실값이 줄어들기때문에, loss(학습 손실값)을 사용하지 않고 val_loss(검증 손실값)으로 과적합을 판단해서 검증을 멈출 수 있다. \n",
    "\n",
    "- val_acc으로 판단할 수도 있지만, 확률적으로 val_loss가 더 검증하기에 적합함\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras에서는 모델 학습을 위해 fit() 함수를 사용합니다. 이 때, 리턴값으로 학습 이력(History) 정보를 리턴합니다. 여기에는 다음과 같은 항목들이 포함되어 있습니다.\n",
    "\n",
    "아래 항목들은 매 epoch 마다의 값들이 저장되어 있습니다.\n",
    "\n",
    "- loss : 학습 손실값 (에러율, 오차)\n",
    "- acc : 학습 정확도\n",
    "- val_loss : 검증 손실값 (에러율,오차)\n",
    "- val_acc : 검증 정확도\n",
    "\n",
    "########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ModelCheckpoint\n",
    "\n",
    "- keras에서 모델을 학습할 때마다 중간중간에 콜백 형태로 알려준다.\n",
    "- ModelCheckpoint의 속성으로 verbose는 해당 함수의 진행 사항의 출력 여부, save_best_only는 모델의 정확도가 최고값을 갱신했을 때만 저장하도록 하는 옵션\n",
    "- monitor='val_loss'  ???  \n",
    "- val_loss 는 오차율\n",
    "- 실행하면 ./model폴더 아래 수많은 .hdf5파일이 생기고, 파일명은 epoch값과 val_loss이므로 가장 최종적으로 생긴파일이 가장 성능이 좋은 모델이 된다.\n",
    "- save_best_only  = True (데이터가 개선되는 것만 저장한다.)\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 모델이 학습되는 과정을 history 변수를 만들어서 저장한다\n",
    "- 테스트 오차는 케라스 내부에서 val_loss로 기록된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################################\n",
    "\n",
    "batch_size\n",
    "\n",
    "- 가용 메모리가 적을 때는batch사이즈를 상대적으로 작게, 보다 안정적으로 학습 시키고 싶다면 batch size를 상대적으로 놓게 설정한다.\n",
    "- 다만, batch size가 커질 수록 일반화 성능은 감소하는 경우가 있음.\n",
    "- 작은 batch size를 사용하면 generalization performance 측면에서 긍정적인 영향을 끼친다는 것이 여러 연구에서 실험적으로 관찰되고 있는 결과\n",
    "\n",
    "ex) model.fit(train_images, train_labels, epochs=5, batch=128)\n",
    "\n",
    "- fit( )메서드를 호출하면 네트워크가 128개 샘플씩 미니 배치로 훈련데이터를 다섯 번 반복한다. 각 반복마다 네트워크가 배치에서 손실에 대한 가중치의 그래디언트를 계산하고, 그에 맞추어 가중치를 업데이트한다.\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 회귀 적용하기\n",
    "\n",
    "보스턴 집값 예측\n",
    "\n",
    "- 집값에 가장 큰 영향을 미치는 것이 무엇인가?\n",
    "- 환경과 집값의 변동을 보여주는 데이터셋\n",
    "- 하나의 정답을 맞추는 것이 아닌, 수치를 예측하는 문제\n",
    "- 주어진 환경요인과 집값의 변동을 예측해 환경 요인만 놓고 집값을 예측\n",
    "- 마지막 레이어에서 활성화함수로 sigmoid가 아닌 relu함수를 쓰면, (relu함수는 0보다 작으면  0, 0보다 크면 x값을 반환한다.) 제 값을 반환하기 때문에, 선형 회귀의 적용이 가능하다.\n",
    "- 선형회귀의 loss함수는 mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       506 non-null    float64\n",
      " 1   1       506 non-null    float64\n",
      " 2   2       506 non-null    float64\n",
      " 3   3       506 non-null    int64  \n",
      " 4   4       506 non-null    float64\n",
      " 5   5       506 non-null    float64\n",
      " 6   6       506 non-null    float64\n",
      " 7   7       506 non-null    float64\n",
      " 8   8       506 non-null    int64  \n",
      " 9   9       506 non-null    float64\n",
      " 10  10      506 non-null    float64\n",
      " 11  11      506 non-null    float64\n",
      " 12  12      506 non-null    float64\n",
      " 13  13      506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n",
      "None\n",
      "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
      "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
      "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
      "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
      "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
      "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
      "\n",
      "       11    12    13  \n",
      "0  396.90  4.98  24.0  \n",
      "1  396.90  9.14  21.6  \n",
      "2  392.83  4.03  34.7  \n",
      "3  394.63  2.94  33.4  \n",
      "4  396.90  5.33  36.2  \n",
      "Epoch 1/200\n",
      "51/51 [==============================] - 0s 579us/step - loss: 11061.4395 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "51/51 [==============================] - 0s 625us/step - loss: 2359.0486 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "51/51 [==============================] - 0s 659us/step - loss: 231.9740 - accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "51/51 [==============================] - 0s 628us/step - loss: 100.9902 - accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "51/51 [==============================] - 0s 641us/step - loss: 85.0260 - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "51/51 [==============================] - 0s 618us/step - loss: 78.0413 - accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "51/51 [==============================] - 0s 606us/step - loss: 76.1913 - accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "51/51 [==============================] - 0s 623us/step - loss: 75.6028 - accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "51/51 [==============================] - 0s 630us/step - loss: 74.2840 - accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "51/51 [==============================] - 0s 590us/step - loss: 73.8276 - accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "51/51 [==============================] - 0s 612us/step - loss: 72.9250 - accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "51/51 [==============================] - 0s 618us/step - loss: 72.0938 - accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "51/51 [==============================] - 0s 609us/step - loss: 71.7510 - accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "51/51 [==============================] - 0s 637us/step - loss: 71.0824 - accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "51/51 [==============================] - 0s 619us/step - loss: 70.3647 - accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "51/51 [==============================] - 0s 627us/step - loss: 70.5677 - accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "51/51 [==============================] - 0s 643us/step - loss: 69.3968 - accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "51/51 [==============================] - 0s 624us/step - loss: 68.7780 - accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "51/51 [==============================] - 0s 611us/step - loss: 68.5349 - accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "51/51 [==============================] - 0s 626us/step - loss: 68.8549 - accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "51/51 [==============================] - 0s 619us/step - loss: 68.0394 - accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "51/51 [==============================] - 0s 630us/step - loss: 67.7611 - accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "51/51 [==============================] - 0s 607us/step - loss: 67.2876 - accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "51/51 [==============================] - 0s 611us/step - loss: 67.2323 - accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "51/51 [==============================] - 0s 624us/step - loss: 66.6633 - accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "51/51 [==============================] - 0s 617us/step - loss: 66.4163 - accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "51/51 [==============================] - 0s 598us/step - loss: 66.0627 - accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "51/51 [==============================] - 0s 595us/step - loss: 66.2270 - accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "51/51 [==============================] - 0s 602us/step - loss: 65.7515 - accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "51/51 [==============================] - 0s 561us/step - loss: 65.5129 - accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "51/51 [==============================] - 0s 598us/step - loss: 65.2062 - accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "51/51 [==============================] - 0s 593us/step - loss: 65.1625 - accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "51/51 [==============================] - 0s 588us/step - loss: 64.0446 - accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "51/51 [==============================] - 0s 611us/step - loss: 64.5607 - accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "51/51 [==============================] - 0s 601us/step - loss: 63.7311 - accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "51/51 [==============================] - 0s 617us/step - loss: 63.7251 - accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "51/51 [==============================] - 0s 631us/step - loss: 63.2676 - accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "51/51 [==============================] - 0s 601us/step - loss: 63.3080 - accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "51/51 [==============================] - 0s 604us/step - loss: 62.5889 - accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "51/51 [==============================] - 0s 625us/step - loss: 62.6818 - accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "51/51 [==============================] - 0s 621us/step - loss: 61.7864 - accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "51/51 [==============================] - 0s 620us/step - loss: 62.2920 - accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "51/51 [==============================] - 0s 636us/step - loss: 61.9645 - accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "51/51 [==============================] - 0s 575us/step - loss: 60.9940 - accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "51/51 [==============================] - 0s 611us/step - loss: 60.3228 - accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "51/51 [==============================] - 0s 603us/step - loss: 60.8040 - accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "51/51 [==============================] - 0s 597us/step - loss: 59.9039 - accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "51/51 [==============================] - 0s 608us/step - loss: 60.0079 - accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "51/51 [==============================] - 0s 633us/step - loss: 58.3959 - accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "51/51 [==============================] - 0s 610us/step - loss: 58.1608 - accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "51/51 [==============================] - 0s 594us/step - loss: 58.7077 - accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "51/51 [==============================] - 0s 601us/step - loss: 57.7026 - accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "51/51 [==============================] - 0s 606us/step - loss: 57.0725 - accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "51/51 [==============================] - 0s 639us/step - loss: 56.2257 - accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "51/51 [==============================] - 0s 620us/step - loss: 55.9368 - accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "51/51 [==============================] - 0s 606us/step - loss: 54.5284 - accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "51/51 [==============================] - 0s 605us/step - loss: 56.1610 - accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "51/51 [==============================] - 0s 606us/step - loss: 54.7417 - accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "51/51 [==============================] - 0s 607us/step - loss: 54.5345 - accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "51/51 [==============================] - 0s 612us/step - loss: 54.0242 - accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "51/51 [==============================] - 0s 585us/step - loss: 53.5281 - accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "51/51 [==============================] - 0s 619us/step - loss: 52.7786 - accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "51/51 [==============================] - 0s 631us/step - loss: 52.5526 - accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "51/51 [==============================] - 0s 623us/step - loss: 51.7630 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200\n",
      "51/51 [==============================] - 0s 623us/step - loss: 51.2881 - accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "51/51 [==============================] - 0s 612us/step - loss: 50.4013 - accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "51/51 [==============================] - 0s 602us/step - loss: 50.6713 - accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "51/51 [==============================] - 0s 591us/step - loss: 49.6469 - accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "51/51 [==============================] - 0s 607us/step - loss: 48.8824 - accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "51/51 [==============================] - 0s 623us/step - loss: 48.2992 - accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "51/51 [==============================] - 0s 584us/step - loss: 47.4869 - accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "51/51 [==============================] - 0s 593us/step - loss: 47.5004 - accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "51/51 [==============================] - 0s 610us/step - loss: 46.3480 - accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "51/51 [==============================] - 0s 598us/step - loss: 47.1632 - accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "51/51 [==============================] - 0s 629us/step - loss: 47.4062 - accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "51/51 [==============================] - 0s 619us/step - loss: 45.6530 - accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "51/51 [==============================] - 0s 604us/step - loss: 44.4019 - accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "51/51 [==============================] - 0s 614us/step - loss: 44.2750 - accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "51/51 [==============================] - 0s 623us/step - loss: 43.7691 - accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "51/51 [==============================] - 0s 610us/step - loss: 43.3039 - accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "51/51 [==============================] - 0s 628us/step - loss: 43.8130 - accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "51/51 [==============================] - 0s 607us/step - loss: 42.3035 - accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "51/51 [==============================] - 0s 613us/step - loss: 41.6677 - accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "51/51 [==============================] - 0s 622us/step - loss: 40.4640 - accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "51/51 [==============================] - 0s 601us/step - loss: 40.6531 - accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "51/51 [==============================] - 0s 632us/step - loss: 39.7143 - accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "51/51 [==============================] - 0s 605us/step - loss: 41.5928 - accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "51/51 [==============================] - 0s 602us/step - loss: 37.8783 - accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "51/51 [==============================] - 0s 608us/step - loss: 37.6583 - accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "51/51 [==============================] - 0s 620us/step - loss: 36.1517 - accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "51/51 [==============================] - 0s 612us/step - loss: 35.6876 - accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "51/51 [==============================] - 0s 605us/step - loss: 35.5910 - accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "51/51 [==============================] - 0s 581us/step - loss: 34.6911 - accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "51/51 [==============================] - 0s 601us/step - loss: 33.8483 - accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "51/51 [==============================] - 0s 615us/step - loss: 33.8429 - accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "51/51 [==============================] - 0s 597us/step - loss: 33.1008 - accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "51/51 [==============================] - 0s 607us/step - loss: 32.7426 - accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "51/51 [==============================] - 0s 627us/step - loss: 33.5837 - accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "51/51 [==============================] - 0s 616us/step - loss: 32.2734 - accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "51/51 [==============================] - 0s 587us/step - loss: 31.5540 - accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "51/51 [==============================] - 0s 615us/step - loss: 32.3836 - accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "51/51 [==============================] - 0s 624us/step - loss: 31.6708 - accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "51/51 [==============================] - 0s 590us/step - loss: 29.6664 - accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "51/51 [==============================] - 0s 631us/step - loss: 29.9896 - accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "51/51 [==============================] - 0s 660us/step - loss: 29.0476 - accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "51/51 [==============================] - 0s 644us/step - loss: 29.1945 - accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "51/51 [==============================] - 0s 672us/step - loss: 29.6810 - accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "51/51 [==============================] - 0s 645us/step - loss: 28.4873 - accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "51/51 [==============================] - 0s 652us/step - loss: 28.1187 - accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "51/51 [==============================] - 0s 655us/step - loss: 28.5698 - accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "51/51 [==============================] - 0s 661us/step - loss: 27.9684 - accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "51/51 [==============================] - 0s 662us/step - loss: 27.1526 - accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "51/51 [==============================] - 0s 671us/step - loss: 28.7449 - accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "51/51 [==============================] - 0s 671us/step - loss: 27.7262 - accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "51/51 [==============================] - 0s 661us/step - loss: 26.8644 - accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "51/51 [==============================] - 0s 676us/step - loss: 28.0963 - accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "51/51 [==============================] - 0s 660us/step - loss: 26.0797 - accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "51/51 [==============================] - 0s 668us/step - loss: 25.6062 - accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "51/51 [==============================] - 0s 658us/step - loss: 26.1599 - accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "51/51 [==============================] - 0s 692us/step - loss: 26.1221 - accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "51/51 [==============================] - 0s 647us/step - loss: 26.2212 - accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "51/51 [==============================] - 0s 664us/step - loss: 26.5902 - accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "51/51 [==============================] - 0s 632us/step - loss: 25.5046 - accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "51/51 [==============================] - 0s 645us/step - loss: 25.3563 - accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "51/51 [==============================] - 0s 682us/step - loss: 24.9736 - accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "51/51 [==============================] - 0s 648us/step - loss: 25.5790 - accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "51/51 [==============================] - 0s 664us/step - loss: 25.4604 - accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "51/51 [==============================] - 0s 648us/step - loss: 24.2351 - accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "51/51 [==============================] - 0s 679us/step - loss: 24.3176 - accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "51/51 [==============================] - 0s 698us/step - loss: 24.3111 - accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "51/51 [==============================] - 0s 688us/step - loss: 25.1304 - accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "51/51 [==============================] - 0s 642us/step - loss: 23.7398 - accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "51/51 [==============================] - 0s 676us/step - loss: 24.5224 - accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "51/51 [==============================] - 0s 649us/step - loss: 24.7700 - accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "51/51 [==============================] - 0s 664us/step - loss: 24.9927 - accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "51/51 [==============================] - 0s 668us/step - loss: 22.8940 - accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "51/51 [==============================] - 0s 636us/step - loss: 22.3723 - accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "51/51 [==============================] - 0s 659us/step - loss: 23.4443 - accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "51/51 [==============================] - 0s 641us/step - loss: 23.2032 - accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "51/51 [==============================] - 0s 666us/step - loss: 23.0435 - accuracy: 0.0000e+00\n",
      "Epoch 141/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 681us/step - loss: 23.1564 - accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "51/51 [==============================] - 0s 659us/step - loss: 22.5525 - accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "51/51 [==============================] - 0s 701us/step - loss: 22.6255 - accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "51/51 [==============================] - 0s 647us/step - loss: 22.0005 - accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "51/51 [==============================] - 0s 695us/step - loss: 22.2222 - accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "51/51 [==============================] - 0s 701us/step - loss: 22.3513 - accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "51/51 [==============================] - 0s 681us/step - loss: 22.6185 - accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "51/51 [==============================] - 0s 681us/step - loss: 21.8208 - accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "51/51 [==============================] - 0s 666us/step - loss: 23.2185 - accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "51/51 [==============================] - 0s 685us/step - loss: 21.8140 - accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "51/51 [==============================] - 0s 670us/step - loss: 22.4018 - accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "51/51 [==============================] - 0s 642us/step - loss: 21.6534 - accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "51/51 [==============================] - 0s 675us/step - loss: 22.2862 - accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "51/51 [==============================] - 0s 635us/step - loss: 21.3876 - accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "51/51 [==============================] - 0s 666us/step - loss: 21.9635 - accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "51/51 [==============================] - 0s 650us/step - loss: 21.7893 - accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "51/51 [==============================] - 0s 673us/step - loss: 21.6711 - accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "51/51 [==============================] - 0s 675us/step - loss: 21.2546 - accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "51/51 [==============================] - 0s 675us/step - loss: 21.4980 - accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "51/51 [==============================] - ETA: 0s - loss: 16.6523 - accuracy: 0.0000e+0 - 0s 667us/step - loss: 20.6775 - accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "51/51 [==============================] - 0s 669us/step - loss: 20.9389 - accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "51/51 [==============================] - 0s 660us/step - loss: 20.8082 - accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "51/51 [==============================] - 0s 652us/step - loss: 20.2106 - accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "51/51 [==============================] - 0s 657us/step - loss: 21.7093 - accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "51/51 [==============================] - 0s 667us/step - loss: 21.8317 - accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "51/51 [==============================] - 0s 660us/step - loss: 21.3222 - accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "51/51 [==============================] - 0s 630us/step - loss: 20.4460 - accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "51/51 [==============================] - 0s 636us/step - loss: 20.0863 - accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "51/51 [==============================] - 0s 522us/step - loss: 19.3972 - accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "51/51 [==============================] - 0s 525us/step - loss: 20.7721 - accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "51/51 [==============================] - 0s 534us/step - loss: 20.2209 - accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "51/51 [==============================] - 0s 590us/step - loss: 20.4158 - accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "51/51 [==============================] - 0s 658us/step - loss: 21.6665 - accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "51/51 [==============================] - 0s 707us/step - loss: 20.2563 - accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "51/51 [==============================] - 0s 658us/step - loss: 20.8558 - accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "51/51 [==============================] - 0s 648us/step - loss: 19.1258 - accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "51/51 [==============================] - 0s 677us/step - loss: 19.8212 - accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "51/51 [==============================] - 0s 628us/step - loss: 20.0511 - accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "51/51 [==============================] - 0s 661us/step - loss: 19.4138 - accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "51/51 [==============================] - 0s 659us/step - loss: 19.1965 - accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "51/51 [==============================] - 0s 672us/step - loss: 19.7271 - accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "51/51 [==============================] - 0s 667us/step - loss: 19.8803 - accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "51/51 [==============================] - 0s 676us/step - loss: 19.1546 - accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "51/51 [==============================] - 0s 681us/step - loss: 20.3055 - accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "51/51 [==============================] - 0s 659us/step - loss: 18.8961 - accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "51/51 [==============================] - 0s 678us/step - loss: 17.9324 - accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "51/51 [==============================] - 0s 693us/step - loss: 18.9548 - accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "51/51 [==============================] - 0s 699us/step - loss: 19.2328 - accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "51/51 [==============================] - 0s 690us/step - loss: 19.2822 - accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "51/51 [==============================] - 0s 688us/step - loss: 19.2403 - accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "51/51 [==============================] - 0s 652us/step - loss: 19.0233 - accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "51/51 [==============================] - 0s 670us/step - loss: 20.4203 - accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "51/51 [==============================] - 0s 638us/step - loss: 18.2985 - accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "51/51 [==============================] - 0s 660us/step - loss: 18.2795 - accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "51/51 [==============================] - 0s 673us/step - loss: 17.9305 - accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "51/51 [==============================] - 0s 647us/step - loss: 18.4516 - accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "51/51 [==============================] - 0s 672us/step - loss: 17.9100 - accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "51/51 [==============================] - 0s 661us/step - loss: 19.0559 - accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "51/51 [==============================] - 0s 682us/step - loss: 18.6784 - accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "51/51 [==============================] - 0s 685us/step - loss: 17.3206 - accuracy: 0.0000e+00\n",
      "16/16 [==============================] - 0s 499us/step - loss: 20.8210 - accuracy: 0.0000e+00\n",
      "\n",
      " Accuracy : 0.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\user\\\\study1\\\\머신러닝\\\\data\\\\housing.csv',delim_whitespace = True, header = None)\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "dataset = df.values \n",
    "\n",
    "\n",
    "X = dataset[:, 0:13]\n",
    "Y = dataset[:,13]\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = 13, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "model.compile(loss = 'mean_squared_error',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X, Y, epochs = 200, batch_size = 10)\n",
    "\n",
    "\n",
    "print('\\n Accuracy : %.4f'%(model.evaluate(X,Y)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2진 분류와는 다르게 선형회귀는,T or F로 나뉘는 것이 아니고, 각기 다른 값을 가지고 있기 때문에 정확도가 낮게 출력된다.\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형회귀 풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/36 [==============================] - 0s 620us/step - loss: 8438.1162\n",
      "Epoch 2/200\n",
      "36/36 [==============================] - 0s 587us/step - loss: 732.8683\n",
      "Epoch 3/200\n",
      "36/36 [==============================] - 0s 626us/step - loss: 512.5950\n",
      "Epoch 4/200\n",
      "36/36 [==============================] - 0s 638us/step - loss: 388.6990\n",
      "Epoch 5/200\n",
      "36/36 [==============================] - 0s 642us/step - loss: 256.9753\n",
      "Epoch 6/200\n",
      "36/36 [==============================] - 0s 607us/step - loss: 170.7661\n",
      "Epoch 7/200\n",
      "36/36 [==============================] - 0s 650us/step - loss: 135.6643\n",
      "Epoch 8/200\n",
      "36/36 [==============================] - 0s 629us/step - loss: 123.0020\n",
      "Epoch 9/200\n",
      "36/36 [==============================] - 0s 650us/step - loss: 92.7171\n",
      "Epoch 10/200\n",
      "36/36 [==============================] - 0s 642us/step - loss: 80.1416\n",
      "Epoch 11/200\n",
      "36/36 [==============================] - 0s 677us/step - loss: 72.3035\n",
      "Epoch 12/200\n",
      "36/36 [==============================] - 0s 625us/step - loss: 69.8260\n",
      "Epoch 13/200\n",
      "36/36 [==============================] - 0s 607us/step - loss: 67.6638\n",
      "Epoch 14/200\n",
      "36/36 [==============================] - 0s 637us/step - loss: 65.8864\n",
      "Epoch 15/200\n",
      "36/36 [==============================] - 0s 632us/step - loss: 65.1428\n",
      "Epoch 16/200\n",
      "36/36 [==============================] - 0s 647us/step - loss: 63.5601\n",
      "Epoch 17/200\n",
      "36/36 [==============================] - 0s 658us/step - loss: 64.5949\n",
      "Epoch 18/200\n",
      "36/36 [==============================] - 0s 664us/step - loss: 61.6633\n",
      "Epoch 19/200\n",
      "36/36 [==============================] - 0s 614us/step - loss: 62.5243\n",
      "Epoch 20/200\n",
      "36/36 [==============================] - 0s 645us/step - loss: 59.6989\n",
      "Epoch 21/200\n",
      "36/36 [==============================] - 0s 644us/step - loss: 59.3723\n",
      "Epoch 22/200\n",
      "36/36 [==============================] - 0s 633us/step - loss: 58.5467\n",
      "Epoch 23/200\n",
      "36/36 [==============================] - 0s 627us/step - loss: 58.7318\n",
      "Epoch 24/200\n",
      "36/36 [==============================] - 0s 674us/step - loss: 56.2303\n",
      "Epoch 25/200\n",
      "36/36 [==============================] - 0s 612us/step - loss: 56.2374\n",
      "Epoch 26/200\n",
      "36/36 [==============================] - 0s 673us/step - loss: 56.2391\n",
      "Epoch 27/200\n",
      "36/36 [==============================] - 0s 653us/step - loss: 54.9049\n",
      "Epoch 28/200\n",
      "36/36 [==============================] - 0s 650us/step - loss: 54.9681\n",
      "Epoch 29/200\n",
      "36/36 [==============================] - 0s 638us/step - loss: 54.0103\n",
      "Epoch 30/200\n",
      "36/36 [==============================] - 0s 634us/step - loss: 53.4721\n",
      "Epoch 31/200\n",
      "36/36 [==============================] - 0s 648us/step - loss: 52.6120\n",
      "Epoch 32/200\n",
      "36/36 [==============================] - 0s 627us/step - loss: 52.3308\n",
      "Epoch 33/200\n",
      "36/36 [==============================] - 0s 610us/step - loss: 51.6922\n",
      "Epoch 34/200\n",
      "36/36 [==============================] - 0s 622us/step - loss: 51.4115\n",
      "Epoch 35/200\n",
      "36/36 [==============================] - 0s 651us/step - loss: 50.9594\n",
      "Epoch 36/200\n",
      "36/36 [==============================] - 0s 670us/step - loss: 50.0806\n",
      "Epoch 37/200\n",
      "36/36 [==============================] - 0s 609us/step - loss: 49.6587\n",
      "Epoch 38/200\n",
      "36/36 [==============================] - 0s 623us/step - loss: 50.1437\n",
      "Epoch 39/200\n",
      "36/36 [==============================] - 0s 652us/step - loss: 48.7939\n",
      "Epoch 40/200\n",
      "36/36 [==============================] - 0s 666us/step - loss: 48.2098\n",
      "Epoch 41/200\n",
      "36/36 [==============================] - 0s 589us/step - loss: 48.1594\n",
      "Epoch 42/200\n",
      "36/36 [==============================] - 0s 645us/step - loss: 47.6336\n",
      "Epoch 43/200\n",
      "36/36 [==============================] - 0s 630us/step - loss: 46.4014\n",
      "Epoch 44/200\n",
      "36/36 [==============================] - 0s 629us/step - loss: 46.0603\n",
      "Epoch 45/200\n",
      "36/36 [==============================] - 0s 649us/step - loss: 46.2956\n",
      "Epoch 46/200\n",
      "36/36 [==============================] - 0s 649us/step - loss: 46.1978\n",
      "Epoch 47/200\n",
      "36/36 [==============================] - 0s 579us/step - loss: 44.8460\n",
      "Epoch 48/200\n",
      "36/36 [==============================] - 0s 661us/step - loss: 43.1158\n",
      "Epoch 49/200\n",
      "36/36 [==============================] - 0s 635us/step - loss: 42.7315\n",
      "Epoch 50/200\n",
      "36/36 [==============================] - 0s 672us/step - loss: 42.7363\n",
      "Epoch 51/200\n",
      "36/36 [==============================] - 0s 657us/step - loss: 41.1165\n",
      "Epoch 52/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 41.3375\n",
      "Epoch 53/200\n",
      "36/36 [==============================] - 0s 657us/step - loss: 40.9050\n",
      "Epoch 54/200\n",
      "36/36 [==============================] - 0s 662us/step - loss: 39.9034\n",
      "Epoch 55/200\n",
      "36/36 [==============================] - 0s 655us/step - loss: 38.7777\n",
      "Epoch 56/200\n",
      "36/36 [==============================] - 0s 689us/step - loss: 39.4455\n",
      "Epoch 57/200\n",
      "36/36 [==============================] - 0s 644us/step - loss: 39.0700\n",
      "Epoch 58/200\n",
      "36/36 [==============================] - 0s 641us/step - loss: 37.7630\n",
      "Epoch 59/200\n",
      "36/36 [==============================] - 0s 663us/step - loss: 36.5026\n",
      "Epoch 60/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 36.5739\n",
      "Epoch 61/200\n",
      "36/36 [==============================] - 0s 630us/step - loss: 38.2261\n",
      "Epoch 62/200\n",
      "36/36 [==============================] - 0s 645us/step - loss: 36.1457\n",
      "Epoch 63/200\n",
      "36/36 [==============================] - 0s 636us/step - loss: 36.0968\n",
      "Epoch 64/200\n",
      "36/36 [==============================] - 0s 615us/step - loss: 34.8202\n",
      "Epoch 65/200\n",
      "36/36 [==============================] - 0s 636us/step - loss: 33.8550\n",
      "Epoch 66/200\n",
      "36/36 [==============================] - 0s 603us/step - loss: 35.0674\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 0s 659us/step - loss: 33.6298\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - 0s 603us/step - loss: 33.4776\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 0s 659us/step - loss: 37.0617\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 0s 642us/step - loss: 33.4909\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 0s 603us/step - loss: 32.8610\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 0s 627us/step - loss: 33.0652\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 0s 619us/step - loss: 31.9464\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 0s 636us/step - loss: 30.9932\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 0s 627us/step - loss: 32.2534\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 0s 607us/step - loss: 31.4632\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 0s 677us/step - loss: 31.7118\n",
      "Epoch 78/200\n",
      "36/36 [==============================] - 0s 616us/step - loss: 30.5142\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 0s 637us/step - loss: 30.4484\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 0s 704us/step - loss: 29.6032\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 29.3145\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 0s 596us/step - loss: 30.3669\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 0s 630us/step - loss: 31.6188\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 0s 636us/step - loss: 29.0275\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 0s 622us/step - loss: 28.3375\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 0s 638us/step - loss: 28.4044\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 0s 656us/step - loss: 29.4635\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 0s 659us/step - loss: 29.5894\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 0s 628us/step - loss: 28.4563\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 0s 693us/step - loss: 27.0863\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 0s 652us/step - loss: 28.2004\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 0s 673us/step - loss: 26.7010\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 0s 650us/step - loss: 27.8506\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 0s 649us/step - loss: 30.4518\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 0s 653us/step - loss: 26.2899\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 25.6983\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 0s 658us/step - loss: 25.7705\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 641us/step - loss: 25.6901\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 0s 659us/step - loss: 25.3404\n",
      "Epoch 100/200\n",
      "36/36 [==============================] - 0s 610us/step - loss: 25.3621\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 0s 620us/step - loss: 25.5198\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 0s 606us/step - loss: 26.4034\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 0s 625us/step - loss: 27.0355\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 0s 600us/step - loss: 29.2855\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 0s 629us/step - loss: 25.3013\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 0s 598us/step - loss: 25.3194\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 0s 609us/step - loss: 24.9012\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 0s 614us/step - loss: 24.4835\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 0s 610us/step - loss: 24.7045\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 0s 608us/step - loss: 25.2438\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 0s 646us/step - loss: 26.0588\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 0s 608us/step - loss: 26.9572\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 0s 647us/step - loss: 24.1499\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 0s 618us/step - loss: 25.3550\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 0s 594us/step - loss: 23.2317\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 0s 610us/step - loss: 23.9471\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 0s 623us/step - loss: 24.0546\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 0s 635us/step - loss: 23.1469\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 0s 663us/step - loss: 23.7542\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 0s 603us/step - loss: 22.8985\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 0s 643us/step - loss: 24.4217\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 0s 616us/step - loss: 23.0718\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 0s 589us/step - loss: 22.9580\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 24.8591\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 0s 614us/step - loss: 23.1668\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 0s 633us/step - loss: 22.6643\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 0s 622us/step - loss: 24.3838\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 0s 601us/step - loss: 22.6999\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 0s 610us/step - loss: 23.6269\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 0s 633us/step - loss: 21.9588\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 0s 612us/step - loss: 23.2078\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 0s 597us/step - loss: 22.1276\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 0s 622us/step - loss: 23.4127\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 0s 608us/step - loss: 24.1749\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - ETA: 0s - loss: 9.892 - 0s 586us/step - loss: 23.8193\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 0s 853us/step - loss: 23.3802\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 0s 663us/step - loss: 21.4158\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 0s 660us/step - loss: 23.3578\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 0s 636us/step - loss: 20.7983\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 0s 636us/step - loss: 23.1516\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 0s 608us/step - loss: 22.0328\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 0s 623us/step - loss: 20.3153\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 0s 598us/step - loss: 23.2594\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 0s 604us/step - loss: 23.0867\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 0s 674us/step - loss: 27.0086\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 0s 628us/step - loss: 21.7518\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 0s 636us/step - loss: 22.0327\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 0s 656us/step - loss: 22.8959\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 22.4906\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 0s 636us/step - loss: 21.5998\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 0s 605us/step - loss: 23.4205\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 0s 608us/step - loss: 21.1253\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 0s 628us/step - loss: 20.1864\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 0s 607us/step - loss: 22.7077\n",
      "Epoch 155/200\n",
      "36/36 [==============================] - 0s 635us/step - loss: 20.3380\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 0s 621us/step - loss: 22.0297\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 0s 633us/step - loss: 20.7166\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 0s 625us/step - loss: 21.2586\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 0s 635us/step - loss: 20.4158\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 0s 614us/step - loss: 21.2197\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 0s 601us/step - loss: 20.8700\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 0s 610us/step - loss: 20.1266\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 0s 618us/step - loss: 20.3223\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 0s 625us/step - loss: 21.2626\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 19.6495\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 0s 617us/step - loss: 19.0726\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 0s 632us/step - loss: 20.4881\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 0s 654us/step - loss: 19.1696\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 0s 655us/step - loss: 19.7511\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 0s 605us/step - loss: 18.2152\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 0s 624us/step - loss: 18.2944\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 0s 652us/step - loss: 18.2965\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 0s 645us/step - loss: 19.2690\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 0s 604us/step - loss: 18.5533\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 0s 629us/step - loss: 18.9418\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 0s 629us/step - loss: 18.7512\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 0s 617us/step - loss: 18.7615\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 0s 646us/step - loss: 19.5743\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 0s 612us/step - loss: 17.7814\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 0s 659us/step - loss: 17.7651\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 0s 612us/step - loss: 17.0862\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 0s 634us/step - loss: 18.2538\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 0s 628us/step - loss: 17.8484\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 0s 638us/step - loss: 20.4784\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 0s 604us/step - loss: 19.8855\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - 0s 618us/step - loss: 17.0514\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 0s 622us/step - loss: 17.9898\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 0s 619us/step - loss: 17.8454\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 0s 631us/step - loss: 17.3030\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - 0s 654us/step - loss: 16.9978\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - 0s 589us/step - loss: 17.2636\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 0s 605us/step - loss: 17.8245\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 0s 601us/step - loss: 17.4317\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 646us/step - loss: 18.5795\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 0s 631us/step - loss: 16.7098\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 0s 586us/step - loss: 17.5683\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 0s 596us/step - loss: 17.0497\n",
      "Epoch 198/200\n",
      "36/36 [==============================] - 0s 614us/step - loss: 17.3721\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 0s 603us/step - loss: 19.3602\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - ETA: 0s - loss: 18.43 - 0s 656us/step - loss: 17.9043\n",
      "실제가격 : 22.600, 예상가격: 23.494\n",
      "실제가격 : 50.000, 예상가격: 25.601\n",
      "실제가격 : 23.000, 예상가격: 24.425\n",
      "실제가격 : 8.300, 예상가격: 12.533\n",
      "실제가격 : 21.200, 예상가격: 18.124\n",
      "실제가격 : 19.900, 예상가격: 20.766\n",
      "실제가격 : 20.600, 예상가격: 21.137\n",
      "실제가격 : 18.700, 예상가격: 22.038\n",
      "실제가격 : 16.100, 예상가격: 16.557\n",
      "실제가격 : 18.600, 예상가격: 7.678\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# 학습셋과 검증셋 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# seed 설정\n",
    "# 재현 가능한 결과를 얻기 위함\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\user\\\\study1\\\\머신러닝\\\\data\\\\housing.csv',delim_whitespace = True, header = None)\n",
    "dataset = df.values \n",
    "\n",
    "\n",
    "X = dataset[:, 0:13]\n",
    "Y = dataset[:,13]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = seed)\n",
    "# random_state : 매번 데이터셋이 변경되는 것을 방지한다.\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30,input_dim = 13, activation = 'relu'))\n",
    "model.add(Dense(6, activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error',\n",
    "             optimizer = 'adam')\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = 200, batch_size = 10)\n",
    "\n",
    "\n",
    "\n",
    "# 예측값과 실제 값의 비교\n",
    "Y_prediction = model.predict(X_test).flatten()\n",
    "\n",
    "# for 구문을 사용해서 10번 반복\n",
    "# label = 실제값\n",
    "# prediction = 예측값\n",
    "\n",
    "for i in range(10):\n",
    "    # i는 500개 데이터에서 각각 행\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print('실제가격 : {:.3f}, 예상가격: {:.3f}'.format(label,prediction))\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################################\n",
    "\n",
    "train / test 분리하는 이유?\n",
    "\n",
    "- 머신러닝 모델에 train 데이터를 100% 학습시킨 후 test 데이터에 모델을 적용했을 때 성능이 생각보다 안나오는 경우 -----> overfitting\n",
    "- 즉, 모델이 내가 가진 학습 데이터에 너무 과적합되도록 학습한 나머지, 이를 조금이라도 벗어난 케이스에 대해서는 예측율이 현저히 떨어지기 때문이라고 이해하시면 됩니다. 그렇기 때문에 Overfitting을 방지하는 것은 전체적인 모델 성능을 따져보았을 때 매우 중요한 프로세스 중 하나이다.\n",
    "![image.png](attachment:image.png)\n",
    "- 위의 그림과 같이 기존 train / test로 구분 되어 있었던 데이터 셋을 train에서 train / validation으로 일정 비율 쪼갠 다음에 학습 시에는 train 셋으로 학습 후 중간중간 validation 셋으로 내가 학습한 모델 평가를 해주는 것\n",
    "- 모델이 과적합되었다면, validation 셋으로 검증시 예측율이나 오차율이 떨어지는 현상을 확인할 수 있으며, 이런 현상이 나타나면 학습을 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################################\n",
    "\n",
    "model.complie에서 왜 metrics가 없는지?\n",
    "\n",
    "- metrics는 실제 화면상으로 출력되는 output을 표현한다. 여기에 metrics = ['accuracy']를 넣어주면 학습이 잘 되었는지, 안되었는지 판단하는 기준이된다.\n",
    "- 선형 회귀는 0과 1의 값으로 나뉘는 이진분류가 아니기에 정확성을 출력하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 ) keras를 사용한 pima indian 당뇨병 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       768 non-null    int64  \n",
      " 1   1       768 non-null    int64  \n",
      " 2   2       768 non-null    int64  \n",
      " 3   3       768 non-null    int64  \n",
      " 4   4       768 non-null    int64  \n",
      " 5   5       768 non-null    float64\n",
      " 6   6       768 non-null    float64\n",
      " 7   7       768 non-null    int64  \n",
      " 8   8       768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "None\n",
      "Epoch 1/500\n",
      "54/54 [==============================] - 0s 601us/step - loss: 4.5527 - accuracy: 0.5419\n",
      "Epoch 2/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 2.8809 - accuracy: 0.5661\n",
      "Epoch 3/500\n",
      "54/54 [==============================] - 0s 683us/step - loss: 2.5973 - accuracy: 0.5773\n",
      "Epoch 4/500\n",
      "54/54 [==============================] - 0s 633us/step - loss: 2.4628 - accuracy: 0.5624\n",
      "Epoch 5/500\n",
      "54/54 [==============================] - 0s 608us/step - loss: 2.1208 - accuracy: 0.5717\n",
      "Epoch 6/500\n",
      "54/54 [==============================] - 0s 652us/step - loss: 1.9358 - accuracy: 0.5568\n",
      "Epoch 7/500\n",
      "54/54 [==============================] - 0s 633us/step - loss: 1.7208 - accuracy: 0.5549\n",
      "Epoch 8/500\n",
      "54/54 [==============================] - 0s 652us/step - loss: 1.5604 - accuracy: 0.5736\n",
      "Epoch 9/500\n",
      "54/54 [==============================] - 0s 621us/step - loss: 1.4516 - accuracy: 0.5661\n",
      "Epoch 10/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 1.3282 - accuracy: 0.5587\n",
      "Epoch 11/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 1.2547 - accuracy: 0.5382\n",
      "Epoch 12/500\n",
      "54/54 [==============================] - 0s 579us/step - loss: 1.1945 - accuracy: 0.5791\n",
      "Epoch 13/500\n",
      "54/54 [==============================] - 0s 593us/step - loss: 1.0932 - accuracy: 0.5680\n",
      "Epoch 14/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 1.0196 - accuracy: 0.5829\n",
      "Epoch 15/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 1.0050 - accuracy: 0.5829\n",
      "Epoch 16/500\n",
      "54/54 [==============================] - 0s 622us/step - loss: 0.9728 - accuracy: 0.5903\n",
      "Epoch 17/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.8989 - accuracy: 0.5996\n",
      "Epoch 18/500\n",
      "54/54 [==============================] - 0s 611us/step - loss: 0.8856 - accuracy: 0.6052\n",
      "Epoch 19/500\n",
      "54/54 [==============================] - 0s 627us/step - loss: 0.8690 - accuracy: 0.6108\n",
      "Epoch 20/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 0.8542 - accuracy: 0.6034\n",
      "Epoch 21/500\n",
      "54/54 [==============================] - 0s 637us/step - loss: 0.7899 - accuracy: 0.6071\n",
      "Epoch 22/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.8063 - accuracy: 0.5978\n",
      "Epoch 23/500\n",
      "54/54 [==============================] - 0s 601us/step - loss: 0.7651 - accuracy: 0.6127\n",
      "Epoch 24/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.7681 - accuracy: 0.6238\n",
      "Epoch 25/500\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.7277 - accuracy: 0.6238\n",
      "Epoch 26/500\n",
      "54/54 [==============================] - 0s 611us/step - loss: 0.7294 - accuracy: 0.6182\n",
      "Epoch 27/500\n",
      "54/54 [==============================] - 0s 616us/step - loss: 0.7393 - accuracy: 0.6313\n",
      "Epoch 28/500\n",
      "54/54 [==============================] - 0s 601us/step - loss: 0.7308 - accuracy: 0.6331\n",
      "Epoch 29/500\n",
      "54/54 [==============================] - 0s 639us/step - loss: 0.7060 - accuracy: 0.6387\n",
      "Epoch 30/500\n",
      "54/54 [==============================] - 0s 618us/step - loss: 0.6974 - accuracy: 0.6331\n",
      "Epoch 31/500\n",
      "54/54 [==============================] - 0s 651us/step - loss: 0.6815 - accuracy: 0.6313\n",
      "Epoch 32/500\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.6954 - accuracy: 0.6592\n",
      "Epoch 33/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.6846 - accuracy: 0.6555\n",
      "Epoch 34/500\n",
      "54/54 [==============================] - 0s 650us/step - loss: 0.7088 - accuracy: 0.6462\n",
      "Epoch 35/500\n",
      "54/54 [==============================] - 0s 667us/step - loss: 0.7026 - accuracy: 0.6406\n",
      "Epoch 36/500\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.7816 - accuracy: 0.60 - 0s 663us/step - loss: 0.6774 - accuracy: 0.6425\n",
      "Epoch 37/500\n",
      "54/54 [==============================] - 0s 644us/step - loss: 0.6475 - accuracy: 0.6462\n",
      "Epoch 38/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.6608 - accuracy: 0.6667\n",
      "Epoch 39/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 0.6468 - accuracy: 0.6518\n",
      "Epoch 40/500\n",
      "54/54 [==============================] - 0s 603us/step - loss: 0.6500 - accuracy: 0.6555\n",
      "Epoch 41/500\n",
      "54/54 [==============================] - 0s 613us/step - loss: 0.6762 - accuracy: 0.6276\n",
      "Epoch 42/500\n",
      "54/54 [==============================] - 0s 639us/step - loss: 0.6471 - accuracy: 0.6611\n",
      "Epoch 43/500\n",
      "54/54 [==============================] - 0s 643us/step - loss: 0.6423 - accuracy: 0.6611\n",
      "Epoch 44/500\n",
      "54/54 [==============================] - 0s 613us/step - loss: 0.6293 - accuracy: 0.6760\n",
      "Epoch 45/500\n",
      "54/54 [==============================] - 0s 642us/step - loss: 0.6316 - accuracy: 0.6983\n",
      "Epoch 46/500\n",
      "54/54 [==============================] - 0s 620us/step - loss: 0.6227 - accuracy: 0.6834\n",
      "Epoch 47/500\n",
      "54/54 [==============================] - 0s 633us/step - loss: 0.6349 - accuracy: 0.6704\n",
      "Epoch 48/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.6185 - accuracy: 0.6872\n",
      "Epoch 49/500\n",
      "54/54 [==============================] - 0s 648us/step - loss: 0.6333 - accuracy: 0.6797\n",
      "Epoch 50/500\n",
      "54/54 [==============================] - 0s 624us/step - loss: 0.6192 - accuracy: 0.6853\n",
      "Epoch 51/500\n",
      "54/54 [==============================] - 0s 623us/step - loss: 0.6196 - accuracy: 0.6797\n",
      "Epoch 52/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.6255 - accuracy: 0.6853\n",
      "Epoch 53/500\n",
      "54/54 [==============================] - 0s 567us/step - loss: 0.6122 - accuracy: 0.7412\n",
      "Epoch 54/500\n",
      "54/54 [==============================] - 0s 623us/step - loss: 0.6626 - accuracy: 0.6723\n",
      "Epoch 55/500\n",
      "54/54 [==============================] - 0s 574us/step - loss: 0.6233 - accuracy: 0.6909\n",
      "Epoch 56/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.6209 - accuracy: 0.7058\n",
      "Epoch 57/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 0.6127 - accuracy: 0.6853\n",
      "Epoch 58/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.6054 - accuracy: 0.7132\n",
      "Epoch 59/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 0.5985 - accuracy: 0.7132\n",
      "Epoch 60/500\n",
      "54/54 [==============================] - 0s 612us/step - loss: 0.6227 - accuracy: 0.6983\n",
      "Epoch 61/500\n",
      "54/54 [==============================] - 0s 648us/step - loss: 0.6314 - accuracy: 0.6667\n",
      "Epoch 62/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.6520 - accuracy: 0.6667\n",
      "Epoch 63/500\n",
      "54/54 [==============================] - 0s 623us/step - loss: 0.6454 - accuracy: 0.6778\n",
      "Epoch 64/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.6617 - accuracy: 0.6667\n",
      "Epoch 65/500\n",
      "54/54 [==============================] - 0s 633us/step - loss: 0.6027 - accuracy: 0.7002\n",
      "Epoch 66/500\n",
      "54/54 [==============================] - 0s 627us/step - loss: 0.6200 - accuracy: 0.7002\n",
      "Epoch 67/500\n",
      "54/54 [==============================] - 0s 668us/step - loss: 0.6458 - accuracy: 0.6816\n",
      "Epoch 68/500\n",
      "54/54 [==============================] - 0s 651us/step - loss: 0.5944 - accuracy: 0.7207\n",
      "Epoch 69/500\n",
      "54/54 [==============================] - 0s 653us/step - loss: 0.6327 - accuracy: 0.6816\n",
      "Epoch 70/500\n",
      "54/54 [==============================] - 0s 633us/step - loss: 0.6217 - accuracy: 0.6834\n",
      "Epoch 71/500\n",
      "54/54 [==============================] - 0s 637us/step - loss: 0.6003 - accuracy: 0.7020\n",
      "Epoch 72/500\n",
      "54/54 [==============================] - 0s 662us/step - loss: 0.6004 - accuracy: 0.6965\n",
      "Epoch 73/500\n",
      "54/54 [==============================] - 0s 653us/step - loss: 0.5932 - accuracy: 0.6890\n",
      "Epoch 74/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.6014 - accuracy: 0.7132\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 642us/step - loss: 0.6344 - accuracy: 0.6760\n",
      "Epoch 76/500\n",
      "54/54 [==============================] - 0s 648us/step - loss: 0.5930 - accuracy: 0.7058\n",
      "Epoch 77/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.6100 - accuracy: 0.6853\n",
      "Epoch 78/500\n",
      "54/54 [==============================] - 0s 667us/step - loss: 0.5832 - accuracy: 0.7225\n",
      "Epoch 79/500\n",
      "54/54 [==============================] - 0s 651us/step - loss: 0.6295 - accuracy: 0.6592\n",
      "Epoch 80/500\n",
      "54/54 [==============================] - 0s 639us/step - loss: 0.5860 - accuracy: 0.7095\n",
      "Epoch 81/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.5919 - accuracy: 0.7169\n",
      "Epoch 82/500\n",
      "54/54 [==============================] - 0s 631us/step - loss: 0.5778 - accuracy: 0.7263\n",
      "Epoch 83/500\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.5814 - accuracy: 0.7188\n",
      "Epoch 84/500\n",
      "54/54 [==============================] - 0s 613us/step - loss: 0.5762 - accuracy: 0.7151\n",
      "Epoch 85/500\n",
      "54/54 [==============================] - 0s 650us/step - loss: 0.6330 - accuracy: 0.6667\n",
      "Epoch 86/500\n",
      "54/54 [==============================] - 0s 622us/step - loss: 0.6439 - accuracy: 0.6872\n",
      "Epoch 87/500\n",
      "54/54 [==============================] - 0s 653us/step - loss: 0.6043 - accuracy: 0.6890\n",
      "Epoch 88/500\n",
      "54/54 [==============================] - 0s 667us/step - loss: 0.5759 - accuracy: 0.7244\n",
      "Epoch 89/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.5722 - accuracy: 0.7188\n",
      "Epoch 90/500\n",
      "54/54 [==============================] - 0s 654us/step - loss: 0.6187 - accuracy: 0.7002\n",
      "Epoch 91/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.6355 - accuracy: 0.6983\n",
      "Epoch 92/500\n",
      "54/54 [==============================] - 0s 660us/step - loss: 0.5734 - accuracy: 0.7076\n",
      "Epoch 93/500\n",
      "54/54 [==============================] - 0s 576us/step - loss: 0.5768 - accuracy: 0.7020\n",
      "Epoch 94/500\n",
      "54/54 [==============================] - 0s 624us/step - loss: 0.6128 - accuracy: 0.7020\n",
      "Epoch 95/500\n",
      "54/54 [==============================] - 0s 623us/step - loss: 0.5740 - accuracy: 0.7337\n",
      "Epoch 96/500\n",
      "54/54 [==============================] - 0s 600us/step - loss: 0.5640 - accuracy: 0.7151\n",
      "Epoch 97/500\n",
      "54/54 [==============================] - 0s 618us/step - loss: 0.5916 - accuracy: 0.7039\n",
      "Epoch 98/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.5785 - accuracy: 0.7114\n",
      "Epoch 99/500\n",
      "54/54 [==============================] - 0s 605us/step - loss: 0.5632 - accuracy: 0.7188\n",
      "Epoch 100/500\n",
      "54/54 [==============================] - 0s 631us/step - loss: 0.5687 - accuracy: 0.7281\n",
      "Epoch 101/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.5754 - accuracy: 0.7281\n",
      "Epoch 102/500\n",
      "54/54 [==============================] - 0s 656us/step - loss: 0.5808 - accuracy: 0.7169\n",
      "Epoch 103/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 0.6051 - accuracy: 0.7002\n",
      "Epoch 104/500\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.5633 - accuracy: 0.7188\n",
      "Epoch 105/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.5892 - accuracy: 0.7020\n",
      "Epoch 106/500\n",
      "54/54 [==============================] - 0s 672us/step - loss: 0.5594 - accuracy: 0.7337\n",
      "Epoch 107/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.5659 - accuracy: 0.7225\n",
      "Epoch 108/500\n",
      "54/54 [==============================] - 0s 619us/step - loss: 0.5620 - accuracy: 0.7132\n",
      "Epoch 109/500\n",
      "54/54 [==============================] - 0s 650us/step - loss: 0.5817 - accuracy: 0.7132\n",
      "Epoch 110/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.5701 - accuracy: 0.7281\n",
      "Epoch 111/500\n",
      "54/54 [==============================] - 0s 667us/step - loss: 0.5843 - accuracy: 0.7039\n",
      "Epoch 112/500\n",
      "54/54 [==============================] - 0s 627us/step - loss: 0.5645 - accuracy: 0.7281\n",
      "Epoch 113/500\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.5979 - accuracy: 0.7020\n",
      "Epoch 114/500\n",
      "54/54 [==============================] - 0s 624us/step - loss: 0.5799 - accuracy: 0.7114\n",
      "Epoch 115/500\n",
      "54/54 [==============================] - 0s 655us/step - loss: 0.5608 - accuracy: 0.7207\n",
      "Epoch 116/500\n",
      "54/54 [==============================] - 0s 664us/step - loss: 0.5761 - accuracy: 0.7076\n",
      "Epoch 117/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.5736 - accuracy: 0.7151\n",
      "Epoch 118/500\n",
      "54/54 [==============================] - 0s 642us/step - loss: 0.5860 - accuracy: 0.7058\n",
      "Epoch 119/500\n",
      "54/54 [==============================] - 0s 643us/step - loss: 0.5648 - accuracy: 0.7058\n",
      "Epoch 120/500\n",
      "54/54 [==============================] - 0s 615us/step - loss: 0.5804 - accuracy: 0.7169\n",
      "Epoch 121/500\n",
      "54/54 [==============================] - 0s 653us/step - loss: 0.5625 - accuracy: 0.7095\n",
      "Epoch 122/500\n",
      "54/54 [==============================] - 0s 666us/step - loss: 0.5489 - accuracy: 0.7449\n",
      "Epoch 123/500\n",
      "54/54 [==============================] - 0s 684us/step - loss: 0.5583 - accuracy: 0.7393\n",
      "Epoch 124/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.5592 - accuracy: 0.7244\n",
      "Epoch 125/500\n",
      "54/54 [==============================] - 0s 650us/step - loss: 0.5538 - accuracy: 0.7263\n",
      "Epoch 126/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.5447 - accuracy: 0.7374\n",
      "Epoch 127/500\n",
      "54/54 [==============================] - 0s 635us/step - loss: 0.5698 - accuracy: 0.7207\n",
      "Epoch 128/500\n",
      "54/54 [==============================] - 0s 627us/step - loss: 0.5547 - accuracy: 0.7300\n",
      "Epoch 129/500\n",
      "54/54 [==============================] - 0s 624us/step - loss: 0.5487 - accuracy: 0.7337\n",
      "Epoch 130/500\n",
      "54/54 [==============================] - 0s 654us/step - loss: 0.6336 - accuracy: 0.6760\n",
      "Epoch 131/500\n",
      "54/54 [==============================] - 0s 591us/step - loss: 0.5611 - accuracy: 0.7374\n",
      "Epoch 132/500\n",
      "54/54 [==============================] - 0s 623us/step - loss: 0.5482 - accuracy: 0.7207\n",
      "Epoch 133/500\n",
      "54/54 [==============================] - 0s 631us/step - loss: 0.5428 - accuracy: 0.7300\n",
      "Epoch 134/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.5567 - accuracy: 0.7281\n",
      "Epoch 135/500\n",
      "54/54 [==============================] - 0s 629us/step - loss: 0.5545 - accuracy: 0.7337\n",
      "Epoch 136/500\n",
      "54/54 [==============================] - 0s 696us/step - loss: 0.5792 - accuracy: 0.7132\n",
      "Epoch 137/500\n",
      "54/54 [==============================] - 0s 637us/step - loss: 0.5677 - accuracy: 0.7020\n",
      "Epoch 138/500\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.5460 - accuracy: 0.7374\n",
      "Epoch 139/500\n",
      "54/54 [==============================] - 0s 653us/step - loss: 0.5640 - accuracy: 0.7318\n",
      "Epoch 140/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 0.5850 - accuracy: 0.7002\n",
      "Epoch 141/500\n",
      "54/54 [==============================] - 0s 644us/step - loss: 0.5489 - accuracy: 0.7393\n",
      "Epoch 142/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 0.5632 - accuracy: 0.7169\n",
      "Epoch 143/500\n",
      "54/54 [==============================] - 0s 635us/step - loss: 0.5441 - accuracy: 0.7281\n",
      "Epoch 144/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.5390 - accuracy: 0.7300\n",
      "Epoch 145/500\n",
      "54/54 [==============================] - 0s 644us/step - loss: 0.5310 - accuracy: 0.7561\n",
      "Epoch 146/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.5464 - accuracy: 0.7337\n",
      "Epoch 147/500\n",
      "54/54 [==============================] - 0s 680us/step - loss: 0.5801 - accuracy: 0.7002\n",
      "Epoch 148/500\n",
      "54/54 [==============================] - 0s 683us/step - loss: 0.5498 - accuracy: 0.7281\n",
      "Epoch 149/500\n",
      "54/54 [==============================] - 0s 675us/step - loss: 0.5444 - accuracy: 0.7393\n",
      "Epoch 150/500\n",
      "54/54 [==============================] - 0s 651us/step - loss: 0.5449 - accuracy: 0.7486\n",
      "Epoch 151/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.5543 - accuracy: 0.7374\n",
      "Epoch 152/500\n",
      "54/54 [==============================] - 0s 654us/step - loss: 0.5469 - accuracy: 0.7412\n",
      "Epoch 153/500\n",
      "54/54 [==============================] - 0s 637us/step - loss: 0.5527 - accuracy: 0.7467\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 645us/step - loss: 0.5799 - accuracy: 0.7188\n",
      "Epoch 155/500\n",
      "54/54 [==============================] - 0s 624us/step - loss: 0.5568 - accuracy: 0.7263\n",
      "Epoch 156/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.6148 - accuracy: 0.7058\n",
      "Epoch 157/500\n",
      "54/54 [==============================] - 0s 635us/step - loss: 0.5449 - accuracy: 0.7598\n",
      "Epoch 158/500\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.5441 - accuracy: 0.7300\n",
      "Epoch 159/500\n",
      "54/54 [==============================] - 0s 616us/step - loss: 0.5283 - accuracy: 0.7356\n",
      "Epoch 160/500\n",
      "54/54 [==============================] - 0s 651us/step - loss: 0.5429 - accuracy: 0.7393\n",
      "Epoch 161/500\n",
      "54/54 [==============================] - 0s 642us/step - loss: 0.5397 - accuracy: 0.7300\n",
      "Epoch 162/500\n",
      "54/54 [==============================] - 0s 653us/step - loss: 0.5306 - accuracy: 0.7449\n",
      "Epoch 163/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.5495 - accuracy: 0.7374\n",
      "Epoch 164/500\n",
      "54/54 [==============================] - 0s 672us/step - loss: 0.5751 - accuracy: 0.6909\n",
      "Epoch 165/500\n",
      "54/54 [==============================] - 0s 562us/step - loss: 0.5710 - accuracy: 0.7151\n",
      "Epoch 166/500\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.5451 - accuracy: 0.7393\n",
      "Epoch 167/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.5387 - accuracy: 0.7598\n",
      "Epoch 168/500\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.5354 - accuracy: 0.7430\n",
      "Epoch 169/500\n",
      "54/54 [==============================] - 0s 615us/step - loss: 0.5258 - accuracy: 0.7467\n",
      "Epoch 170/500\n",
      "54/54 [==============================] - 0s 665us/step - loss: 0.5408 - accuracy: 0.7300\n",
      "Epoch 171/500\n",
      "54/54 [==============================] - 0s 661us/step - loss: 0.5319 - accuracy: 0.7374\n",
      "Epoch 172/500\n",
      "54/54 [==============================] - 0s 642us/step - loss: 0.5467 - accuracy: 0.7281\n",
      "Epoch 173/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.5367 - accuracy: 0.7356\n",
      "Epoch 174/500\n",
      "54/54 [==============================] - 0s 615us/step - loss: 0.5449 - accuracy: 0.7337\n",
      "Epoch 175/500\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.5258 - accuracy: 0.7393\n",
      "Epoch 176/500\n",
      "54/54 [==============================] - 0s 606us/step - loss: 0.5377 - accuracy: 0.7523\n",
      "Epoch 177/500\n",
      "54/54 [==============================] - 0s 631us/step - loss: 0.5295 - accuracy: 0.7561\n",
      "Epoch 178/500\n",
      "54/54 [==============================] - 0s 613us/step - loss: 0.5525 - accuracy: 0.7020\n",
      "Epoch 179/500\n",
      "54/54 [==============================] - 0s 554us/step - loss: 0.5745 - accuracy: 0.7207\n",
      "Epoch 180/500\n",
      "54/54 [==============================] - 0s 648us/step - loss: 0.5421 - accuracy: 0.7467\n",
      "Epoch 181/500\n",
      "54/54 [==============================] - 0s 600us/step - loss: 0.5571 - accuracy: 0.7318\n",
      "Epoch 182/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.5216 - accuracy: 0.7542\n",
      "Epoch 183/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 0.5540 - accuracy: 0.7169\n",
      "Epoch 184/500\n",
      "54/54 [==============================] - 0s 643us/step - loss: 0.5315 - accuracy: 0.7635\n",
      "Epoch 185/500\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.5481 - accuracy: 0.7318\n",
      "Epoch 186/500\n",
      "54/54 [==============================] - 0s 639us/step - loss: 0.5316 - accuracy: 0.7374\n",
      "Epoch 187/500\n",
      "54/54 [==============================] - 0s 644us/step - loss: 0.5339 - accuracy: 0.7374\n",
      "Epoch 188/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.5390 - accuracy: 0.7318\n",
      "Epoch 189/500\n",
      "54/54 [==============================] - 0s 620us/step - loss: 0.5306 - accuracy: 0.7393\n",
      "Epoch 190/500\n",
      "54/54 [==============================] - 0s 653us/step - loss: 0.5181 - accuracy: 0.7616\n",
      "Epoch 191/500\n",
      "54/54 [==============================] - 0s 657us/step - loss: 0.5457 - accuracy: 0.7393\n",
      "Epoch 192/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.5244 - accuracy: 0.7486\n",
      "Epoch 193/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 0.5363 - accuracy: 0.7374\n",
      "Epoch 194/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.5333 - accuracy: 0.7542\n",
      "Epoch 195/500\n",
      "54/54 [==============================] - 0s 617us/step - loss: 0.5704 - accuracy: 0.7263\n",
      "Epoch 196/500\n",
      "54/54 [==============================] - 0s 658us/step - loss: 0.5251 - accuracy: 0.7523\n",
      "Epoch 197/500\n",
      "54/54 [==============================] - 0s 816us/step - loss: 0.5408 - accuracy: 0.7300\n",
      "Epoch 198/500\n",
      "54/54 [==============================] - 0s 664us/step - loss: 0.5288 - accuracy: 0.7393\n",
      "Epoch 199/500\n",
      "54/54 [==============================] - 0s 663us/step - loss: 0.5275 - accuracy: 0.7412\n",
      "Epoch 200/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.5195 - accuracy: 0.7542\n",
      "Epoch 201/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.5384 - accuracy: 0.7467\n",
      "Epoch 202/500\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.3180 - accuracy: 1.00 - 0s 615us/step - loss: 0.5743 - accuracy: 0.7356\n",
      "Epoch 203/500\n",
      "54/54 [==============================] - 0s 629us/step - loss: 0.5533 - accuracy: 0.7281\n",
      "Epoch 204/500\n",
      "54/54 [==============================] - 0s 629us/step - loss: 0.5440 - accuracy: 0.7393\n",
      "Epoch 205/500\n",
      "54/54 [==============================] - 0s 666us/step - loss: 0.5303 - accuracy: 0.7467\n",
      "Epoch 206/500\n",
      "54/54 [==============================] - 0s 627us/step - loss: 0.5378 - accuracy: 0.7505\n",
      "Epoch 207/500\n",
      "54/54 [==============================] - 0s 657us/step - loss: 0.5168 - accuracy: 0.7542\n",
      "Epoch 208/500\n",
      "54/54 [==============================] - 0s 627us/step - loss: 0.5715 - accuracy: 0.7207\n",
      "Epoch 209/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.5315 - accuracy: 0.7393\n",
      "Epoch 210/500\n",
      "54/54 [==============================] - 0s 627us/step - loss: 0.5344 - accuracy: 0.7412\n",
      "Epoch 211/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.5291 - accuracy: 0.7449\n",
      "Epoch 212/500\n",
      "54/54 [==============================] - 0s 667us/step - loss: 0.5504 - accuracy: 0.7300\n",
      "Epoch 213/500\n",
      "54/54 [==============================] - 0s 663us/step - loss: 0.5421 - accuracy: 0.7467\n",
      "Epoch 214/500\n",
      "54/54 [==============================] - 0s 664us/step - loss: 0.5490 - accuracy: 0.7207\n",
      "Epoch 215/500\n",
      "54/54 [==============================] - 0s 651us/step - loss: 0.5385 - accuracy: 0.7281\n",
      "Epoch 216/500\n",
      "54/54 [==============================] - 0s 615us/step - loss: 0.5261 - accuracy: 0.7318\n",
      "Epoch 217/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.5173 - accuracy: 0.7523\n",
      "Epoch 218/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.5505 - accuracy: 0.7169\n",
      "Epoch 219/500\n",
      "54/54 [==============================] - 0s 623us/step - loss: 0.5468 - accuracy: 0.7300\n",
      "Epoch 220/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.5243 - accuracy: 0.7523\n",
      "Epoch 221/500\n",
      "54/54 [==============================] - 0s 664us/step - loss: 0.5245 - accuracy: 0.7430\n",
      "Epoch 222/500\n",
      "54/54 [==============================] - 0s 654us/step - loss: 0.5144 - accuracy: 0.7654\n",
      "Epoch 223/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.5408 - accuracy: 0.7412\n",
      "Epoch 224/500\n",
      "54/54 [==============================] - 0s 651us/step - loss: 0.5511 - accuracy: 0.7188\n",
      "Epoch 225/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.5299 - accuracy: 0.7561\n",
      "Epoch 226/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.5245 - accuracy: 0.7523\n",
      "Epoch 227/500\n",
      "54/54 [==============================] - 0s 684us/step - loss: 0.5130 - accuracy: 0.7449\n",
      "Epoch 228/500\n",
      "54/54 [==============================] - 0s 629us/step - loss: 0.5386 - accuracy: 0.7356\n",
      "Epoch 229/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.5074 - accuracy: 0.7393\n",
      "Epoch 230/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.5123 - accuracy: 0.7579\n",
      "Epoch 231/500\n",
      "54/54 [==============================] - 0s 624us/step - loss: 0.5135 - accuracy: 0.7561\n",
      "Epoch 232/500\n",
      "54/54 [==============================] - 0s 623us/step - loss: 0.5277 - accuracy: 0.7523\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 658us/step - loss: 0.5087 - accuracy: 0.7635\n",
      "Epoch 234/500\n",
      "54/54 [==============================] - 0s 582us/step - loss: 0.5115 - accuracy: 0.7635\n",
      "Epoch 235/500\n",
      "54/54 [==============================] - 0s 683us/step - loss: 0.5478 - accuracy: 0.7356\n",
      "Epoch 236/500\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.5117 - accuracy: 0.7523\n",
      "Epoch 237/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 0.5501 - accuracy: 0.7337\n",
      "Epoch 238/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.5242 - accuracy: 0.7523\n",
      "Epoch 239/500\n",
      "54/54 [==============================] - 0s 635us/step - loss: 0.5213 - accuracy: 0.7356\n",
      "Epoch 240/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.5356 - accuracy: 0.7467\n",
      "Epoch 241/500\n",
      "54/54 [==============================] - 0s 639us/step - loss: 0.5312 - accuracy: 0.7449\n",
      "Epoch 242/500\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.3858 - accuracy: 0.80 - 0s 630us/step - loss: 0.5393 - accuracy: 0.7300\n",
      "Epoch 243/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.5131 - accuracy: 0.7412\n",
      "Epoch 244/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.5106 - accuracy: 0.7412\n",
      "Epoch 245/500\n",
      "54/54 [==============================] - 0s 598us/step - loss: 0.5186 - accuracy: 0.7635\n",
      "Epoch 246/500\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.5366 - accuracy: 0.7449\n",
      "Epoch 247/500\n",
      "54/54 [==============================] - 0s 643us/step - loss: 0.5162 - accuracy: 0.7523\n",
      "Epoch 248/500\n",
      "54/54 [==============================] - 0s 606us/step - loss: 0.5274 - accuracy: 0.7579\n",
      "Epoch 249/500\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.5212 - accuracy: 0.7635\n",
      "Epoch 250/500\n",
      "54/54 [==============================] - 0s 619us/step - loss: 0.5162 - accuracy: 0.7412\n",
      "Epoch 251/500\n",
      "54/54 [==============================] - 0s 622us/step - loss: 0.5015 - accuracy: 0.7579\n",
      "Epoch 252/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.5108 - accuracy: 0.7430\n",
      "Epoch 253/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.5249 - accuracy: 0.7356\n",
      "Epoch 254/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.5281 - accuracy: 0.7561\n",
      "Epoch 255/500\n",
      "54/54 [==============================] - 0s 648us/step - loss: 0.5482 - accuracy: 0.7691\n",
      "Epoch 256/500\n",
      "54/54 [==============================] - 0s 626us/step - loss: 0.5164 - accuracy: 0.7523\n",
      "Epoch 257/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.5145 - accuracy: 0.7505\n",
      "Epoch 258/500\n",
      "54/54 [==============================] - 0s 619us/step - loss: 0.5245 - accuracy: 0.7467\n",
      "Epoch 259/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.5099 - accuracy: 0.7542\n",
      "Epoch 260/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.5183 - accuracy: 0.7486\n",
      "Epoch 261/500\n",
      "54/54 [==============================] - 0s 637us/step - loss: 0.5359 - accuracy: 0.7579\n",
      "Epoch 262/500\n",
      "54/54 [==============================] - 0s 650us/step - loss: 0.5058 - accuracy: 0.7542\n",
      "Epoch 263/500\n",
      "54/54 [==============================] - 0s 694us/step - loss: 0.5215 - accuracy: 0.7635\n",
      "Epoch 264/500\n",
      "54/54 [==============================] - 0s 650us/step - loss: 0.4972 - accuracy: 0.7598\n",
      "Epoch 265/500\n",
      "54/54 [==============================] - 0s 660us/step - loss: 0.5063 - accuracy: 0.7486\n",
      "Epoch 266/500\n",
      "54/54 [==============================] - 0s 660us/step - loss: 0.5054 - accuracy: 0.7486\n",
      "Epoch 267/500\n",
      "54/54 [==============================] - 0s 635us/step - loss: 0.5292 - accuracy: 0.7449\n",
      "Epoch 268/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.5048 - accuracy: 0.7765\n",
      "Epoch 269/500\n",
      "54/54 [==============================] - 0s 593us/step - loss: 0.5063 - accuracy: 0.7523\n",
      "Epoch 270/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.5177 - accuracy: 0.7561\n",
      "Epoch 271/500\n",
      "54/54 [==============================] - 0s 633us/step - loss: 0.5355 - accuracy: 0.7207\n",
      "Epoch 272/500\n",
      "54/54 [==============================] - 0s 624us/step - loss: 0.5094 - accuracy: 0.7430\n",
      "Epoch 273/500\n",
      "54/54 [==============================] - 0s 623us/step - loss: 0.5194 - accuracy: 0.7449\n",
      "Epoch 274/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.5158 - accuracy: 0.7467\n",
      "Epoch 275/500\n",
      "54/54 [==============================] - 0s 651us/step - loss: 0.5505 - accuracy: 0.7318\n",
      "Epoch 276/500\n",
      "54/54 [==============================] - 0s 637us/step - loss: 0.5273 - accuracy: 0.7654\n",
      "Epoch 277/500\n",
      "54/54 [==============================] - 0s 610us/step - loss: 0.5022 - accuracy: 0.7430\n",
      "Epoch 278/500\n",
      "54/54 [==============================] - 0s 617us/step - loss: 0.5003 - accuracy: 0.7728\n",
      "Epoch 279/500\n",
      "54/54 [==============================] - 0s 660us/step - loss: 0.5072 - accuracy: 0.7523\n",
      "Epoch 280/500\n",
      "54/54 [==============================] - 0s 648us/step - loss: 0.5301 - accuracy: 0.7430\n",
      "Epoch 281/500\n",
      "54/54 [==============================] - 0s 660us/step - loss: 0.5036 - accuracy: 0.7579\n",
      "Epoch 282/500\n",
      "54/54 [==============================] - 0s 677us/step - loss: 0.5028 - accuracy: 0.7374\n",
      "Epoch 283/500\n",
      "54/54 [==============================] - 0s 640us/step - loss: 0.5022 - accuracy: 0.7672\n",
      "Epoch 284/500\n",
      "54/54 [==============================] - 0s 648us/step - loss: 0.5009 - accuracy: 0.7579\n",
      "Epoch 285/500\n",
      "54/54 [==============================] - 0s 613us/step - loss: 0.4990 - accuracy: 0.7449\n",
      "Epoch 286/500\n",
      "54/54 [==============================] - 0s 607us/step - loss: 0.5513 - accuracy: 0.7337\n",
      "Epoch 287/500\n",
      "54/54 [==============================] - 0s 619us/step - loss: 0.5010 - accuracy: 0.7598\n",
      "Epoch 288/500\n",
      "54/54 [==============================] - 0s 643us/step - loss: 0.5114 - accuracy: 0.7523\n",
      "Epoch 289/500\n",
      "54/54 [==============================] - 0s 603us/step - loss: 0.5162 - accuracy: 0.7393\n",
      "Epoch 290/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.5050 - accuracy: 0.7486\n",
      "Epoch 291/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 0.5124 - accuracy: 0.7467\n",
      "Epoch 292/500\n",
      "54/54 [==============================] - 0s 609us/step - loss: 0.5089 - accuracy: 0.7635\n",
      "Epoch 293/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.4997 - accuracy: 0.7523\n",
      "Epoch 294/500\n",
      "54/54 [==============================] - 0s 658us/step - loss: 0.4945 - accuracy: 0.7672\n",
      "Epoch 295/500\n",
      "54/54 [==============================] - 0s 620us/step - loss: 0.4978 - accuracy: 0.7635\n",
      "Epoch 296/500\n",
      "54/54 [==============================] - 0s 627us/step - loss: 0.4934 - accuracy: 0.7691\n",
      "Epoch 297/500\n",
      "54/54 [==============================] - 0s 619us/step - loss: 0.4985 - accuracy: 0.7672\n",
      "Epoch 298/500\n",
      "54/54 [==============================] - 0s 660us/step - loss: 0.5084 - accuracy: 0.7709\n",
      "Epoch 299/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.5290 - accuracy: 0.7486\n",
      "Epoch 300/500\n",
      "54/54 [==============================] - 0s 616us/step - loss: 0.4869 - accuracy: 0.7728\n",
      "Epoch 301/500\n",
      "54/54 [==============================] - 0s 654us/step - loss: 0.5065 - accuracy: 0.7412\n",
      "Epoch 302/500\n",
      "54/54 [==============================] - 0s 662us/step - loss: 0.5105 - accuracy: 0.7486\n",
      "Epoch 303/500\n",
      "54/54 [==============================] - 0s 642us/step - loss: 0.5002 - accuracy: 0.7542\n",
      "Epoch 304/500\n",
      "54/54 [==============================] - 0s 615us/step - loss: 0.5185 - accuracy: 0.7393\n",
      "Epoch 305/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 0.5044 - accuracy: 0.7486\n",
      "Epoch 306/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.4969 - accuracy: 0.7672\n",
      "Epoch 307/500\n",
      "54/54 [==============================] - 0s 665us/step - loss: 0.4928 - accuracy: 0.7635\n",
      "Epoch 308/500\n",
      "54/54 [==============================] - 0s 675us/step - loss: 0.5238 - accuracy: 0.7486\n",
      "Epoch 309/500\n",
      "54/54 [==============================] - 0s 672us/step - loss: 0.4994 - accuracy: 0.7635\n",
      "Epoch 310/500\n",
      "54/54 [==============================] - 0s 642us/step - loss: 0.4994 - accuracy: 0.7672\n",
      "Epoch 311/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.4870 - accuracy: 0.7803\n",
      "Epoch 312/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 682us/step - loss: 0.5149 - accuracy: 0.7430\n",
      "Epoch 313/500\n",
      "54/54 [==============================] - 0s 692us/step - loss: 0.4948 - accuracy: 0.7598\n",
      "Epoch 314/500\n",
      "54/54 [==============================] - 0s 683us/step - loss: 0.5016 - accuracy: 0.7691\n",
      "Epoch 315/500\n",
      "54/54 [==============================] - 0s 665us/step - loss: 0.4907 - accuracy: 0.7691\n",
      "Epoch 316/500\n",
      "54/54 [==============================] - 0s 691us/step - loss: 0.5105 - accuracy: 0.7579\n",
      "Epoch 317/500\n",
      "54/54 [==============================] - 0s 715us/step - loss: 0.5066 - accuracy: 0.7318\n",
      "Epoch 318/500\n",
      "54/54 [==============================] - 0s 730us/step - loss: 0.5040 - accuracy: 0.7561\n",
      "Epoch 319/500\n",
      "54/54 [==============================] - 0s 724us/step - loss: 0.5080 - accuracy: 0.7561\n",
      "Epoch 320/500\n",
      "54/54 [==============================] - 0s 660us/step - loss: 0.4894 - accuracy: 0.7691\n",
      "Epoch 321/500\n",
      "54/54 [==============================] - 0s 683us/step - loss: 0.4954 - accuracy: 0.7561\n",
      "Epoch 322/500\n",
      "54/54 [==============================] - 0s 695us/step - loss: 0.5065 - accuracy: 0.7505\n",
      "Epoch 323/500\n",
      "54/54 [==============================] - 0s 674us/step - loss: 0.5108 - accuracy: 0.7467\n",
      "Epoch 324/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.4875 - accuracy: 0.7579\n",
      "Epoch 325/500\n",
      "54/54 [==============================] - 0s 609us/step - loss: 0.4895 - accuracy: 0.7709\n",
      "Epoch 326/500\n",
      "54/54 [==============================] - 0s 626us/step - loss: 0.5310 - accuracy: 0.7318\n",
      "Epoch 327/500\n",
      "54/54 [==============================] - 0s 610us/step - loss: 0.5078 - accuracy: 0.7542\n",
      "Epoch 328/500\n",
      "54/54 [==============================] - 0s 586us/step - loss: 0.5175 - accuracy: 0.7318\n",
      "Epoch 329/500\n",
      "54/54 [==============================] - 0s 615us/step - loss: 0.5092 - accuracy: 0.7467\n",
      "Epoch 330/500\n",
      "54/54 [==============================] - 0s 592us/step - loss: 0.4962 - accuracy: 0.7523\n",
      "Epoch 331/500\n",
      "54/54 [==============================] - 0s 542us/step - loss: 0.4868 - accuracy: 0.7877\n",
      "Epoch 332/500\n",
      "54/54 [==============================] - 0s 493us/step - loss: 0.4994 - accuracy: 0.7486\n",
      "Epoch 333/500\n",
      "54/54 [==============================] - 0s 517us/step - loss: 0.5010 - accuracy: 0.7598\n",
      "Epoch 334/500\n",
      "54/54 [==============================] - 0s 493us/step - loss: 0.5056 - accuracy: 0.7579\n",
      "Epoch 335/500\n",
      "54/54 [==============================] - 0s 510us/step - loss: 0.5082 - accuracy: 0.7467\n",
      "Epoch 336/500\n",
      "54/54 [==============================] - 0s 515us/step - loss: 0.4874 - accuracy: 0.7747\n",
      "Epoch 337/500\n",
      "54/54 [==============================] - 0s 610us/step - loss: 0.4967 - accuracy: 0.7728\n",
      "Epoch 338/500\n",
      "54/54 [==============================] - 0s 566us/step - loss: 0.4918 - accuracy: 0.7672\n",
      "Epoch 339/500\n",
      "54/54 [==============================] - 0s 662us/step - loss: 0.4966 - accuracy: 0.7598\n",
      "Epoch 340/500\n",
      "54/54 [==============================] - 0s 610us/step - loss: 0.5016 - accuracy: 0.7505\n",
      "Epoch 341/500\n",
      "54/54 [==============================] - 0s 811us/step - loss: 0.5054 - accuracy: 0.7486\n",
      "Epoch 342/500\n",
      "54/54 [==============================] - 0s 589us/step - loss: 0.4893 - accuracy: 0.7598\n",
      "Epoch 343/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.5101 - accuracy: 0.7393\n",
      "Epoch 344/500\n",
      "54/54 [==============================] - 0s 652us/step - loss: 0.5118 - accuracy: 0.7337\n",
      "Epoch 345/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.4967 - accuracy: 0.7598\n",
      "Epoch 346/500\n",
      "54/54 [==============================] - 0s 618us/step - loss: 0.4950 - accuracy: 0.7691\n",
      "Epoch 347/500\n",
      "54/54 [==============================] - 0s 624us/step - loss: 0.4933 - accuracy: 0.7542\n",
      "Epoch 348/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.4928 - accuracy: 0.7654\n",
      "Epoch 349/500\n",
      "54/54 [==============================] - 0s 609us/step - loss: 0.4893 - accuracy: 0.7616\n",
      "Epoch 350/500\n",
      "54/54 [==============================] - 0s 637us/step - loss: 0.5632 - accuracy: 0.7393\n",
      "Epoch 351/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.5064 - accuracy: 0.7523\n",
      "Epoch 352/500\n",
      "54/54 [==============================] - 0s 642us/step - loss: 0.4892 - accuracy: 0.7821\n",
      "Epoch 353/500\n",
      "54/54 [==============================] - 0s 608us/step - loss: 0.4999 - accuracy: 0.7654\n",
      "Epoch 354/500\n",
      "54/54 [==============================] - 0s 617us/step - loss: 0.4842 - accuracy: 0.7542\n",
      "Epoch 355/500\n",
      "54/54 [==============================] - 0s 629us/step - loss: 0.4908 - accuracy: 0.7728\n",
      "Epoch 356/500\n",
      "54/54 [==============================] - 0s 648us/step - loss: 0.4991 - accuracy: 0.7505\n",
      "Epoch 357/500\n",
      "54/54 [==============================] - 0s 642us/step - loss: 0.4877 - accuracy: 0.7691\n",
      "Epoch 358/500\n",
      "54/54 [==============================] - 0s 644us/step - loss: 0.4830 - accuracy: 0.7635\n",
      "Epoch 359/500\n",
      "54/54 [==============================] - 0s 643us/step - loss: 0.4980 - accuracy: 0.7486\n",
      "Epoch 360/500\n",
      "54/54 [==============================] - 0s 644us/step - loss: 0.5298 - accuracy: 0.7430\n",
      "Epoch 361/500\n",
      "54/54 [==============================] - 0s 644us/step - loss: 0.5415 - accuracy: 0.7356\n",
      "Epoch 362/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.4806 - accuracy: 0.7635\n",
      "Epoch 363/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.5022 - accuracy: 0.7579\n",
      "Epoch 364/500\n",
      "54/54 [==============================] - 0s 611us/step - loss: 0.4854 - accuracy: 0.7598\n",
      "Epoch 365/500\n",
      "54/54 [==============================] - 0s 612us/step - loss: 0.4971 - accuracy: 0.7635\n",
      "Epoch 366/500\n",
      "54/54 [==============================] - 0s 616us/step - loss: 0.5114 - accuracy: 0.7505\n",
      "Epoch 367/500\n",
      "54/54 [==============================] - 0s 604us/step - loss: 0.4885 - accuracy: 0.7672\n",
      "Epoch 368/500\n",
      "54/54 [==============================] - 0s 612us/step - loss: 0.4953 - accuracy: 0.7579\n",
      "Epoch 369/500\n",
      "54/54 [==============================] - 0s 597us/step - loss: 0.5024 - accuracy: 0.7505\n",
      "Epoch 370/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.4974 - accuracy: 0.7561\n",
      "Epoch 371/500\n",
      "54/54 [==============================] - 0s 614us/step - loss: 0.4879 - accuracy: 0.7561\n",
      "Epoch 372/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.4898 - accuracy: 0.7709\n",
      "Epoch 373/500\n",
      "54/54 [==============================] - 0s 639us/step - loss: 0.5132 - accuracy: 0.7561\n",
      "Epoch 374/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.4979 - accuracy: 0.7467\n",
      "Epoch 375/500\n",
      "54/54 [==============================] - 0s 556us/step - loss: 0.5043 - accuracy: 0.7467\n",
      "Epoch 376/500\n",
      "54/54 [==============================] - 0s 643us/step - loss: 0.4894 - accuracy: 0.7709\n",
      "Epoch 377/500\n",
      "54/54 [==============================] - 0s 635us/step - loss: 0.5030 - accuracy: 0.7654\n",
      "Epoch 378/500\n",
      "54/54 [==============================] - 0s 633us/step - loss: 0.4803 - accuracy: 0.7765\n",
      "Epoch 379/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.5158 - accuracy: 0.7598\n",
      "Epoch 380/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.4998 - accuracy: 0.7505\n",
      "Epoch 381/500\n",
      "54/54 [==============================] - 0s 633us/step - loss: 0.4769 - accuracy: 0.7784\n",
      "Epoch 382/500\n",
      "54/54 [==============================] - 0s 627us/step - loss: 0.5009 - accuracy: 0.7579\n",
      "Epoch 383/500\n",
      "54/54 [==============================] - 0s 584us/step - loss: 0.4911 - accuracy: 0.7765\n",
      "Epoch 384/500\n",
      "54/54 [==============================] - 0s 555us/step - loss: 0.4922 - accuracy: 0.7728\n",
      "Epoch 385/500\n",
      "54/54 [==============================] - 0s 653us/step - loss: 0.4888 - accuracy: 0.7691\n",
      "Epoch 386/500\n",
      "54/54 [==============================] - 0s 663us/step - loss: 0.5007 - accuracy: 0.7579\n",
      "Epoch 387/500\n",
      "54/54 [==============================] - 0s 657us/step - loss: 0.4849 - accuracy: 0.7691\n",
      "Epoch 388/500\n",
      "54/54 [==============================] - 0s 679us/step - loss: 0.4930 - accuracy: 0.7561\n",
      "Epoch 389/500\n",
      "54/54 [==============================] - 0s 670us/step - loss: 0.4897 - accuracy: 0.7691\n",
      "Epoch 390/500\n",
      "54/54 [==============================] - 0s 666us/step - loss: 0.4942 - accuracy: 0.7728\n",
      "Epoch 391/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 630us/step - loss: 0.4873 - accuracy: 0.7672\n",
      "Epoch 392/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.5222 - accuracy: 0.7412\n",
      "Epoch 393/500\n",
      "54/54 [==============================] - 0s 556us/step - loss: 0.5038 - accuracy: 0.7486\n",
      "Epoch 394/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.4819 - accuracy: 0.7747\n",
      "Epoch 395/500\n",
      "54/54 [==============================] - 0s 629us/step - loss: 0.5066 - accuracy: 0.7654\n",
      "Epoch 396/500\n",
      "54/54 [==============================] - 0s 696us/step - loss: 0.5197 - accuracy: 0.7393\n",
      "Epoch 397/500\n",
      "54/54 [==============================] - 0s 696us/step - loss: 0.5136 - accuracy: 0.7412\n",
      "Epoch 398/500\n",
      "54/54 [==============================] - 0s 663us/step - loss: 0.4851 - accuracy: 0.7635\n",
      "Epoch 399/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 0.5037 - accuracy: 0.7579\n",
      "Epoch 400/500\n",
      "54/54 [==============================] - 0s 628us/step - loss: 0.4830 - accuracy: 0.7709\n",
      "Epoch 401/500\n",
      "54/54 [==============================] - 0s 637us/step - loss: 0.4815 - accuracy: 0.7747\n",
      "Epoch 402/500\n",
      "54/54 [==============================] - 0s 644us/step - loss: 0.5038 - accuracy: 0.7598\n",
      "Epoch 403/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.4904 - accuracy: 0.7561\n",
      "Epoch 404/500\n",
      "54/54 [==============================] - 0s 665us/step - loss: 0.4874 - accuracy: 0.7654\n",
      "Epoch 405/500\n",
      "54/54 [==============================] - 0s 656us/step - loss: 0.4917 - accuracy: 0.7579\n",
      "Epoch 406/500\n",
      "54/54 [==============================] - 0s 673us/step - loss: 0.5092 - accuracy: 0.7467\n",
      "Epoch 407/500\n",
      "54/54 [==============================] - 0s 694us/step - loss: 0.4792 - accuracy: 0.7765\n",
      "Epoch 408/500\n",
      "54/54 [==============================] - 0s 626us/step - loss: 0.4932 - accuracy: 0.7467\n",
      "Epoch 409/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.4957 - accuracy: 0.7654\n",
      "Epoch 410/500\n",
      "54/54 [==============================] - 0s 597us/step - loss: 0.4894 - accuracy: 0.7747\n",
      "Epoch 411/500\n",
      "54/54 [==============================] - 0s 617us/step - loss: 0.4915 - accuracy: 0.7765\n",
      "Epoch 412/500\n",
      "54/54 [==============================] - 0s 656us/step - loss: 0.4943 - accuracy: 0.7616\n",
      "Epoch 413/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.4987 - accuracy: 0.7598\n",
      "Epoch 414/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.4832 - accuracy: 0.7561\n",
      "Epoch 415/500\n",
      "54/54 [==============================] - 0s 631us/step - loss: 0.5018 - accuracy: 0.7561\n",
      "Epoch 416/500\n",
      "54/54 [==============================] - 0s 621us/step - loss: 0.5067 - accuracy: 0.7523\n",
      "Epoch 417/500\n",
      "54/54 [==============================] - 0s 633us/step - loss: 0.4878 - accuracy: 0.7691\n",
      "Epoch 418/500\n",
      "54/54 [==============================] - 0s 601us/step - loss: 0.5047 - accuracy: 0.7579\n",
      "Epoch 419/500\n",
      "54/54 [==============================] - 0s 618us/step - loss: 0.4892 - accuracy: 0.7579\n",
      "Epoch 420/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.5475 - accuracy: 0.7318\n",
      "Epoch 421/500\n",
      "54/54 [==============================] - 0s 666us/step - loss: 0.4982 - accuracy: 0.7486\n",
      "Epoch 422/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.5109 - accuracy: 0.7523\n",
      "Epoch 423/500\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.4965 - accuracy: 0.7579\n",
      "Epoch 424/500\n",
      "54/54 [==============================] - 0s 662us/step - loss: 0.4990 - accuracy: 0.7523\n",
      "Epoch 425/500\n",
      "54/54 [==============================] - 0s 666us/step - loss: 0.4887 - accuracy: 0.7579\n",
      "Epoch 426/500\n",
      "54/54 [==============================] - 0s 626us/step - loss: 0.4897 - accuracy: 0.7616\n",
      "Epoch 427/500\n",
      "54/54 [==============================] - 0s 672us/step - loss: 0.4918 - accuracy: 0.7412\n",
      "Epoch 428/500\n",
      "54/54 [==============================] - 0s 635us/step - loss: 0.4884 - accuracy: 0.7803\n",
      "Epoch 429/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.4908 - accuracy: 0.7709\n",
      "Epoch 430/500\n",
      "54/54 [==============================] - 0s 630us/step - loss: 0.4829 - accuracy: 0.7672\n",
      "Epoch 431/500\n",
      "54/54 [==============================] - 0s 658us/step - loss: 0.5206 - accuracy: 0.7486\n",
      "Epoch 432/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.5158 - accuracy: 0.7337\n",
      "Epoch 433/500\n",
      "54/54 [==============================] - 0s 660us/step - loss: 0.4978 - accuracy: 0.7523\n",
      "Epoch 434/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.4861 - accuracy: 0.7616\n",
      "Epoch 435/500\n",
      "54/54 [==============================] - 0s 614us/step - loss: 0.5023 - accuracy: 0.7672\n",
      "Epoch 436/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 0.4852 - accuracy: 0.7691\n",
      "Epoch 437/500\n",
      "54/54 [==============================] - 0s 617us/step - loss: 0.4916 - accuracy: 0.7561\n",
      "Epoch 438/500\n",
      "54/54 [==============================] - 0s 645us/step - loss: 0.5124 - accuracy: 0.7635\n",
      "Epoch 439/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.5031 - accuracy: 0.7579\n",
      "Epoch 440/500\n",
      "54/54 [==============================] - 0s 614us/step - loss: 0.4968 - accuracy: 0.7616\n",
      "Epoch 441/500\n",
      "54/54 [==============================] - 0s 646us/step - loss: 0.4824 - accuracy: 0.7635\n",
      "Epoch 442/500\n",
      "54/54 [==============================] - 0s 629us/step - loss: 0.4837 - accuracy: 0.7635\n",
      "Epoch 443/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.5149 - accuracy: 0.7393\n",
      "Epoch 444/500\n",
      "54/54 [==============================] - 0s 669us/step - loss: 0.4892 - accuracy: 0.7598\n",
      "Epoch 445/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.4890 - accuracy: 0.7598\n",
      "Epoch 446/500\n",
      "54/54 [==============================] - 0s 629us/step - loss: 0.5283 - accuracy: 0.7356\n",
      "Epoch 447/500\n",
      "54/54 [==============================] - 0s 616us/step - loss: 0.4765 - accuracy: 0.7654\n",
      "Epoch 448/500\n",
      "54/54 [==============================] - 0s 638us/step - loss: 0.4817 - accuracy: 0.7542\n",
      "Epoch 449/500\n",
      "54/54 [==============================] - 0s 614us/step - loss: 0.4986 - accuracy: 0.7561\n",
      "Epoch 450/500\n",
      "54/54 [==============================] - 0s 644us/step - loss: 0.4825 - accuracy: 0.7598\n",
      "Epoch 451/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.4925 - accuracy: 0.7635\n",
      "Epoch 452/500\n",
      "54/54 [==============================] - 0s 617us/step - loss: 0.4819 - accuracy: 0.7654\n",
      "Epoch 453/500\n",
      "54/54 [==============================] - 0s 657us/step - loss: 0.4882 - accuracy: 0.7579\n",
      "Epoch 454/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.4961 - accuracy: 0.7467\n",
      "Epoch 455/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.4887 - accuracy: 0.7598\n",
      "Epoch 456/500\n",
      "54/54 [==============================] - 0s 642us/step - loss: 0.4818 - accuracy: 0.7561\n",
      "Epoch 457/500\n",
      "54/54 [==============================] - 0s 635us/step - loss: 0.4825 - accuracy: 0.7616\n",
      "Epoch 458/500\n",
      "54/54 [==============================] - 0s 632us/step - loss: 0.4791 - accuracy: 0.7561\n",
      "Epoch 459/500\n",
      "54/54 [==============================] - 0s 662us/step - loss: 0.4874 - accuracy: 0.7709\n",
      "Epoch 460/500\n",
      "54/54 [==============================] - 0s 653us/step - loss: 0.4896 - accuracy: 0.7635\n",
      "Epoch 461/500\n",
      "54/54 [==============================] - 0s 606us/step - loss: 0.4940 - accuracy: 0.7765\n",
      "Epoch 462/500\n",
      "54/54 [==============================] - 0s 689us/step - loss: 0.4859 - accuracy: 0.7635\n",
      "Epoch 463/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.5294 - accuracy: 0.7449\n",
      "Epoch 464/500\n",
      "54/54 [==============================] - 0s 639us/step - loss: 0.4866 - accuracy: 0.7691\n",
      "Epoch 465/500\n",
      "54/54 [==============================] - 0s 611us/step - loss: 0.4773 - accuracy: 0.7579\n",
      "Epoch 466/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.4792 - accuracy: 0.7784\n",
      "Epoch 467/500\n",
      "54/54 [==============================] - 0s 634us/step - loss: 0.4759 - accuracy: 0.7654\n",
      "Epoch 468/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.4752 - accuracy: 0.7765\n",
      "Epoch 469/500\n",
      "54/54 [==============================] - 0s 625us/step - loss: 0.4885 - accuracy: 0.7561\n",
      "Epoch 470/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 642us/step - loss: 0.4828 - accuracy: 0.7765\n",
      "Epoch 471/500\n",
      "54/54 [==============================] - 0s 660us/step - loss: 0.4757 - accuracy: 0.7654\n",
      "Epoch 472/500\n",
      "54/54 [==============================] - 0s 503us/step - loss: 0.4981 - accuracy: 0.7728\n",
      "Epoch 473/500\n",
      "54/54 [==============================] - 0s 548us/step - loss: 0.4778 - accuracy: 0.7635\n",
      "Epoch 474/500\n",
      "54/54 [==============================] - 0s 508us/step - loss: 0.4752 - accuracy: 0.7747\n",
      "Epoch 475/500\n",
      "54/54 [==============================] - 0s 598us/step - loss: 0.4701 - accuracy: 0.7784\n",
      "Epoch 476/500\n",
      "54/54 [==============================] - 0s 619us/step - loss: 0.4809 - accuracy: 0.7784\n",
      "Epoch 477/500\n",
      "54/54 [==============================] - 0s 665us/step - loss: 0.4858 - accuracy: 0.7691\n",
      "Epoch 478/500\n",
      "54/54 [==============================] - 0s 663us/step - loss: 0.5029 - accuracy: 0.7449\n",
      "Epoch 479/500\n",
      "54/54 [==============================] - 0s 650us/step - loss: 0.4821 - accuracy: 0.7561\n",
      "Epoch 480/500\n",
      "54/54 [==============================] - 0s 670us/step - loss: 0.4924 - accuracy: 0.7579\n",
      "Epoch 481/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.4837 - accuracy: 0.7709\n",
      "Epoch 482/500\n",
      "54/54 [==============================] - 0s 649us/step - loss: 0.4849 - accuracy: 0.7691\n",
      "Epoch 483/500\n",
      "54/54 [==============================] - 0s 671us/step - loss: 0.4798 - accuracy: 0.7672\n",
      "Epoch 484/500\n",
      "54/54 [==============================] - 0s 669us/step - loss: 0.4720 - accuracy: 0.7747\n",
      "Epoch 485/500\n",
      "54/54 [==============================] - 0s 715us/step - loss: 0.4791 - accuracy: 0.7821\n",
      "Epoch 486/500\n",
      "54/54 [==============================] - 0s 683us/step - loss: 0.4777 - accuracy: 0.7672\n",
      "Epoch 487/500\n",
      "54/54 [==============================] - 0s 669us/step - loss: 0.4942 - accuracy: 0.7691\n",
      "Epoch 488/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.5155 - accuracy: 0.7318\n",
      "Epoch 489/500\n",
      "54/54 [==============================] - 0s 619us/step - loss: 0.4883 - accuracy: 0.7561\n",
      "Epoch 490/500\n",
      "54/54 [==============================] - 0s 636us/step - loss: 0.4800 - accuracy: 0.7709\n",
      "Epoch 491/500\n",
      "54/54 [==============================] - 0s 643us/step - loss: 0.4916 - accuracy: 0.7672\n",
      "Epoch 492/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 0.5004 - accuracy: 0.7858\n",
      "Epoch 493/500\n",
      "54/54 [==============================] - 0s 631us/step - loss: 0.4867 - accuracy: 0.7672\n",
      "Epoch 494/500\n",
      "54/54 [==============================] - 0s 641us/step - loss: 0.4780 - accuracy: 0.7635\n",
      "Epoch 495/500\n",
      "54/54 [==============================] - 0s 610us/step - loss: 0.4772 - accuracy: 0.7616\n",
      "Epoch 496/500\n",
      "54/54 [==============================] - 0s 619us/step - loss: 0.4884 - accuracy: 0.7523\n",
      "Epoch 497/500\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.4848 - accuracy: 0.7747\n",
      "Epoch 498/500\n",
      "54/54 [==============================] - 0s 647us/step - loss: 0.5002 - accuracy: 0.7635\n",
      "Epoch 499/500\n",
      "54/54 [==============================] - 0s 620us/step - loss: 0.4844 - accuracy: 0.7635\n",
      "Epoch 500/500\n",
      "54/54 [==============================] - 0s 659us/step - loss: 0.4741 - accuracy: 0.7691\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.5006 - accuracy: 0.7446\n",
      "\n",
      " Accuracy : 0.7446\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('C:\\\\Users\\\\user\\\\study1\\\\머신러닝\\\\data\\\\pima-indians-diabetes.csv',header = None)\n",
    "print(dataset.info())\n",
    "\n",
    "df = dataset.values\n",
    "X = df[:,0:8]\n",
    "Y = df[:,8]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = seed)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(6, input_dim = 8, activation = 'relu'))\n",
    "model.add(Dense(6, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train,epochs = 500, batch_size = 10)\n",
    "\n",
    "print('\\n Accuracy : %.4f'%(model.evaluate(X_test,Y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       768 non-null    int64  \n",
      " 1   1       768 non-null    int64  \n",
      " 2   2       768 non-null    int64  \n",
      " 3   3       768 non-null    int64  \n",
      " 4   4       768 non-null    int64  \n",
      " 5   5       768 non-null    float64\n",
      " 6   6       768 non-null    float64\n",
      " 7   7       768 non-null    int64  \n",
      " 8   8       768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "None\n",
      "Epoch 1/500\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1.6857 - accuracy: 0.5992 - val_loss: 0.8067 - val_accuracy: 0.5630\n",
      "Epoch 2/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.8860 - accuracy: 0.5934 - val_loss: 1.0348 - val_accuracy: 0.5512\n",
      "Epoch 3/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7886 - accuracy: 0.6245 - val_loss: 0.8299 - val_accuracy: 0.6654\n",
      "Epoch 4/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6995 - accuracy: 0.6634 - val_loss: 0.7614 - val_accuracy: 0.5945\n",
      "Epoch 5/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6895 - accuracy: 0.6362 - val_loss: 0.7412 - val_accuracy: 0.5984\n",
      "Epoch 6/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7406 - accuracy: 0.6498 - val_loss: 0.6883 - val_accuracy: 0.6575\n",
      "Epoch 7/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7567 - accuracy: 0.6420 - val_loss: 0.7253 - val_accuracy: 0.6260\n",
      "Epoch 8/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7546 - accuracy: 0.6401 - val_loss: 0.6728 - val_accuracy: 0.6614\n",
      "Epoch 9/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6753 - accuracy: 0.6459 - val_loss: 0.7199 - val_accuracy: 0.6732\n",
      "Epoch 10/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6672 - accuracy: 0.6712 - val_loss: 0.7970 - val_accuracy: 0.5591\n",
      "Epoch 11/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6608 - accuracy: 0.6673 - val_loss: 0.6503 - val_accuracy: 0.6417\n",
      "Epoch 12/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5968 - accuracy: 0.6965 - val_loss: 0.6298 - val_accuracy: 0.6890\n",
      "Epoch 13/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6112 - accuracy: 0.7043 - val_loss: 0.6412 - val_accuracy: 0.6772\n",
      "Epoch 14/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6217 - accuracy: 0.6868 - val_loss: 0.6286 - val_accuracy: 0.6929\n",
      "Epoch 15/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6726 - accuracy: 0.6556 - val_loss: 0.6331 - val_accuracy: 0.6732\n",
      "Epoch 16/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6538 - accuracy: 0.6751 - val_loss: 0.6370 - val_accuracy: 0.6260\n",
      "Epoch 17/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6370 - accuracy: 0.6790 - val_loss: 0.8149 - val_accuracy: 0.6811\n",
      "Epoch 18/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6288 - accuracy: 0.6751 - val_loss: 0.6456 - val_accuracy: 0.6890\n",
      "Epoch 19/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6179 - accuracy: 0.6926 - val_loss: 0.6121 - val_accuracy: 0.6929\n",
      "Epoch 20/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5831 - accuracy: 0.7237 - val_loss: 0.6411 - val_accuracy: 0.6811\n",
      "Epoch 21/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6015 - accuracy: 0.6868 - val_loss: 0.6440 - val_accuracy: 0.6929\n",
      "Epoch 22/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6886 - accuracy: 0.6693 - val_loss: 0.5972 - val_accuracy: 0.7047\n",
      "Epoch 23/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7527 - accuracy: 0.6518 - val_loss: 0.7665 - val_accuracy: 0.6772\n",
      "Epoch 24/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.8225 - accuracy: 0.6498 - val_loss: 0.6287 - val_accuracy: 0.6890\n",
      "Epoch 25/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6032 - accuracy: 0.7062 - val_loss: 0.6199 - val_accuracy: 0.6772\n",
      "Epoch 26/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5609 - accuracy: 0.7276 - val_loss: 0.6171 - val_accuracy: 0.6890\n",
      "Epoch 27/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5737 - accuracy: 0.7335 - val_loss: 0.6506 - val_accuracy: 0.6929\n",
      "Epoch 28/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5994 - accuracy: 0.6965 - val_loss: 0.6626 - val_accuracy: 0.6772\n",
      "Epoch 29/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5734 - accuracy: 0.7296 - val_loss: 0.5990 - val_accuracy: 0.7008\n",
      "Epoch 30/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5733 - accuracy: 0.7004 - val_loss: 0.6940 - val_accuracy: 0.6260\n",
      "Epoch 31/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6143 - accuracy: 0.7296 - val_loss: 0.6557 - val_accuracy: 0.6378\n",
      "Epoch 32/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6203 - accuracy: 0.6829 - val_loss: 0.6884 - val_accuracy: 0.6850\n",
      "Epoch 33/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5793 - accuracy: 0.7179 - val_loss: 0.6089 - val_accuracy: 0.6772\n",
      "Epoch 34/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5784 - accuracy: 0.7237 - val_loss: 0.6511 - val_accuracy: 0.6693\n",
      "Epoch 35/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5869 - accuracy: 0.7043 - val_loss: 0.7081 - val_accuracy: 0.6220\n",
      "Epoch 36/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6429 - accuracy: 0.7004 - val_loss: 0.6209 - val_accuracy: 0.6929\n",
      "Epoch 37/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5712 - accuracy: 0.7218 - val_loss: 0.6168 - val_accuracy: 0.6890\n",
      "Epoch 38/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5636 - accuracy: 0.7140 - val_loss: 0.6185 - val_accuracy: 0.6850\n",
      "Epoch 39/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5838 - accuracy: 0.6907 - val_loss: 0.6048 - val_accuracy: 0.6969\n",
      "Epoch 40/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7198 - val_loss: 0.6988 - val_accuracy: 0.5984\n",
      "Epoch 41/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5743 - accuracy: 0.7218 - val_loss: 0.6072 - val_accuracy: 0.6850\n",
      "Epoch 42/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6122 - accuracy: 0.6946 - val_loss: 0.6070 - val_accuracy: 0.7008\n",
      "Epoch 43/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5613 - accuracy: 0.7335 - val_loss: 0.5946 - val_accuracy: 0.7244\n",
      "Epoch 44/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5792 - accuracy: 0.6926 - val_loss: 0.6549 - val_accuracy: 0.6220\n",
      "Epoch 45/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5384 - accuracy: 0.7529 - val_loss: 0.6302 - val_accuracy: 0.7126\n",
      "Epoch 46/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5898 - accuracy: 0.7140 - val_loss: 0.6075 - val_accuracy: 0.6929\n",
      "Epoch 47/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5798 - accuracy: 0.6907 - val_loss: 0.6819 - val_accuracy: 0.6929\n",
      "Epoch 48/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5546 - accuracy: 0.7257 - val_loss: 0.7694 - val_accuracy: 0.6850\n",
      "Epoch 49/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.7218 - val_loss: 0.6434 - val_accuracy: 0.6614\n",
      "Epoch 50/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5509 - accuracy: 0.7354 - val_loss: 0.5945 - val_accuracy: 0.7126\n",
      "Epoch 51/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5793 - accuracy: 0.7082 - val_loss: 0.6084 - val_accuracy: 0.7047\n",
      "Epoch 52/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5394 - accuracy: 0.7276 - val_loss: 0.5963 - val_accuracy: 0.7047\n",
      "Epoch 53/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5622 - accuracy: 0.7140 - val_loss: 0.6091 - val_accuracy: 0.6890\n",
      "Epoch 54/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.7374 - val_loss: 0.7193 - val_accuracy: 0.5827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5769 - accuracy: 0.6965 - val_loss: 0.6275 - val_accuracy: 0.6457\n",
      "Epoch 56/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5237 - accuracy: 0.7490 - val_loss: 0.6198 - val_accuracy: 0.6850\n",
      "Epoch 57/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7296 - val_loss: 0.6214 - val_accuracy: 0.6890\n",
      "Epoch 58/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5158 - accuracy: 0.7549 - val_loss: 0.6069 - val_accuracy: 0.7047\n",
      "Epoch 59/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5458 - accuracy: 0.7510 - val_loss: 0.9668 - val_accuracy: 0.4882\n",
      "Epoch 60/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5667 - accuracy: 0.7315 - val_loss: 0.6725 - val_accuracy: 0.6220\n",
      "Epoch 61/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7121 - val_loss: 0.7508 - val_accuracy: 0.5709\n",
      "Epoch 62/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5450 - accuracy: 0.7412 - val_loss: 0.6167 - val_accuracy: 0.6929\n",
      "Epoch 63/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5269 - accuracy: 0.7393 - val_loss: 0.6747 - val_accuracy: 0.6654\n",
      "Epoch 64/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5682 - accuracy: 0.7237 - val_loss: 0.9166 - val_accuracy: 0.5512\n",
      "Epoch 65/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5334 - accuracy: 0.7510 - val_loss: 0.5723 - val_accuracy: 0.7402\n",
      "Epoch 66/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6402 - accuracy: 0.7082 - val_loss: 0.7439 - val_accuracy: 0.6929\n",
      "Epoch 67/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7315 - val_loss: 0.7096 - val_accuracy: 0.5984\n",
      "Epoch 68/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5041 - accuracy: 0.7588 - val_loss: 0.6079 - val_accuracy: 0.6850\n",
      "Epoch 69/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7237 - val_loss: 0.6212 - val_accuracy: 0.6811\n",
      "Epoch 70/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5110 - accuracy: 0.7529 - val_loss: 0.6073 - val_accuracy: 0.6890\n",
      "Epoch 71/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5096 - accuracy: 0.7412 - val_loss: 0.6128 - val_accuracy: 0.7008\n",
      "Epoch 72/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5110 - accuracy: 0.7529 - val_loss: 0.5923 - val_accuracy: 0.7283\n",
      "Epoch 73/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5033 - accuracy: 0.7665 - val_loss: 0.7199 - val_accuracy: 0.5709\n",
      "Epoch 74/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5086 - accuracy: 0.7529 - val_loss: 0.5820 - val_accuracy: 0.7126\n",
      "Epoch 75/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4957 - accuracy: 0.7704 - val_loss: 0.5844 - val_accuracy: 0.7244\n",
      "Epoch 76/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4945 - accuracy: 0.7588 - val_loss: 0.7052 - val_accuracy: 0.6142\n",
      "Epoch 77/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5831 - accuracy: 0.7082 - val_loss: 0.5961 - val_accuracy: 0.7402\n",
      "Epoch 78/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5170 - accuracy: 0.7432 - val_loss: 0.6489 - val_accuracy: 0.6850\n",
      "Epoch 79/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5021 - accuracy: 0.7510 - val_loss: 0.6296 - val_accuracy: 0.7008\n",
      "Epoch 80/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5108 - accuracy: 0.7412 - val_loss: 0.5944 - val_accuracy: 0.7126\n",
      "Epoch 81/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5099 - accuracy: 0.7374 - val_loss: 0.5875 - val_accuracy: 0.7323\n",
      "Epoch 82/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5781 - accuracy: 0.7062 - val_loss: 0.6471 - val_accuracy: 0.6772\n",
      "Epoch 83/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5581 - accuracy: 0.7471 - val_loss: 0.6197 - val_accuracy: 0.6969\n",
      "Epoch 84/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7490 - val_loss: 0.5999 - val_accuracy: 0.6969\n",
      "Epoch 85/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5006 - accuracy: 0.7607 - val_loss: 0.7118 - val_accuracy: 0.6339\n",
      "Epoch 86/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5257 - accuracy: 0.7471 - val_loss: 0.5961 - val_accuracy: 0.7126\n",
      "Epoch 87/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5526 - accuracy: 0.7296 - val_loss: 0.7138 - val_accuracy: 0.6220\n",
      "Epoch 88/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5187 - accuracy: 0.7490 - val_loss: 0.6138 - val_accuracy: 0.7047\n",
      "Epoch 89/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5274 - accuracy: 0.7471 - val_loss: 0.5903 - val_accuracy: 0.7244\n",
      "Epoch 90/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5552 - accuracy: 0.7296 - val_loss: 0.5998 - val_accuracy: 0.7205\n",
      "Epoch 91/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5106 - accuracy: 0.7432 - val_loss: 0.6166 - val_accuracy: 0.6890\n",
      "Epoch 92/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4844 - accuracy: 0.7568 - val_loss: 0.5982 - val_accuracy: 0.7244\n",
      "Epoch 93/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4836 - accuracy: 0.7724 - val_loss: 0.5975 - val_accuracy: 0.7126\n",
      "Epoch 94/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4903 - accuracy: 0.7607 - val_loss: 0.6244 - val_accuracy: 0.7283\n",
      "Epoch 95/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5397 - accuracy: 0.7451 - val_loss: 0.5861 - val_accuracy: 0.7283\n",
      "Epoch 96/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4919 - accuracy: 0.7510 - val_loss: 0.5815 - val_accuracy: 0.7362\n",
      "Epoch 97/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4954 - accuracy: 0.7471 - val_loss: 0.5879 - val_accuracy: 0.7441\n",
      "Epoch 98/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4952 - accuracy: 0.7529 - val_loss: 0.5947 - val_accuracy: 0.7283\n",
      "Epoch 99/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4827 - accuracy: 0.7685 - val_loss: 0.6334 - val_accuracy: 0.6772\n",
      "Epoch 100/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4870 - accuracy: 0.7685 - val_loss: 0.6064 - val_accuracy: 0.6969\n",
      "Epoch 101/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5291 - accuracy: 0.7529 - val_loss: 0.6054 - val_accuracy: 0.7087\n",
      "Epoch 102/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5114 - accuracy: 0.7529 - val_loss: 0.5894 - val_accuracy: 0.6890\n",
      "Epoch 103/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4982 - accuracy: 0.7704 - val_loss: 0.6179 - val_accuracy: 0.6929\n",
      "Epoch 104/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4884 - accuracy: 0.7393 - val_loss: 0.6516 - val_accuracy: 0.6693\n",
      "Epoch 105/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5122 - accuracy: 0.7549 - val_loss: 0.5900 - val_accuracy: 0.7323\n",
      "Epoch 106/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.7549 - val_loss: 0.6973 - val_accuracy: 0.6496\n",
      "Epoch 107/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4986 - accuracy: 0.7451 - val_loss: 0.5943 - val_accuracy: 0.7323\n",
      "Epoch 108/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.7471 - val_loss: 0.5845 - val_accuracy: 0.7283\n",
      "Epoch 109/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.7568 - val_loss: 0.6310 - val_accuracy: 0.6929\n",
      "Epoch 110/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4939 - accuracy: 0.7568 - val_loss: 0.5942 - val_accuracy: 0.7441\n",
      "Epoch 111/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4843 - accuracy: 0.7549 - val_loss: 0.6132 - val_accuracy: 0.7323\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5354 - accuracy: 0.7237 - val_loss: 0.5835 - val_accuracy: 0.7087\n",
      "Epoch 113/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4927 - accuracy: 0.7626 - val_loss: 0.5969 - val_accuracy: 0.7087\n",
      "Epoch 114/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4895 - accuracy: 0.7685 - val_loss: 0.6121 - val_accuracy: 0.7126\n",
      "Epoch 115/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4914 - accuracy: 0.7588 - val_loss: 0.5957 - val_accuracy: 0.7008\n",
      "Epoch 116/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4828 - accuracy: 0.7451 - val_loss: 0.5883 - val_accuracy: 0.6969\n",
      "Epoch 117/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4833 - accuracy: 0.7626 - val_loss: 0.5861 - val_accuracy: 0.7323\n",
      "Epoch 118/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4832 - accuracy: 0.7568 - val_loss: 0.5990 - val_accuracy: 0.7402\n",
      "Epoch 119/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5067 - accuracy: 0.7510 - val_loss: 0.6285 - val_accuracy: 0.7205\n",
      "Epoch 120/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4731 - accuracy: 0.7665 - val_loss: 0.5888 - val_accuracy: 0.7362\n",
      "Epoch 121/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4825 - accuracy: 0.7646 - val_loss: 0.5756 - val_accuracy: 0.7402\n",
      "Epoch 122/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.7802 - val_loss: 0.5908 - val_accuracy: 0.7244\n",
      "Epoch 123/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5057 - accuracy: 0.7704 - val_loss: 0.5896 - val_accuracy: 0.6929\n",
      "Epoch 124/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4882 - accuracy: 0.7588 - val_loss: 0.5976 - val_accuracy: 0.7283\n",
      "Epoch 125/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.7802 - val_loss: 0.5918 - val_accuracy: 0.7205\n",
      "Epoch 126/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4678 - accuracy: 0.7685 - val_loss: 0.6211 - val_accuracy: 0.6969\n",
      "Epoch 127/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.7802 - val_loss: 0.6176 - val_accuracy: 0.7087\n",
      "Epoch 128/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5020 - accuracy: 0.7588 - val_loss: 0.6365 - val_accuracy: 0.7244\n",
      "Epoch 129/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4866 - accuracy: 0.7665 - val_loss: 0.5969 - val_accuracy: 0.7323\n",
      "Epoch 130/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.7704 - val_loss: 0.6105 - val_accuracy: 0.7520\n",
      "Epoch 131/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4993 - accuracy: 0.7432 - val_loss: 0.7001 - val_accuracy: 0.6535\n",
      "Epoch 132/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4899 - accuracy: 0.7510 - val_loss: 0.6254 - val_accuracy: 0.7283\n",
      "Epoch 133/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5066 - accuracy: 0.7685 - val_loss: 0.7141 - val_accuracy: 0.6299\n",
      "Epoch 134/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4851 - accuracy: 0.7840 - val_loss: 0.5965 - val_accuracy: 0.7362\n",
      "Epoch 135/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4549 - accuracy: 0.7879 - val_loss: 0.6048 - val_accuracy: 0.7283\n",
      "Epoch 136/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4557 - accuracy: 0.7704 - val_loss: 0.6757 - val_accuracy: 0.6890\n",
      "Epoch 137/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4692 - accuracy: 0.7821 - val_loss: 0.6772 - val_accuracy: 0.6811\n",
      "Epoch 138/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5313 - accuracy: 0.7432 - val_loss: 0.6066 - val_accuracy: 0.7205\n",
      "Epoch 139/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4996 - accuracy: 0.7335 - val_loss: 0.5795 - val_accuracy: 0.7126\n",
      "Epoch 140/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7724 - val_loss: 0.5862 - val_accuracy: 0.7323\n",
      "Epoch 141/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7821 - val_loss: 0.5999 - val_accuracy: 0.7126\n",
      "Epoch 142/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4845 - accuracy: 0.7529 - val_loss: 0.6189 - val_accuracy: 0.7205\n",
      "Epoch 143/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4483 - accuracy: 0.7840 - val_loss: 0.5958 - val_accuracy: 0.7480\n",
      "Epoch 144/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.7704 - val_loss: 0.5753 - val_accuracy: 0.7283\n",
      "Epoch 145/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4464 - accuracy: 0.7802 - val_loss: 0.6102 - val_accuracy: 0.7165\n",
      "Epoch 146/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.7918 - val_loss: 0.6213 - val_accuracy: 0.7087\n",
      "Epoch 147/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4807 - accuracy: 0.7568 - val_loss: 0.5844 - val_accuracy: 0.7441\n",
      "Epoch 148/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4514 - accuracy: 0.7646 - val_loss: 0.6126 - val_accuracy: 0.7323\n",
      "Epoch 149/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4527 - accuracy: 0.7840 - val_loss: 0.6165 - val_accuracy: 0.7165\n",
      "Epoch 150/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4898 - accuracy: 0.7782 - val_loss: 0.7584 - val_accuracy: 0.6654\n",
      "Epoch 151/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4973 - accuracy: 0.7549 - val_loss: 0.6272 - val_accuracy: 0.7008\n",
      "Epoch 152/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4511 - accuracy: 0.7821 - val_loss: 0.5981 - val_accuracy: 0.7323\n",
      "Epoch 153/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4362 - accuracy: 0.7840 - val_loss: 0.6140 - val_accuracy: 0.7205\n",
      "Epoch 154/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4639 - accuracy: 0.7646 - val_loss: 0.6590 - val_accuracy: 0.6772\n",
      "Epoch 155/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4711 - accuracy: 0.7646 - val_loss: 0.6070 - val_accuracy: 0.7205\n",
      "Epoch 156/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.7802 - val_loss: 0.6117 - val_accuracy: 0.7244\n",
      "Epoch 157/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4531 - accuracy: 0.7821 - val_loss: 0.5991 - val_accuracy: 0.7480\n",
      "Epoch 158/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4899 - accuracy: 0.7490 - val_loss: 0.6030 - val_accuracy: 0.7559\n",
      "Epoch 159/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4525 - accuracy: 0.7763 - val_loss: 0.5984 - val_accuracy: 0.7323\n",
      "Epoch 160/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4474 - accuracy: 0.7840 - val_loss: 0.6075 - val_accuracy: 0.7087\n",
      "Epoch 161/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5032 - accuracy: 0.7432 - val_loss: 0.5725 - val_accuracy: 0.7165\n",
      "Epoch 162/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.7840 - val_loss: 0.5954 - val_accuracy: 0.7480\n",
      "Epoch 163/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.7957 - val_loss: 0.6094 - val_accuracy: 0.7323\n",
      "Epoch 164/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4503 - accuracy: 0.7802 - val_loss: 0.6110 - val_accuracy: 0.7205\n",
      "Epoch 165/500\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4440 - accuracy: 0.7724 - val_loss: 0.5850 - val_accuracy: 0.7598\n",
      "24/24 [==============================] - 0s 707us/step - loss: 0.4723 - accuracy: 0.7930\n",
      "\n",
      " Accuracy : 0.7930\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('C:\\\\Users\\\\user\\\\study1\\\\머신러닝\\\\data\\\\pima-indians-diabetes.csv',header = None)\n",
    "print(dataset.info())\n",
    "\n",
    "df = dataset.values\n",
    "X = df[:,0:8]\n",
    "Y = df[:,8]\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = 8, activation = 'relu'))\n",
    "model.add(Dense(30, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 100)\n",
    "\n",
    "model.fit(X, Y ,validation_split = 0.33, epochs = 500, batch_size = 10,\n",
    "         callbacks = early_stopping_callback)\n",
    "\n",
    "\n",
    "######### evaluate(X,Y)는 학습셋을 집어넣은것 -> 잘못됨 !!!!!!!!!!!!\n",
    "print('\\n Accuracy : %.4f'%(model.evaluate(X,Y)[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)  train_test_split & EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       768 non-null    int64  \n",
      " 1   1       768 non-null    int64  \n",
      " 2   2       768 non-null    int64  \n",
      " 3   3       768 non-null    int64  \n",
      " 4   4       768 non-null    int64  \n",
      " 5   5       768 non-null    float64\n",
      " 6   6       768 non-null    float64\n",
      " 7   7       768 non-null    int64  \n",
      " 8   8       768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "None\n",
      "Epoch 1/500\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 1.8165 - accuracy: 0.5717 - val_loss: 0.6926 - val_accuracy: 0.6623\n",
      "Epoch 2/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.8762 - accuracy: 0.6257 - val_loss: 0.7093 - val_accuracy: 0.6970\n",
      "Epoch 3/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.7121 - accuracy: 0.6518 - val_loss: 0.8256 - val_accuracy: 0.6797\n",
      "Epoch 4/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.8342 - accuracy: 0.6071 - val_loss: 0.6837 - val_accuracy: 0.6494\n",
      "Epoch 5/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.7225 - accuracy: 0.6592 - val_loss: 0.6706 - val_accuracy: 0.6320\n",
      "Epoch 6/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.7150 - accuracy: 0.6648 - val_loss: 0.8077 - val_accuracy: 0.6710\n",
      "Epoch 7/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.7049 - accuracy: 0.6518 - val_loss: 0.7895 - val_accuracy: 0.5931\n",
      "Epoch 8/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6757 - accuracy: 0.6667 - val_loss: 0.7458 - val_accuracy: 0.6537\n",
      "Epoch 9/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.7288 - accuracy: 0.6443 - val_loss: 0.6189 - val_accuracy: 0.6753\n",
      "Epoch 10/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6124 - accuracy: 0.6797 - val_loss: 0.6240 - val_accuracy: 0.6623\n",
      "Epoch 11/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6031 - accuracy: 0.7039 - val_loss: 0.6365 - val_accuracy: 0.7013\n",
      "Epoch 12/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6674 - accuracy: 0.6667 - val_loss: 0.6960 - val_accuracy: 0.5931\n",
      "Epoch 13/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5852 - accuracy: 0.7076 - val_loss: 0.6662 - val_accuracy: 0.6580\n",
      "Epoch 14/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6031 - accuracy: 0.6685 - val_loss: 0.7107 - val_accuracy: 0.6017\n",
      "Epoch 15/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6557 - accuracy: 0.6834 - val_loss: 0.7581 - val_accuracy: 0.6494\n",
      "Epoch 16/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6097 - accuracy: 0.6629 - val_loss: 0.6424 - val_accuracy: 0.7186\n",
      "Epoch 17/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6532 - accuracy: 0.6778 - val_loss: 0.6362 - val_accuracy: 0.6797\n",
      "Epoch 18/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6154 - accuracy: 0.7002 - val_loss: 0.9082 - val_accuracy: 0.6797\n",
      "Epoch 19/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.7963 - accuracy: 0.6629 - val_loss: 0.6524 - val_accuracy: 0.7143\n",
      "Epoch 20/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6521 - accuracy: 0.6723 - val_loss: 0.6547 - val_accuracy: 0.6883\n",
      "Epoch 21/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6053 - accuracy: 0.6946 - val_loss: 0.6548 - val_accuracy: 0.6883\n",
      "Epoch 22/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5810 - accuracy: 0.6909 - val_loss: 0.7007 - val_accuracy: 0.7013\n",
      "Epoch 23/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5687 - accuracy: 0.7039 - val_loss: 0.7575 - val_accuracy: 0.5974\n",
      "Epoch 24/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5966 - accuracy: 0.6890 - val_loss: 0.6814 - val_accuracy: 0.6753\n",
      "Epoch 25/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5952 - accuracy: 0.6909 - val_loss: 0.6954 - val_accuracy: 0.6061\n",
      "Epoch 26/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6241 - accuracy: 0.6909 - val_loss: 0.6120 - val_accuracy: 0.7273\n",
      "Epoch 27/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6729 - accuracy: 0.6741 - val_loss: 0.6350 - val_accuracy: 0.6623\n",
      "Epoch 28/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6161 - accuracy: 0.7076 - val_loss: 0.6229 - val_accuracy: 0.6970\n",
      "Epoch 29/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5546 - accuracy: 0.7132 - val_loss: 0.6440 - val_accuracy: 0.6970\n",
      "Epoch 30/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6243 - accuracy: 0.7039 - val_loss: 0.7867 - val_accuracy: 0.5541\n",
      "Epoch 31/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5713 - accuracy: 0.7002 - val_loss: 0.6390 - val_accuracy: 0.6710\n",
      "Epoch 32/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6437 - accuracy: 0.6853 - val_loss: 0.7704 - val_accuracy: 0.5714\n",
      "Epoch 33/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5981 - accuracy: 0.7151 - val_loss: 0.7017 - val_accuracy: 0.6147\n",
      "Epoch 34/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6254 - accuracy: 0.7095 - val_loss: 0.6451 - val_accuracy: 0.6667\n",
      "Epoch 35/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5975 - accuracy: 0.7039 - val_loss: 0.6265 - val_accuracy: 0.6970\n",
      "Epoch 36/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5761 - accuracy: 0.6890 - val_loss: 0.6863 - val_accuracy: 0.6061\n",
      "Epoch 37/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5570 - accuracy: 0.7114 - val_loss: 0.6167 - val_accuracy: 0.6970\n",
      "Epoch 38/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5624 - accuracy: 0.7132 - val_loss: 0.6213 - val_accuracy: 0.6926\n",
      "Epoch 39/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.7132 - val_loss: 0.6445 - val_accuracy: 0.6710\n",
      "Epoch 40/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5456 - accuracy: 0.7374 - val_loss: 0.7710 - val_accuracy: 0.6017\n",
      "Epoch 41/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5527 - accuracy: 0.7225 - val_loss: 0.6287 - val_accuracy: 0.6797\n",
      "Epoch 42/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5569 - accuracy: 0.7244 - val_loss: 0.7164 - val_accuracy: 0.6710\n",
      "Epoch 43/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.7263 - val_loss: 0.6353 - val_accuracy: 0.6753\n",
      "Epoch 44/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5359 - accuracy: 0.7374 - val_loss: 0.6180 - val_accuracy: 0.7013\n",
      "Epoch 45/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5276 - accuracy: 0.7374 - val_loss: 0.6400 - val_accuracy: 0.6710\n",
      "Epoch 46/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5483 - accuracy: 0.7207 - val_loss: 0.6866 - val_accuracy: 0.6320\n",
      "Epoch 47/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5537 - accuracy: 0.7207 - val_loss: 0.6692 - val_accuracy: 0.6407\n",
      "Epoch 48/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5305 - accuracy: 0.7467 - val_loss: 0.6202 - val_accuracy: 0.6926\n",
      "Epoch 49/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5257 - accuracy: 0.7430 - val_loss: 0.6208 - val_accuracy: 0.6623\n",
      "Epoch 50/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5131 - accuracy: 0.7542 - val_loss: 0.6691 - val_accuracy: 0.6450\n",
      "Epoch 51/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5327 - accuracy: 0.7430 - val_loss: 0.6349 - val_accuracy: 0.6667\n",
      "Epoch 52/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5533 - accuracy: 0.7561 - val_loss: 0.6854 - val_accuracy: 0.6364\n",
      "Epoch 53/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5176 - accuracy: 0.7505 - val_loss: 0.6287 - val_accuracy: 0.6883\n",
      "Epoch 54/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.7318 - val_loss: 0.6506 - val_accuracy: 0.6797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5516 - accuracy: 0.7412 - val_loss: 0.6422 - val_accuracy: 0.6926\n",
      "Epoch 56/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5216 - accuracy: 0.7523 - val_loss: 0.6236 - val_accuracy: 0.6926\n",
      "Epoch 57/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5111 - accuracy: 0.7467 - val_loss: 0.6332 - val_accuracy: 0.7013\n",
      "Epoch 58/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5106 - accuracy: 0.7374 - val_loss: 0.7175 - val_accuracy: 0.6234\n",
      "Epoch 59/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5015 - accuracy: 0.7598 - val_loss: 0.6441 - val_accuracy: 0.6840\n",
      "Epoch 60/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4999 - accuracy: 0.7579 - val_loss: 0.6150 - val_accuracy: 0.6753\n",
      "Epoch 61/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.7430 - val_loss: 0.6738 - val_accuracy: 0.6623\n",
      "Epoch 62/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.7281 - val_loss: 0.7117 - val_accuracy: 0.6537\n",
      "Epoch 63/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.6163 - accuracy: 0.7076 - val_loss: 0.6562 - val_accuracy: 0.6970\n",
      "Epoch 64/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5681 - accuracy: 0.7114 - val_loss: 0.6599 - val_accuracy: 0.6710\n",
      "Epoch 65/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5153 - accuracy: 0.7486 - val_loss: 0.7106 - val_accuracy: 0.6320\n",
      "Epoch 66/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5491 - accuracy: 0.7393 - val_loss: 0.9085 - val_accuracy: 0.5671\n",
      "Epoch 67/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5774 - accuracy: 0.7039 - val_loss: 0.6590 - val_accuracy: 0.6883\n",
      "Epoch 68/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5264 - accuracy: 0.7616 - val_loss: 0.6614 - val_accuracy: 0.6623\n",
      "Epoch 69/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5212 - accuracy: 0.7542 - val_loss: 0.6603 - val_accuracy: 0.6494\n",
      "Epoch 70/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5320 - accuracy: 0.7356 - val_loss: 0.6370 - val_accuracy: 0.6710\n",
      "Epoch 71/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4967 - accuracy: 0.7505 - val_loss: 0.6328 - val_accuracy: 0.6753\n",
      "Epoch 72/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5128 - accuracy: 0.7467 - val_loss: 0.6304 - val_accuracy: 0.6797\n",
      "Epoch 73/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4993 - accuracy: 0.7356 - val_loss: 0.6906 - val_accuracy: 0.6364\n",
      "Epoch 74/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5175 - accuracy: 0.7616 - val_loss: 0.7036 - val_accuracy: 0.6277\n",
      "Epoch 75/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5542 - accuracy: 0.7114 - val_loss: 0.8683 - val_accuracy: 0.5584\n",
      "Epoch 76/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4945 - accuracy: 0.7412 - val_loss: 0.6549 - val_accuracy: 0.7143\n",
      "Epoch 77/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5100 - accuracy: 0.7505 - val_loss: 0.6512 - val_accuracy: 0.6926\n",
      "Epoch 78/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4769 - accuracy: 0.7747 - val_loss: 0.6473 - val_accuracy: 0.6623\n",
      "Epoch 79/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7169 - val_loss: 0.6495 - val_accuracy: 0.6667\n",
      "Epoch 80/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4929 - accuracy: 0.7467 - val_loss: 0.6140 - val_accuracy: 0.6840\n",
      "Epoch 81/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5025 - accuracy: 0.7747 - val_loss: 0.6184 - val_accuracy: 0.7100\n",
      "Epoch 82/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4747 - accuracy: 0.7728 - val_loss: 0.6330 - val_accuracy: 0.6840\n",
      "Epoch 83/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4850 - accuracy: 0.7598 - val_loss: 0.6263 - val_accuracy: 0.6926\n",
      "Epoch 84/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4692 - accuracy: 0.7803 - val_loss: 0.6571 - val_accuracy: 0.6537\n",
      "Epoch 85/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5143 - accuracy: 0.7542 - val_loss: 0.6700 - val_accuracy: 0.6537\n",
      "Epoch 86/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4930 - accuracy: 0.7598 - val_loss: 0.6338 - val_accuracy: 0.7100\n",
      "Epoch 87/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4968 - accuracy: 0.7505 - val_loss: 0.6227 - val_accuracy: 0.6797\n",
      "Epoch 88/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4629 - accuracy: 0.7914 - val_loss: 0.6143 - val_accuracy: 0.6883\n",
      "Epoch 89/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4711 - accuracy: 0.7765 - val_loss: 0.7895 - val_accuracy: 0.6537\n",
      "Epoch 90/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.7449 - val_loss: 0.7607 - val_accuracy: 0.7143\n",
      "Epoch 91/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5372 - accuracy: 0.7467 - val_loss: 0.6386 - val_accuracy: 0.6667\n",
      "Epoch 92/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4679 - accuracy: 0.7952 - val_loss: 0.6833 - val_accuracy: 0.6450\n",
      "Epoch 93/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4884 - accuracy: 0.7616 - val_loss: 0.6425 - val_accuracy: 0.6753\n",
      "Epoch 94/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4782 - accuracy: 0.7765 - val_loss: 0.6628 - val_accuracy: 0.6667\n",
      "Epoch 95/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4935 - accuracy: 0.7486 - val_loss: 0.6338 - val_accuracy: 0.7013\n",
      "Epoch 96/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7691 - val_loss: 0.6836 - val_accuracy: 0.6623\n",
      "Epoch 97/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.7784 - val_loss: 0.6948 - val_accuracy: 0.6710\n",
      "Epoch 98/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4754 - accuracy: 0.7803 - val_loss: 0.6275 - val_accuracy: 0.6970\n",
      "Epoch 99/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7877 - val_loss: 0.6535 - val_accuracy: 0.6667\n",
      "Epoch 100/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4806 - accuracy: 0.7654 - val_loss: 0.6216 - val_accuracy: 0.7100\n",
      "Epoch 101/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4754 - accuracy: 0.7691 - val_loss: 0.7006 - val_accuracy: 0.6623\n",
      "Epoch 102/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4757 - accuracy: 0.7709 - val_loss: 0.6586 - val_accuracy: 0.6667\n",
      "Epoch 103/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4650 - accuracy: 0.7803 - val_loss: 0.6801 - val_accuracy: 0.6580\n",
      "Epoch 104/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4661 - accuracy: 0.7765 - val_loss: 0.6749 - val_accuracy: 0.6667\n",
      "Epoch 105/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5165 - accuracy: 0.7393 - val_loss: 0.6481 - val_accuracy: 0.6710\n",
      "Epoch 106/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.7840 - val_loss: 0.6688 - val_accuracy: 0.6840\n",
      "Epoch 107/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.7561 - val_loss: 0.6376 - val_accuracy: 0.6926\n",
      "Epoch 108/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4782 - accuracy: 0.7747 - val_loss: 0.7291 - val_accuracy: 0.6450\n",
      "Epoch 109/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4684 - accuracy: 0.7691 - val_loss: 0.6832 - val_accuracy: 0.7316\n",
      "Epoch 110/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4791 - accuracy: 0.7747 - val_loss: 0.7009 - val_accuracy: 0.6623\n",
      "Epoch 111/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5060 - accuracy: 0.7374 - val_loss: 0.6539 - val_accuracy: 0.6840\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4702 - accuracy: 0.7709 - val_loss: 0.6756 - val_accuracy: 0.6797\n",
      "Epoch 113/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4538 - accuracy: 0.7858 - val_loss: 0.6295 - val_accuracy: 0.6710\n",
      "Epoch 114/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4416 - accuracy: 0.7896 - val_loss: 0.6887 - val_accuracy: 0.6537\n",
      "Epoch 115/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4378 - accuracy: 0.7877 - val_loss: 0.6473 - val_accuracy: 0.6970\n",
      "Epoch 116/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4533 - accuracy: 0.7877 - val_loss: 0.7025 - val_accuracy: 0.6580\n",
      "Epoch 117/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4550 - accuracy: 0.7952 - val_loss: 0.7041 - val_accuracy: 0.6537\n",
      "Epoch 118/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.8026 - val_loss: 0.6650 - val_accuracy: 0.6753\n",
      "Epoch 119/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.7970 - val_loss: 0.6744 - val_accuracy: 0.6537\n",
      "Epoch 120/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4632 - accuracy: 0.7877 - val_loss: 0.6244 - val_accuracy: 0.6623\n",
      "Epoch 121/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4370 - accuracy: 0.7877 - val_loss: 0.6814 - val_accuracy: 0.6753\n",
      "Epoch 122/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7989 - val_loss: 0.6684 - val_accuracy: 0.6883\n",
      "Epoch 123/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8007 - val_loss: 0.7300 - val_accuracy: 0.6753\n",
      "Epoch 124/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.7765 - val_loss: 0.7340 - val_accuracy: 0.6580\n",
      "Epoch 125/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4357 - accuracy: 0.8007 - val_loss: 0.6662 - val_accuracy: 0.6710\n",
      "Epoch 126/500\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4287 - accuracy: 0.7989 - val_loss: 0.7607 - val_accuracy: 0.6364\n",
      "8/8 [==============================] - 0s 620us/step - loss: 0.7607 - accuracy: 0.6364\n",
      "\n",
      " Accuracy : 0.6364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('C:\\\\Users\\\\user\\\\study1\\\\머신러닝\\\\data\\\\pima-indians-diabetes.csv',header = None)\n",
    "print(dataset.info())\n",
    "\n",
    "df = dataset.values\n",
    "X = df[:,0:8]\n",
    "Y = df[:,8]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = seed)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = 8, activation = 'relu'))\n",
    "model.add(Dense(30, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 100)\n",
    "\n",
    "model.fit(X_train, Y_train ,validation_data = (X_test,Y_test), epochs = 500, batch_size = 10,\n",
    "         callbacks = early_stopping_callback)\n",
    "\n",
    "print('\\n Accuracy : %.4f'%(model.evaluate(X_test,Y_test)[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 인식, CNN 익히기\n",
    "\n",
    "### 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습셋 이미지 수 : 60000 개\n",
      "테스트셋 이미지 수 : 10000 개\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t3\t18\t18\t18\t126\t136\t175\t26\t166\t255\t247\t127\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t30\t36\t94\t154\t170\t253\t253\t253\t253\t253\t225\t172\t253\t242\t195\t64\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t49\t238\t253\t253\t253\t253\t253\t253\t253\t253\t251\t93\t82\t82\t56\t39\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t18\t219\t253\t253\t253\t253\t253\t198\t182\t247\t241\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t80\t156\t107\t253\t253\t205\t11\t0\t43\t154\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t14\t1\t154\t253\t90\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t139\t253\t190\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t11\t190\t253\t70\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t35\t241\t225\t160\t108\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t81\t240\t253\t253\t119\t25\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t45\t186\t253\t253\t150\t27\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t16\t93\t252\t253\t187\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t249\t253\t249\t64\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t46\t130\t183\t253\t253\t207\t2\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t39\t148\t229\t253\t253\t253\t250\t182\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t24\t114\t221\t253\t253\t253\t253\t201\t78\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t23\t66\t213\t253\t253\t253\t253\t198\t81\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t18\t171\t219\t253\t253\t253\t253\t195\t80\t9\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t55\t172\t226\t253\t253\t253\t253\t244\t133\t11\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t136\t253\t253\t253\t212\t135\t132\t16\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "class : 5\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import sys\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "    # 이미지를 X, 이미지에 0~9까지 붙인 이름표를 Y_class 로 구분함.\n",
    "    # 학습에 사용될 부분은 train으로 사용될 부분은 test로 불러옴\n",
    "(X_train, Y_class_train), (X_test, Y_class_test) = mnist.load_data()\n",
    "\n",
    "print('학습셋 이미지 수 : %d 개' % (X_train.shape[0]))\n",
    "print('테스트셋 이미지 수 : %d 개' %(X_test.shape[0]))\n",
    "\n",
    "\n",
    "# X_train[0] X_train의 0번째 이미지, cmap = 'Greys' 흑백으로 출력\n",
    "# 픽셀의 색을 표현할 때에는 각각의 RGB값으로 표현한다.\n",
    "# 흑백데이터는 픽셀의 각각에 등급이 매겨짐\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0], cmap = 'Greys')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 이미지는 숫자의 집합으로 바뀌어 학습셋으로 사용됨\n",
    "for x in X_train[0]:\n",
    "    for i in x:\n",
    "        sys.stdout.write('%d\\t'%i)\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "    \n",
    "# 주어진 가로 28,. 세로 28의 2차원 배열을 784개의 1차원 배열로 바꿔줌\n",
    "# 총 샘플수는 앞서 사용한 X_train.shpae[0]을 이용하고, 1차원 속성의 수는 784개\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "\n",
    "\n",
    "# 0~ 255사이의 값으로 이루어진 값을 0~1사이의 값으로 바꿔야한다. ----> keras 데이터에서 최적의 성능을 보이기 위해서\n",
    "# 각 값은 실수형으로 바꿔주고, 255로 나눈다.\n",
    "# 데이터의 폭이 클 때 적절한 값으로 분산의 정도를 바꾸는 과정을 데이터 정규화라고 한다.\n",
    "X_train = X_train.astype('float64')\n",
    "X_train = X_train / 255\n",
    "\n",
    "# X_test에도 동일한 작업을 한다.\n",
    "X_test = X_test.reshape(X_test.shape[0], 784).astype('float64') / 255\n",
    "\n",
    "\n",
    "# 숫자 이미지에 매겨진 이름을 확인해보기\n",
    "print('class : %d' % (Y_class_train[0]))\n",
    "\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_class_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_class_test, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 숫자의 레이블 값인 5가 출력되는 것을 볼 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
