{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\user\\study2\\mecab_python-0.996_ko_0.9.2_msvc-cp37-cp37m-win_amd64.whl\n",
      "Installing collected packages: mecab-python\n",
      "Successfully installed mecab-python-0.996-ko-0.9.2-msvc\n"
     ]
    }
   ],
   "source": [
    "! pip install mecab_python-0.996_ko_0.9.2_msvc-cp37-cp37m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeCab\tSL,*,*,*,*,*,*,*\n",
      "이\tJKS,*,F,이,*,*,*,*\n",
      "잘\tMAG,*,T,잘,*,*,*,*\n",
      "설치\tNNG,행위,F,설치,*,*,*,*\n",
      "되\tXSV,*,F,되,*,*,*,*\n",
      "었\tEP,*,T,었,*,*,*,*\n",
      "는지\tEC,*,F,는지,*,*,*,*\n",
      "확인\tNNG,행위,T,확인,*,*,*,*\n",
      "중\tNNB,*,T,중,*,*,*,*\n",
      "입니다\tVCP+EF,*,F,입니다,Inflect,VCP,EF,이/VCP/*+ᄇ니다/EF/*\n",
      ".\tSF,*,*,*,*,*,*,*\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 설치확인\n",
    "\n",
    "import MeCab\n",
    "m = MeCab.Tagger()\n",
    "out = m.parse('MeCab이 잘 설치되었는지 확인중입니다.')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\user\\study2\\jpype1-1.0.2-cp37-cp37m-win_amd64.whl\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.2-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: typing-extensions, JPype1\n",
      "Successfully installed JPype1-1.0.2 typing-extensions-3.7.4.2\n"
     ]
    }
   ],
   "source": [
    "! pip install JPype1-1.0.2-cp37-cp37m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.conda\\envs\\study2\\lib\\site-packages (from konlpy) (0.4.3)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\.conda\\envs\\study2\\lib\\site-packages (from konlpy) (1.0.2)\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Downloading tweepy-3.9.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting numpy>=1.6\n",
      "  Downloading numpy-1.19.1-cp37-cp37m-win_amd64.whl (12.9 MB)\n",
      "Collecting lxml>=4.1.0\n",
      "  Downloading lxml-4.5.2-cp37-cp37m-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\users\\user\\.conda\\envs\\study2\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\user\\.conda\\envs\\study2\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Collecting requests[socks]>=2.11.1\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.conda\\envs\\study2\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: beautifulsoup4, urllib3, idna, chardet, PySocks, requests, oauthlib, requests-oauthlib, tweepy, numpy, lxml, konlpy\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.6.0 chardet-3.0.4 idna-2.10 konlpy-0.5.2 lxml-4.5.2 numpy-1.19.1 oauthlib-3.1.0 requests-2.24.0 requests-oauthlib-1.3.0 tweepy-3.9.0 urllib3-1.25.10\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['지금', '코', '코앤엘파이', '앤', '엘', '파이', '한국어', '형태소', '분석기', '설치', '확인']\n"
     ]
    }
   ],
   "source": [
    "# 설치확인\n",
    "from konlpy.tag import Kkma\n",
    "Kkma_pos = Kkma()\n",
    "K_nouns = Kkma_pos.nouns('지금부터 코앤엘파이 한국어 형태소 분석기 설치를 확인해 봅니다.')\n",
    "print(K_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기사 내용 크롤링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Italian Renaissance: Vitruvian Man by Leonardo da VinciIt is the present-day darling of the tech world. The current renaissance of Artificial Intelligence (AI) with its sister discipline Machine Learning (ML) has led every IT firm worth its salt to engineer some form of AI onto its platform, into its toolsets and throughout its software applications.IBM CEO Ginni Rometty has already proclaimed that AI will change 100 percent of jobs over the next decade.And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.When did it all go so right?But AI used to be a fanciful notion mostly confined science fiction, so when did it all go right?In recent years we’ve had some big changes in technology. Aside from the proliferation of mobile devices that has impacted us all, memory has become a lot cheaper, data storage has become a lot easier (in cloud, and elsewhere) and computer processing speeds have continued to outstrip previous records. With the power of quantum computing around the corner, is the AI renaissance simply a result of the coming together of these ‘tech ingredient’ forces?“It isn't just massive compute power. There are important algorithmic changes that have been developed. Plus, it is much easier to gain access to more data in an Internet-connected world,” said Ted Dunning, CTO at data platform, AI and analytics company MapR. “All three aspects (compute, algorithms, data) combine to make todays machine learning possible. Also and quite frankly, a lot of applications only need data availability... we could have implemented them 25 years ago pretty easily if the data had been available and the output of the model could have been integrated back into the business flow.”So, in many ways, Dunning really heralds the modern era of the web as the key facilitator for the new age of AI. Information has become not just ubiquitous; it has also become easier to access and more accurately classified into structured, semi-structured and unstructured data in its rawest form.Tuning AI towards lifeDunning and MapR point out that the new generation of AI & ML is now rediscovering ideas, some of which were first thought of some 50 years ago. The difference today is, each time keep adding a bit of something new. A bit of computing power here, better data there, new ideas for organizing and optimizing a network and after a while we get to build new AI systems that really do useful work. So how should we continue to engineer these new systems?\"A key to success [in the new era of AI] is to focus on the design of the human-AI interactions as much as in the AI itself,\" said Jesus Mantas, general manager and managing partner in IBM Global Business Services. “Many AI programs focus primarily on machine learning algorithms and training datasets, but fail to address the most important success factors: the design of human-machine relationships, new AI-powered workflows… and perfecting the choreography of processes, technology and humans. Those programs rarely scale or achieve benefits. The companies succeeding to scale AI and its benefits demonstrate that skilled, purposeful design of workflows and user interactions lead to faster adoption and business benefits.\"CEO of AI code analytics platform company Gamma is Vishal Rai. In general terms, Rai agrees that the AI renaissance has been driven by tectonic shifts in three areas in the computing world: computing power, swathes of data (and its accessibility)… but also by human ingenuity.He points to new developments coming out of both Silicon Valley but further afield also (China being a prime example, Huawei builds its smartphone chipsets around its Kirin AI-enriched microprocessor) and says that this is all helping to create future industries such as autonomous driving and health care diagnostics.Real world application of AI applicationsSo in what ways are the new real world applications of AI manifesting themselves and starting to impact the services we use below the surface?Cloud computing software intelligence and Application Performance Management (APM) specialist Dynatrace has now extended its AI-powered platform to include IBM Z mainframe support for CICS (a mainframe programming language), IMS (a mainframe database) and middleware. To put that in less technical terms, Dynatrace can be used to monitor software that sits on mainframes to make sure it stays healthy.Why bother? Because the mainframe was never built to be hammered by devices with massively busy data streams like mobile banking apps, games and other online niceties. This means we need AI to understand what impact the mainframe is having on the newer systems we build.“While enterprises are moving applications to modern cloud stacks for agility and competitive advantage, these applications often still depend on critical transactions and ‘crown jewels’ customer data residing on IBM Z mainframes. This puts pressure on these resources to perform tasks that were not envisioned when the mainframes were launched,” said Steve Tack, SVP of products at Dynatrace. “Because Dynatrace provides end-to-end hybrid visibility [through our Davis AI engine], customers can optimize new services, catch performance degradations before user impact, and understand exactly who has been impacted by an incident. This enables customers to confidently innovate applications that leverage data from mainframes to increase revenue, build brand loyalty, and create competitive advantage.”AI as a work of artMany would argue that the path to contemporary AI has been a long slog, but the systems we build now keep finding clever shortcuts… so the momentum for the AI renaissance is actually building cumulatively.Some argue that AI never went away and that the current popularization of AI and its ensuing discussion is just a natural progression of a technology that simply needed to come through a period of adolescence. Either way, AI is in your smartphone and in your cloud computing services, so renaissance or not, let’s hope it continues to become a work of art.Renaissance art\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\study2\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\user\\.conda\\envs\\study2\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "resp = requests.get('https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/#5797412c1f25')\n",
    "\n",
    "soup = BeautifulSoup(resp.text)\n",
    "\n",
    "p_tags = soup.select('div.article-body p')\n",
    "content = ' '\n",
    "\n",
    "for p in p_tags:\n",
    "    content += p.text\n",
    "    \n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영문토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.7.14-cp37-cp37m-win_amd64.whl (268 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434680 sha256=a00246726884d6c2cb204ae6de121102d06ec16c2d48a7a6efba7fa64be3e19e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\45\\6c\\46\\a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, joblib, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 joblib-0.16.0 nltk-3.5 regex-2020.7.14 tqdm-4.48.2\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tokenize() : 마침표와 구두점(온점, 컴마, 물음표, 세미콜론, 느낌표 등과 같은 기호)으로 구분하여 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Italian', 'Renaissance', ':', 'Vitruvian', 'Man', 'by', 'Leonardo', 'da', 'VinciIt', 'is', 'the', 'present-day', 'darling', 'of', 'the', 'tech', 'world', '.', 'The', 'current']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "token1 = word_tokenize(content)\n",
    "print(token1[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordpunctTokenizer() : 알파벳이 아닌 문자를 구분하여 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Italian', 'Renaissance', ':', 'Vitruvian', 'Man', 'by', 'Leonardo', 'da', 'VinciIt', 'is', 'the', 'present', '-', 'day', 'darling', 'of', 'the', 'tech', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "token2 = WordPunctTokenizer().tokenize(content)\n",
    "print(token2[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TreebankWordTokenizer() : 정규표현식에 기반한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Italian', 'Renaissance', ':', 'Vitruvian', 'Man', 'by', 'Leonardo', 'da', 'VinciIt', 'is', 'the', 'present-day', 'darling', 'of', 'the', 'tech', 'world.', 'The', 'current', 'renaissance']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "token = TreebankWordTokenizer().tokenize(content)\n",
    "print(token[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영문 품사부착\n",
    "분리한 토큰마다 품사를 부착한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Italian', 'JJ'), ('Renaissance', 'NNP'), (':', ':'), ('Vitruvian', 'JJ'), ('Man', 'NN'), ('by', 'IN'), ('Leonardo', 'NNP'), ('da', 'NN'), ('VinciIt', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('present-day', 'JJ'), ('darling', 'NN'), ('of', 'IN'), ('the', 'DT'), ('tech', 'JJ'), ('world', 'NN'), ('.', '.'), ('The', 'DT'), ('current', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "taggedToken = pos_tag(token1)\n",
    "print(taggedToken[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개체명 인식\n",
    "청킹이란 ? 데이터/마이닝 데이터+마이닝 하나의 단어로 묶어줌 같이 있어야 의미를 갖는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: ['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n",
      "pos-tag: [('Barack', 'NNP'), ('Obama', 'NNP'), ('likes', 'VBZ'), ('fried', 'VBN'), ('chicken', 'JJ'), ('very', 'RB'), ('much', 'JJ')]\n",
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (ORGANIZATION Obama/NNP)\n",
      "  likes/VBZ\n",
      "  fried/VBN\n",
      "  chicken/JJ\n",
      "  very/RB\n",
      "  much/JJ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#토큰화\n",
    "token1 = word_tokenize('Barack Obama likes fried chicken very much')\n",
    "print('token:', token1)\n",
    "\n",
    "# pos-tag\n",
    "taggedToken = pos_tag(token1)\n",
    "print('pos-tag:' , taggedToken)\n",
    "\n",
    "#청킹  #개체명인식\n",
    "from nltk import ne_chunk\n",
    "neToken= ne_chunk(taggedToken)\n",
    "print(neToken)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전처리\n",
    "- 텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
    "\n",
    "## 토큰화\n",
    "- 텍스트를 자연어 처리를 위해 분리하는 것\n",
    "- 단어 토큰화, 문장토큰화\n",
    "\n",
    "\n",
    "- from konlpy.tag import *\n",
    "- 한번에 모든 형태소 분석기를 호출할 수 있음\n",
    "- 모듈로 만들때에는 필요한 것만 각각 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인생은', '모두가', '함께하는', '여행이다.', '매일매일', '사는', '동안', '우리가', '할', '수', '있는', '건', '최선을', '다해', '이', '멋진', '여행을', '만끽하는', '것이다.']\n"
     ]
    }
   ],
   "source": [
    "text =  '인생은 모두가 함께하는 여행이다. 매일매일 사는 동안 우리가 할 수 있는 건 최선을 다해 이 멋진 여행을 만끽하는 것이다.'\n",
    "print(text.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코모란을 사용하면 더욱 상세한 분석이 가능하다. 앞뒤에 무슨 문장이 올지 분석을 할때에는 코모란을 사용해 상세하게 분석을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인생', '은', '모두', '가', '함께', '하', '는', '여행', '이', '다', '.', '매일', '매일', '살', '는', '동안', '우리', '가', '하', 'ㄹ', '수', '있', '는', '건', '최선', '을', '다', '하', '아', '이', '멋지', 'ㄴ', '여행', '을', '만끽', '하', '는', '것', '이', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "#코모란\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "#선언\n",
    "komoran = Komoran()\n",
    "\n",
    "#토큰화  : morphs\n",
    "komoran_tokens = komoran.morphs(text)\n",
    "print(komoran_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인생', '은', '모두', '가', '함께하', '는', '여행', '이', '다', '.', '매일매일', '사', '는', '동안', '우리', '가', '하', 'ㄹ', '수', '있', '는', '거', '은', '최선', '을', '다하', '어', '이', '멋지', 'ㄴ', '여행', '을', '만끽', '하', '는', '것', '이', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "#한나눔\n",
    "\n",
    "from konlpy.tag  import Hannanum\n",
    "hannanum = Hannanum()\n",
    "hannanum_tokens = hannanum.morphs(text)\n",
    "print(hannanum_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인생', '은', '모두', '가', '함께', '하는', '여행', '이다', '.', '매', '일', '매일', '사는', '동안', '우리', '가', '할', '수', '있는', '건', '최선', '을', '다해', '이', '멋진', '여행', '을', '만끽', '하는', '것', '이다', '.']\n"
     ]
    }
   ],
   "source": [
    "# OKT\n",
    "\n",
    "from konlpy.tag  import Okt\n",
    "okt = Okt()\n",
    "okt_tokens = okt.morphs(text)\n",
    "print(okt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인생', '은', '모두', '가', '함께', '하', '는', '여행', '이', '다', '.', '매일', '매일', '살', '는', '동안', '우리', '가', '하', 'ㄹ', '수', '있', '는', '것', '은', '최선', '을', '다하', '어', '이', '멋지', 'ㄴ', '여행', '을', '만끽', '하', '는', '것', '이', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "# OKT\n",
    "\n",
    "from konlpy.tag  import Kkma\n",
    "Kkma = Kkma()\n",
    "Kkma_tokens = Kkma.morphs(text)\n",
    "print(Kkma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['인생',\n",
       " '은',\n",
       " '모두',\n",
       " '가',\n",
       " '함께',\n",
       " '하',\n",
       " '는',\n",
       " '여행',\n",
       " '이',\n",
       " '다',\n",
       " '.',\n",
       " '매일',\n",
       " '매일',\n",
       " '살',\n",
       " '는',\n",
       " '동안',\n",
       " '우리',\n",
       " '가',\n",
       " '하',\n",
       " 'ㄹ',\n",
       " '수',\n",
       " '있',\n",
       " '는',\n",
       " '건',\n",
       " '최선',\n",
       " '을',\n",
       " '다',\n",
       " '하',\n",
       " '아',\n",
       " '이',\n",
       " '멋지',\n",
       " 'ㄴ',\n",
       " '여행',\n",
       " '을',\n",
       " '만끽',\n",
       " '하',\n",
       " '는',\n",
       " '것',\n",
       " '이',\n",
       " '다',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "komoran_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인생', 'NNG'), ('은', 'NNP'), ('모두', 'MAG'), ('가', 'VV'), ('아', 'EC'), ('함께', 'MAG'), ('하', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('여행', 'NNG'), ('이', 'MM'), ('다', 'MAG'), ('.', 'SF'), ('매일', 'MAG'), ('매일', 'MAG'), ('살', 'VV'), ('ㄹ', 'ETM'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('동안', 'NNG'), ('우리', 'NP'), ('가', 'VV'), ('아', 'EC'), ('하', 'NNG'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('건', 'NNB'), ('최선', 'NNP'), ('을', 'NNG'), ('다', 'MAG'), ('하', 'NNG'), ('아', 'IC'), ('이', 'MM'), ('멋', 'NNG'), ('지', 'NNB'), ('ㄴ', 'JX'), ('여행', 'NNG'), ('을', 'NNG'), ('만끽', 'NNP'), ('하', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('것', 'NNB'), ('이', 'MM'), ('다', 'MAG'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "# 코모란\n",
    "# postagging\n",
    "\n",
    "komoranTag = []\n",
    "\n",
    "for token in komoran_tokens:\n",
    "    komoranTag += komoran.pos(token)\n",
    "print(komoranTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인생', 'N'), ('은', 'N'), ('모두', 'M'), ('가', 'J'), ('함께하', 'P'), ('어', 'E'), ('늘', 'P'), ('ㄴ', 'E'), ('여행', 'N'), ('이', 'M'), ('다', 'M'), ('.', 'S'), ('매일매일', 'M'), ('사', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('동안', 'N'), ('우리', 'N'), ('가', 'J'), ('하', 'I'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('것', 'N'), ('은', 'N'), ('최선', 'N'), ('을', 'N'), ('다하', 'P'), ('어', 'E'), ('어', 'N'), ('이', 'M'), ('멋지', 'N'), ('ㄴ', 'N'), ('여행', 'N'), ('을', 'N'), ('만끽', 'N'), ('하', 'I'), ('늘', 'P'), ('ㄴ', 'E'), ('것', 'N'), ('이', 'M'), ('다', 'M'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석기마다 포스태깅이 다르다.\n",
    "\n",
    "hannanumTag = []\n",
    "for token in hannanum_tokens:\n",
    "    hannanumTag += hannanum.pos(token)\n",
    "print(hannanumTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인생', 'Noun'), ('은', 'Noun'), ('모두', 'Noun'), ('가', 'Verb'), ('함께', 'Adverb'), ('하는', 'Verb'), ('여행', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation'), ('매', 'Noun'), ('일', 'Noun'), ('매일', 'Noun'), ('사는', 'Verb'), ('동안', 'Noun'), ('우리', 'Noun'), ('가', 'Verb'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('건', 'Noun'), ('최선', 'Noun'), ('을', 'Josa'), ('다해', 'Noun'), ('이', 'Noun'), ('멋진', 'Adjective'), ('여행', 'Noun'), ('을', 'Josa'), ('만끽', 'Noun'), ('하는', 'Verb'), ('것', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "oktTag = []\n",
    "for token in okt_tokens:\n",
    "    oktTag += okt.pos(token)\n",
    "print(oktTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인생', 'NNG'), ('은', 'NNG'), ('모두', 'MAG'), ('가', 'NNG'), ('함께', 'MAG'), ('하', 'NNG'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('여행', 'NNG'), ('이', 'NNG'), ('다', 'NNG'), ('.', 'SF'), ('매일', 'MAG'), ('매일', 'MAG'), ('살', 'NNG'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('동안', 'NNG'), ('우리', 'NP'), ('가', 'NNG'), ('하', 'NNG'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('것', 'NNB'), ('은', 'NNG'), ('최선', 'NNG'), ('을', 'NNG'), ('다하', 'VV'), ('어', 'NNG'), ('이', 'NNG'), ('멋지', 'VA'), ('ㄴ', 'NNG'), ('여행', 'NNG'), ('을', 'NNG'), ('만끽', 'NNG'), ('하', 'NNG'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('것', 'NNB'), ('이', 'NNG'), ('다', 'NNG'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "kkmaTag = []\n",
    "\n",
    "for token in Kkma_tokens:\n",
    "    kkmaTag += Kkma.pos(token)\n",
    "print(kkmaTag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 처리\n",
    "- 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
    "- 불필요함 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoNLPy에서 품사 태깅목록 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('가', 'Verb'), 2),\n",
       " (('하는', 'Verb'), 2),\n",
       " (('여행', 'Noun'), 2),\n",
       " (('이다', 'Josa'), 2),\n",
       " (('.', 'Punctuation'), 2),\n",
       " (('을', 'Josa'), 2),\n",
       " (('인생', 'Noun'), 1),\n",
       " (('은', 'Noun'), 1),\n",
       " (('모두', 'Noun'), 1),\n",
       " (('함께', 'Adverb'), 1),\n",
       " (('매', 'Noun'), 1),\n",
       " (('일', 'Noun'), 1),\n",
       " (('매일', 'Noun'), 1),\n",
       " (('사는', 'Verb'), 1),\n",
       " (('동안', 'Noun'), 1),\n",
       " (('우리', 'Noun'), 1),\n",
       " (('할', 'Verb'), 1),\n",
       " (('수', 'Noun'), 1),\n",
       " (('있는', 'Adjective'), 1),\n",
       " (('건', 'Noun'), 1),\n",
       " (('최선', 'Noun'), 1),\n",
       " (('다해', 'Noun'), 1),\n",
       " (('이', 'Noun'), 1),\n",
       " (('멋진', 'Adjective'), 1),\n",
       " (('만끽', 'Noun'), 1),\n",
       " (('것', 'Noun'), 1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okt \n",
    "# 최빈어 조회.최빈어를 조회하여 불용어 제거 대상을 선정\n",
    "# 단어 빈도 조회\n",
    "\n",
    "from collections import Counter\n",
    "Counter(oktTag).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인생', '모두', '가', '하는', '여행', '매', '일', '매일', '사는', '동안', '우리', '가', '할', '수', '있는', '건', '최선', '다해', '이', '멋진', '여행', '만끽', '하는', '것']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 처리\n",
    "stopPos = ['Determiner','Adverb','Conjunction','Josa','PreEomi','Eomi','Suffix',\n",
    "          'Punctuation', 'Foreign','Alpha','Number','Unknown']\n",
    "stopWord = ['을','은']\n",
    "word = []\n",
    "\n",
    "for tag in oktTag:\n",
    "    if tag[1] not in stopPos:\n",
    "        if tag[0] not in stopWord:\n",
    "            word.append(tag[0])\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인생', '은', '모두', '가', '함께', '하는', '여행', '이다', '.', '매', '일', '매일', '사는', '동안', '우리', '가', '할', '수', '있는', '건', '최선', '을', '다해', '이', '멋진', '여행', '을', '만끽', '하는', '것', '이다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(okt_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 형태소 분석 (웹크롤링 기사 실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (서울=연합뉴스) 홍지인 기자 = 구글의 칼라 브롬버그 '공익을 위한 AI' 프로그램 리드가 4일 역삼동 구글코리아에서 화상 기자간담회를 갖고 있다. 2020. 2. 4. (서울=연합뉴스) 홍지인 기자 = 구글은 인공지능(AI) 기술을 활용한 기상 예측 모델을 개발해 일부 분야에서는 미국 정부보다 더 뛰어난 예보 결과를 내놓고 있다고 4일 밝혔다. 구글의 칼라 브롬버그 '공익을 위한 AI' 프로그램 리드는 이날 서울 역삼동 구글코리아에서 가진 화상 기자간담회에서 \"신경망을 이용한 기상 예측은 기존 예측 방법보다 훨씬 정확하다\"고 말했다.  구글의 '나우캐스트' 기상 예측 모델은 6시간 이내 단기 예보에 초점을 맞추고 기상 레이더 관측 자료와 위성 사진 등을 모아 유넷(U-Net)이라는 신경망으로 계산한다. 기존 모델로는 몇 시간이 걸리는 작업을 5~10분 만에 내놓을 수 있고, 공간 해상도도 1㎞로 미국 해양대기청(NOAA)의 예보모델 'HRRR'보다 10배 더 상세하다고 구글은 소개했다.  예보 정확도 면에서는 1~3시간 단기예보의 경우도 HRRR보다 더 뛰어나다고 구글은 분석했다. 단, 5~6시간 이상 예보에서는 HRRR이 더 정확했다.  칼라 리드는 \"지금 시점에서 당장 기상 예측 모델을 상용화할 계획은 없다\"며 \"머신러닝 기법을 이용해 얼마나 날씨를 정확하게 예측할 수 있는지에 대해 연구과제로만 삼고 있다\"고 말했다.  함유근 전남대 지구환경과학부 교수는 이날 간담회에서 '합성곱 신경망 기법'(CNN·Convolutional Neural Network)을 응용한 엘니뇨 예측 모형을 소개했다. 그가 개발한 엘니뇨 예측 모형은 향후 18개월 동안 70% 이상 정확도로 엘니뇨 발생 가능성을 맞출 수 있다. 기존 모델은 예능 가능 기간이 1년 정도였다고 함 교수는 소개했다.  함 교수는 \"엘니뇨의 예측 성능이 18개월로 늘어나면 엘니뇨로 인해 발생하는 전 세계적인 곡물 가격 변동 등에 선제적으로 대응할 수 있게 된다\"고 설명했다. ljungberg@yna.co.kr<저작권자(c) 연합뉴스, 무단 전재-재배포 금지>2020/02/04 11:33 송고\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.yna.co.kr/view/AKR20200204081000017?section=it/it'\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text)\n",
    "\n",
    "p_tags = soup.select('div.story-news p')\n",
    "\n",
    "content = ' '\n",
    "\n",
    "for p in p_tags:\n",
    "    content += p.text\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['서울', '구글', '칼라', '브롬버그', '공익', '위한', 'AI', '프로그램', '리드', '4', '일', '역삼동', '구글', '코리아', '에서', '화상', '간담회', '갖', '있', '2020', '2', '4', '서울', '구글', '인공지능', 'AI', '기술', '활용', '한', '기상', '예측', '모델', '개발', '해', '일부', '분야', '에서', '미국', '정부', '보다', '더', '뛰어난', '예보', '결과', '내놓', '있', '다고', '4', '일', '밝혔', '구글', '칼라', '브롬버그', '공익', '위한', 'AI', '프로그램', '리드', '이날', '서울', '역삼동', '구글', '코리아', '에서', '가진', '화상', '간담회', '에서', '신경망', '이용', '한', '기상', '예측', '기존', '예측', '방법', '보다', '훨씬', '정확', '하', '말', '했', '구글', '나우', '캐스트', '기상', '예측', '모델', '6', '시간', '이내', '단기', '예보', '초점', '맞추', '기상', '레이더', '관측', '자료', '와', '위성', '사진', '등', '모아', '유', '넷', 'U', 'Net', '라는', '신경망', '으로', '계산', '한다', '기존', '모델', '몇', '시간', '걸리', '작업', '5', '10', '분', '만', '내놓', '수', '있', '공간', '해상도', '도', '1', '미국', '해양', '대', '기청', 'NOAA', '예보', '모델', 'HRRR', '보다', '10', '배', '더', '상세', '하', '다고', '구글', '소개', '했', '예보', '정확', '도', '면', '에서', '1', '3', '시간', '단기', '예보', '경우', '도', 'HRRR', '보다', '더', '뛰어나', '다고', '구글', '분석', '했', '5', '6', '시간', '이상', '예보', '에서', 'HRRR', '더', '정확', '했', '칼라', '리드', '지금', '시점', '에서', '당장', '기상', '예측', '모델', '상용', '화', '할', '계획', '없', '며', '머신', '러닝', '기법', '이용', '해', '얼마나', '날씨', '정확', '하', '게', '예측', '할', '수', '있', '는지', '대해', '연구', '과제', '만', '삼', '있', '말', '했', '함유근', '전', '남대', '지구', '환경', '과', '학부', '교수', '이날', '간담회', '에서', '합성곱', '신경망', '기법', 'CNN', 'Convolutional', 'Neural', 'Network', '응용', '한', '엘니뇨', '예측', '모형', '소개', '했', '그', '개발', '한', '엘니뇨', '예측', '모형', '향후', '18', '개월', '동안', '70', '이상', '정확도', '엘니뇨', '발생', '가능', '성', '맞출', '수', '있', '기존', '모델', '예능', '가능', '기간', '1', '년', '정도', '였', '다고', '함', '교수', '소개', '했', '함', '교수', '엘니뇨', '예측', '성능', '18', '개월', '늘어나', '면', '엘니뇨', '인해', '발생', '하', '전', '세계', '적', '인', '곡물', '가격', '변동', '등', '선제', '적', '으로', '대응', '할', '수', '있', '게', '된다', '설명', '했', 'ljungberg', 'yna', 'co', 'kr', '저작권자', 'c', '전재', '재', '2020', '02', '04', '11', '33', '송고']\n"
     ]
    }
   ],
   "source": [
    "#  ps_tagging\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "mecab_tokens = mecab.pos(content)\n",
    "\n",
    "# Mecab.nouns() 명사추출\n",
    "# Mecab.morphs() 문장을 형태소 단위로 끊어준다.\n",
    "# Mecab.pos() 문장을 형태로 단위로 끊고 형태소마다 품사를 분석해준다.\n",
    "\n",
    "\n",
    "\n",
    "# 불용어 처리\n",
    "stopPos = ['Determiner','Adverb','Conjunction','Josa','PreEomi','Eomi','Suffix',\n",
    "          'Punctuation', 'Foreign','Alpha','Number','Unknown','SY','SF','SC',\n",
    "          'MAJ','MAJ','SSO','SSC','JKO']\n",
    "stopWord = ['을','은', '이', '가', '를','는','기자','홍지인','연합뉴스','의','고',\n",
    "           '로','다','에','무단','금지','배포']\n",
    "word = []\n",
    "\n",
    "for tag in mecab_tokens:\n",
    "    # tag[1] == postagging \n",
    "    if tag[1] not in stopPos:\n",
    "        # tag[0] == 한글부분\n",
    "        if tag[0] not in stopWord:\n",
    "            word.append(tag[0])\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(', 'SSO'), ('서울', 'NNP'), ('=', 'SY'), ('연합뉴스', 'NNP'), (')', 'SSC'), ('홍지인', 'NNP'), ('기자', 'NNG'), ('=', 'SY'), ('구글', 'NNG'), ('의', 'NNG'), ('칼라', 'NNG'), ('브롬버그', 'NNP'), (\"'\", 'SY'), ('공익', 'NNG'), ('을', 'JKO'), ('위한', 'VV+ETM'), ('AI', 'SL'), (\"'\", 'SY'), ('프로그램', 'NNG'), ('리드', 'NNP'), ('가', 'VV+EC'), ('4', 'SN'), ('일', 'NNG'), ('역삼동', 'NNP'), ('구글', 'NNG'), ('코리아', 'NNP'), ('에서', 'JKB'), ('화상', 'NNG'), ('기자', 'NNG'), ('간담회', 'NNG'), ('를', 'JKO'), ('갖', 'NNG'), ('고', 'EC'), ('있', 'VA'), ('다', 'MAG'), ('.', 'SF'), ('2020', 'SN'), ('.', 'SF'), ('2', 'SN'), ('.', 'SF'), ('4', 'SN'), ('.', 'SF'), ('(', 'SSO'), ('서울', 'NNP'), ('=', 'SY'), ('연합뉴스', 'NNP'), (')', 'SSC'), ('홍지인', 'NNP'), ('기자', 'NNG'), ('=', 'SY'), ('구글', 'NNG'), ('은', 'NNG'), ('인공지능', 'NNP'), ('(', 'SSO'), ('AI', 'SL'), (')', 'SSC'), ('기술', 'NNG'), ('을', 'JKO'), ('활용', 'NNG'), ('한', 'MM'), ('기상', 'NNG'), ('예측', 'NNG'), ('모델', 'NNG'), ('을', 'JKO'), ('개발', 'NNG'), ('해', 'VV+EC'), ('일부', 'NNG'), ('분야', 'NNG'), ('에서', 'JKB'), ('는', 'JX'), ('미국', 'NNP'), ('정부', 'NNG'), ('보', 'VV'), ('다', 'EC'), ('더', 'MAG'), ('뛰어난', 'VA+ETM'), ('예보', 'NNG'), ('결과', 'NNG'), ('를', 'JKO'), ('내', 'NP+JKG'), ('놓', 'NNG'), ('고', 'EC'), ('있', 'VA'), ('다고', 'EC'), ('4', 'SN'), ('일', 'NNG'), ('밝혔', 'VV+EP'), ('다', 'MAG'), ('.', 'SF'), ('구글', 'NNG'), ('의', 'NNG'), ('칼라', 'NNG'), ('브롬버그', 'NNP'), (\"'\", 'SY'), ('공익', 'NNG'), ('을', 'JKO'), ('위한', 'VV+ETM'), ('AI', 'SL'), (\"'\", 'SY'), ('프로그램', 'NNG'), ('리드', 'NNP'), ('는', 'JX'), ('이날', 'NNG'), ('서울', 'NNP'), ('역삼동', 'NNP'), ('구글', 'NNG'), ('코리아', 'NNP'), ('에서', 'JKB'), ('가진', 'VV+ETM'), ('화상', 'NNG'), ('기자', 'NNG'), ('간담회', 'NNG'), ('에서', 'JKB'), ('\"', 'SY'), ('신경망', 'NNG'), ('을', 'JKO'), ('이용', 'NNG'), ('한', 'MM'), ('기상', 'NNG'), ('예측', 'NNG'), ('은', 'NNG'), ('기존', 'NNG'), ('예측', 'NNG'), ('방법', 'NNG'), ('보', 'VV'), ('다', 'EC'), ('훨씬', 'MAG'), ('정확', 'NNG'), ('하', 'VV'), ('다', 'MAG'), ('\"', 'SY'), ('고', 'EC'), ('말', 'NNG'), ('했', 'VV+EP'), ('다', 'MAG'), ('.', 'SF'), ('구글', 'NNG'), ('의', 'NNG'), (\"'\", 'SY'), ('나우', 'NNP'), ('캐스트', 'NNG'), (\"'\", 'SY'), ('기상', 'NNG'), ('예측', 'NNG'), ('모델', 'NNG'), ('은', 'NNG'), ('6', 'SN'), ('시간', 'NNG'), ('이내', 'NNG'), ('단기', 'NNG'), ('예보', 'NNG'), ('에', 'IC'), ('초점', 'NNG'), ('을', 'JKO'), ('맞추', 'VV'), ('고', 'EC'), ('기상', 'NNG'), ('레이더', 'NNG'), ('관측', 'NNG'), ('자료', 'NNG'), ('와', 'VV+EC'), ('위성', 'NNG'), ('사진', 'NNG'), ('등', 'NNG'), ('을', 'JKO'), ('모아', 'VV+EC'), ('유', 'NNP'), ('넷', 'NR'), ('(', 'SSO'), ('U', 'SL'), ('-', 'SY'), ('Net', 'SL'), (')', 'SSC'), ('이', 'MM'), ('라는', 'ETM'), ('신경망', 'NNG'), ('으로', 'JKB'), ('계산', 'NNG'), ('한다', 'VV+EC'), ('.', 'SF'), ('기존', 'NNG'), ('모델', 'NNG'), ('로', 'JKB'), ('는', 'JX'), ('몇', 'MM'), ('시간', 'NNG'), ('이', 'MM'), ('걸리', 'NNG'), ('는', 'JX'), ('작업', 'NNG'), ('을', 'JKO'), ('5', 'SN'), ('~', 'SY'), ('10', 'SN'), ('분', 'NNG'), ('만', 'JX'), ('에', 'IC'), ('내', 'NP+JKG'), ('놓', 'NNG'), ('을', 'JKO'), ('수', 'NNG'), ('있', 'VA'), ('고', 'EC'), (',', 'SC'), ('공간', 'NNG'), ('해상도', 'NNG'), ('도', 'NNG'), ('1', 'SN'), ('㎞', 'SY'), ('로', 'JKB'), ('미국', 'NNP'), ('해양', 'NNG'), ('대', 'NNG'), ('기청', 'NNG'), ('(', 'SSO'), ('NOAA', 'SL'), (')', 'SSC'), ('의', 'NNG'), ('예보', 'NNG'), ('모델', 'NNG'), (\"'\", 'SY'), ('HRRR', 'SL'), (\"'\", 'SY'), ('보', 'VV'), ('다', 'EC'), ('10', 'SN'), ('배', 'NNG'), ('더', 'MAG'), ('상세', 'NNG'), ('하', 'VV'), ('다고', 'EC'), ('구글', 'NNG'), ('은', 'NNG'), ('소개', 'NNG'), ('했', 'VV+EP'), ('다', 'MAG'), ('.', 'SF'), ('예보', 'NNG'), ('정확', 'NNG'), ('도', 'NNG'), ('면', 'NNG'), ('에서', 'JKB'), ('는', 'JX'), ('1', 'SN'), ('~', 'SY'), ('3', 'SN'), ('시간', 'NNG'), ('단기', 'NNG'), ('예보', 'NNG'), ('의', 'NNG'), ('경우', 'NNG'), ('도', 'NNG'), ('HRRR', 'SL'), ('보', 'VV'), ('다', 'EC'), ('더', 'MAG'), ('뛰어나', 'VA+EC'), ('다고', 'EC'), ('구글', 'NNG'), ('은', 'NNG'), ('분석', 'NNG'), ('했', 'VV+EP'), ('다', 'MAG'), ('.', 'SF'), ('단', 'MM'), (',', 'SC'), ('5', 'SN'), ('~', 'SY'), ('6', 'SN'), ('시간', 'NNG'), ('이상', 'NNG'), ('예보', 'NNG'), ('에서', 'JKB'), ('는', 'JX'), ('HRRR', 'SL'), ('이', 'MM'), ('더', 'MAG'), ('정확', 'NNG'), ('했', 'VV+EP'), ('다', 'MAG'), ('.', 'SF'), ('칼라', 'NNG'), ('리드', 'NNP'), ('는', 'JX'), ('\"', 'SY'), ('지금', 'NNG'), ('시점', 'NNG'), ('에서', 'JKB'), ('당장', 'NNG'), ('기상', 'NNG'), ('예측', 'NNG'), ('모델', 'NNG'), ('을', 'JKO'), ('상용', 'NNG'), ('화', 'NNG'), ('할', 'VV+ETM'), ('계획', 'NNG'), ('은', 'NNG'), ('없', 'VA'), ('다', 'MAG'), ('\"', 'SY'), ('며', 'EC'), ('\"', 'SY'), ('머신', 'NNG'), ('러닝', 'NNG'), ('기법', 'NNG'), ('을', 'JKO'), ('이용', 'NNG'), ('해', 'VV+EC'), ('얼마나', 'MAG'), ('날씨', 'NNG'), ('를', 'JKO'), ('정확', 'NNG'), ('하', 'VV'), ('게', 'NNG'), ('예측', 'NNG'), ('할', 'VV+ETM'), ('수', 'NNG'), ('있', 'VA'), ('는지', 'EC'), ('에', 'IC'), ('대해', 'VV+EC'), ('연구', 'NNG'), ('과제', 'NNG'), ('로', 'JKB'), ('만', 'JX'), ('삼', 'NR'), ('고', 'EC'), ('있', 'VA'), ('다', 'MAG'), ('\"', 'SY'), ('고', 'EC'), ('말', 'NNG'), ('했', 'VV+EP'), ('다', 'MAG'), ('.', 'SF'), ('함유근', 'NNP'), ('전', 'NNG'), ('남대', 'NNG'), ('지구', 'NNG'), ('환경', 'NNG'), ('과', 'NNG'), ('학부', 'NNG'), ('교수', 'NNG'), ('는', 'JX'), ('이날', 'NNG'), ('간담회', 'NNG'), ('에서', 'JKB'), (\"'\", 'SY'), ('합성곱', 'NNP'), ('신경망', 'NNG'), ('기법', 'NNG'), (\"'(\", 'SY'), ('CNN', 'SL'), ('·', 'SC'), ('Convolutional', 'SL'), ('Neural', 'SL'), ('Network', 'SL'), (')', 'SSC'), ('을', 'JKO'), ('응용', 'NNG'), ('한', 'MM'), ('엘니뇨', 'NNG'), ('예측', 'NNG'), ('모형', 'NNG'), ('을', 'JKO'), ('소개', 'NNG'), ('했', 'VV+EP'), ('다', 'MAG'), ('.', 'SF'), ('그', 'MM'), ('가', 'VV+EC'), ('개발', 'NNG'), ('한', 'MM'), ('엘니뇨', 'NNG'), ('예측', 'NNG'), ('모형', 'NNG'), ('은', 'NNG'), ('향후', 'NNG'), ('18', 'SN'), ('개월', 'NNBC'), ('동안', 'NNG'), ('70', 'SN'), ('%', 'SY'), ('이상', 'NNG'), ('정확도', 'NNG'), ('로', 'JKB'), ('엘니뇨', 'NNG'), ('발생', 'NNG'), ('가능', 'NNG'), ('성', 'NNG'), ('을', 'JKO'), ('맞출', 'VV+ETM'), ('수', 'NNG'), ('있', 'VA'), ('다', 'MAG'), ('.', 'SF'), ('기존', 'NNG'), ('모델', 'NNG'), ('은', 'NNG'), ('예능', 'NNG'), ('가능', 'NNG'), ('기간', 'NNG'), ('이', 'MM'), ('1', 'SN'), ('년', 'NNG'), ('정도', 'NNG'), ('였', 'EP'), ('다고', 'EC'), ('함', 'NNG'), ('교수', 'NNG'), ('는', 'JX'), ('소개', 'NNG'), ('했', 'VV+EP'), ('다', 'MAG'), ('.', 'SF'), ('함', 'NNG'), ('교수', 'NNG'), ('는', 'JX'), ('\"', 'SY'), ('엘니뇨', 'NNG'), ('의', 'NNG'), ('예측', 'NNG'), ('성능', 'NNG'), ('이', 'MM'), ('18', 'SN'), ('개월', 'NNBC'), ('로', 'JKB'), ('늘어나', 'VV+EC'), ('면', 'NNG'), ('엘니뇨', 'NNG'), ('로', 'JKB'), ('인해', 'VV+EC'), ('발생', 'NNG'), ('하', 'VV'), ('는', 'JX'), ('전', 'NNG'), ('세계', 'NNG'), ('적', 'NNG'), ('인', 'NNG'), ('곡물', 'NNG'), ('가격', 'NNG'), ('변동', 'NNG'), ('등', 'NNG'), ('에', 'IC'), ('선제', 'NNG'), ('적', 'NNG'), ('으로', 'JKB'), ('대응', 'NNG'), ('할', 'VV+ETM'), ('수', 'NNG'), ('있', 'VA'), ('게', 'NNG'), ('된다', 'VV+EC'), ('\"', 'SY'), ('고', 'EC'), ('설명', 'NNG'), ('했', 'VV+EP'), ('다', 'MAG'), ('.', 'SF'), ('ljungberg', 'SL'), ('@', 'SY'), ('yna', 'SL'), ('.', 'SF'), ('co', 'SL'), ('.', 'SF'), ('kr', 'SL'), ('<', 'SY'), ('저작권자', 'NNG'), ('(', 'SSO'), ('c', 'SL'), (')', 'SSC'), ('연합뉴스', 'NNP'), (',', 'SC'), ('무단', 'NNG'), ('전재', 'NNG'), ('-', 'SY'), ('재', 'XPN'), ('배포', 'NNG'), ('금지', 'NNG'), ('>', 'SY'), ('2020', 'SN'), ('/', 'SC'), ('02', 'SN'), ('/', 'SC'), ('04', 'SN'), ('11', 'SN'), (':', 'SC'), ('33', 'SN'), ('송고', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print(mecab_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
